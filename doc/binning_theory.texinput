% -*- mode: latex -*-
%
\providecommand{\Dir}{\ensuremath{\mathrm{Dir}}}
\providecommand{\Beta}{\ensuremath{\mathrm{Beta}}}

\newcommand{\Yf}{\boldsymbol{Y}}
\newcommand{\Xf}{\boldsymbol{X}}
\newcommand{\pf}{\boldsymbol{p}}
\newcommand{\Df}{\boldsymbol{D}}
\newcommand{\Ef}{\boldsymbol{E}}
\newcommand{\xf}{\boldsymbol{x}}
\newcommand{\yf}{\boldsymbol{y}}
\newcommand{\zf}{\boldsymbol{z}}
\newcommand{\cf}{\boldsymbol{c}}
\newcommand{\gf}{\boldsymbol{g}}
\newcommand{\alphaf}{\boldsymbol{\alpha}}
\newcommand{\betaf}{\boldsymbol{\beta}}
\newcommand{\gammaf}{\boldsymbol{\gamma}}

\newcommand{\nf}{\boldsymbol{n}}

\newcommand{\xt}{\tilde{x}}
\newcommand{\yt}{\tilde{y}}

\newcommand{\Xc}{\mathcal{X}}
\newcommand{\Yc}{\mathcal{Y}}
\newcommand{\Fc}{\mathcal{F}}
\newcommand{\Gc}{\mathcal{G}}
\newcommand{\Hc}{\mathcal{H}}
\newcommand{\Oc}{\mathcal{O}}
\newcommand{\Pc}{\mathcal{P}}
\newcommand{\Cc}{\mathcal{C}}

\section*{Introduction}
\label{introduction}
\begin{equation}
  \begin{split}
    P(\vec{p},B|D,m_B) &= \frac{P(D,\vec{p},B|m_B)}{P(D|m_B)}\\
                       &= \frac{P(D|\vec{p},B,m_B)\pi(\vec{p},B|m_B)}{P(D|m_B)}
  \end{split}
\end{equation}
$\vec{p} = (p_y^b)_{K\times m_B}$, $m_B = |B|$, $\mathcal{X} = \{x_1, x_2, \dots, x_L\}$, $\mathcal{Y} =
\{y_1, y_2, \dots, y_K\}$ (discretized into $L$ intervals of equal
size) and $m_T$ is the number of trials.

\section*{Probabilities}
The tripe $(\Omega, \mathcal{F}, P)$ is called a probability space,
where $\Omega$ denotes the set of elementary events, $\mathcal{F}$ a
$\sigma$-algebra of subsets of $\Omega$, and $P$ a probability
measure of events $A \in \mathcal{F}$. Let $(\Omega, \mathcal{F}, P)$
be a probability space and $(\Omega', \mathcal{F}')$ a measure space,
then the function $X: \Omega \rightarrow \Omega'$ is called a random
variable. Probability mass function $p(\omega_i) = P(\{\omega_i\})$

\section*{Inference}

\begin{definition}[Infinite exchangeability]
  An infinite sequence of random quantities $X_1, X_2, \dots$ is said
  to be infinitely exchangeable with respect to a probability measure
  $P$ if for every subsequence of length $n$
  \begin{equation}
    P(X_1 = \omega_1, \dots, X_n = \omega_n) = P(X_{1} = \omega_{\pi(1)},
    \dots, X_{n} = \omega_{\pi(n)})
  \end{equation}
  for all permutations $\pi$ defined on the set $\{1, \dots, n\}$.
\end{definition}

\begin{theorem}[de Finetti's representation theorem]
  Let $X_1, X_2, \dots$ denote an infinitely exchangeable
  sequence of random quantities with probability measure $P$. There
  exists a cumulative distribution function $\Pi$ such that the joint
  mass function $p(\omega_1, \dots, \omega_n) = P(X_1 = \omega_1, \dots,
  X_n = \omega_n)$ has the form
  \begin{equation}
    \begin{split}
      p(x_1, \dots, x_n) &=
      \int_{\Delta^L} p(x_i|\theta)
      \mathrm{d} \Pi(\vec{\theta})\\
      &=
      \int_{\Delta^L} \prod_{i=1}^{n}\vec{\theta}^{n_y}
      \mathrm{d} \Pi(\vec{\theta}), ~ \text{where}\\
      \Pi(\vec{\theta}) &= \lim\limits_{n \rightarrow \infty}
      P \left\{\frac{n_y}{n} \le \theta_y \right\}.
    \end{split}
  \end{equation}
\end{theorem}

\section*{Assumptions and Prior Knowledge}
Todo: De Finetti, n-exchangeability, from which it follows that the
likelihood function is given by the multinomial likelihood
\begin{equation}
  P(D|\vec{p},B,m_B) = \prod_{b\in B} \prod_{y\in \mathcal{Y}}(p_y^b)^{n^b_y},
\end{equation}
where $n^b_y$ denotes the count statistic for event $y$ in bin
$b$. The conjugate prior for a multinomial likelihood function is
given by the Dirichlet distribution $\Dir(\vec{p}^b;\vec{\alpha}^b)$, which
states the belief that the probabilities of $K$ rival events in bin
$b$ are $p_y^b$ given that each event has been observed $\alpha^b_y - 1$
times. The choice of this prior can be justified by \textit{Johnson's
  sufficiency postulate}. Hence
\begin{equation}
  P(\vec{p},B|m_B) = \prod_{b\in B}\frac{1}{\Beta(\vec{\alpha}^b)}
  \prod_{y\in \mathcal{Y}}(p_y^b)^{\alpha^b_y-1},
\end{equation}

\begin{equation}
  \Beta(z_1,z_2,\dots,z_m) =
  \frac{\prod_{i=1}^{m}\Gamma(z_i)}{\Gamma(\sum_{i=1}^{m}z_i)},~
  \text{with}~
  \Gamma(z) = \int_0^\infty t^{z-1}e^{-t}\mathrm{d}t
\end{equation}

For $\pi(\vec{p},B|m_B)$ a non-informative prior assumption is made, i.e.
\begin{equation}
  \pi(\vec{p},B|m_B) = \pi(\vec{p}|m_B)\pi(B|m_B),
\end{equation}
which means that the prior knowledge about $\vec{p}$ is independent of
$B$ and vice versa.

A non-informative prior is chosen for the multi-bins
\begin{equation}
  \pi(B|m_B) = {L \choose m_B}^{-1}
\end{equation}
Define $\pi(\vec{p}|m_B)$!

\section*{Evidence}
\begin{equation}
  \begin{split}
    P(D|m_B)
    &= \sum_{B\in \mathcal{P}_{m_B}(\mathcal{X})}
      P(D|B,m_B) \pi(B|m_B)\\
  \end{split}
\end{equation}
\begin{equation}
  \begin{split}
    P(D|B,m_B)
    &=
      \int_{\underbrace{\Delta^K \times \Delta^K \times \dots \times
          \Delta^K}_{m_B~\text{times}}}\mathrm{d}\vec{p}
      P(D|\vec{p},B,m_B) \pi(\vec{p}|m_B)\\
    &=
      \int_{\Delta^K \times \Delta^K \times \dots \times \Delta^K}\mathrm{d}\vec{p}
      \prod_{b\in B}\prod_{y\in \mathcal{Y}}(p_y^b)^{n^b_y}
      \prod_{b\in B}\frac{1}{\Beta(\vec{\alpha}^b)}
      \prod_{y\in \mathcal{Y}}(p_y^b)^{\alpha^b_y-1}\\
    &=
      \int_{\Delta^K \times \Delta^K \times \dots \times \Delta^K}\mathrm{d}\vec{p}
      \prod_{b\in B}\frac{1}{\Beta(\vec{\alpha}^b)}
      \prod_{y\in \mathcal{Y}}(p_y^b)^{n^b_y+\alpha^b_y-1},
  \end{split}
\end{equation}
where $\Delta^K := \{(p_1,p_2,\dots,p_K)\in\mathbb{R} : \sum_{i=1}^{K}
p_i = 1 ~ \text{and} ~ p_i \ge 0~\forall i\}$ denotes the
$K$-dimensional probability simplex. The solution of the Euler
integral of the first kind is given by the multinomial Beta-function.

\begin{equation}
  \begin{split}
    \Beta(\vec{z}) =
    \underbrace{\int_{\Delta^K}\prod_{z\in\mathcal{Z}}t^{z-1}\mathrm{d}t}_{\text{Euler
      integral of the first kind}}=
    \underbrace{\frac{\prod_{z\in\mathcal{Z}}\Gamma(z)}{\Gamma(\sum_{z\in\mathcal{Z}}z)}}_{\text{multinomial
      Beta function}}
  \end{split}
\end{equation}

Hence
\begin{equation}
  \begin{split}
    P(D|m_B)
    &= \sum_{B\in \mathcal{P}_{m_B}(\mathcal{X})}
    \pi(B|m_B)
    \prod_{b\in B}\Beta(\vec{\alpha}^b)^{-1}
    \frac{\prod_{y\in
        \mathcal{Y}}\Gamma(n_y^b+\alpha^b_y)}{\Gamma(\sum_{y\in
        \mathcal{Y}}n_y^b+\alpha^b_y)}\\
    &=  \sum_{B\in \mathcal{P}_{m_B}(\mathcal{X})}
    \pi(B|m_B)
    \prod_{b\in B}\frac{\Beta(\vec{n}^b+\vec{\alpha}^b)}{\Beta(\vec{\alpha}^b)}
  \end{split}
\end{equation}

\section*{Expectation and Variance}
To characterize the inferred distribution, we will compute the first
three moments from the moment generating function
\begin{equation}
  \begin{split}
    \mu_i' = \int x^i\mathrm{d}P(x) = \frac{P(D^i|m_B)}{P(D|m_B)},
  \end{split}
\end{equation}
where $P(x)$ denotes the Lebesgue-Stieltjes probability density
function and $\mu_i'$ the $i$th raw moment. The central moments
$\mu_i$ are given by
\begin{equation}
  \begin{split}
    \mu_i = \int (x-\mu_1')^i\mathrm{d}P(x),
  \end{split}
\end{equation}
which can be computed from the raw moments by using the binomial
transform
\begin{equation}
  \begin{split}
    \mu_i = \sum_{k=0}^{i} {i \choose k} (-1)^{i-k}\mu_k'(\mu_1')^{i-k}.
  \end{split}
\end{equation}
Hence
\begin{equation}
  \begin{split}
    \mathrm{E}[p_y^b|D,m_b] 
    = P(y_b|D,m_B)
    = \frac{P(y_b,D|m_B)}{P(D|m_B)}
    = \frac{P(D'|m_B)}{P(D|m_B)},
  \end{split}
\end{equation}

\begin{equation}
  \begin{split}
    \mathrm{Var}[p_y^b|D,m_b]
    &= P(y_b,y_b|D,m_B) - (P(y_b|D,m_B))^2\\
    &= \frac{P(D''|m_B)}{P(D|m_B)} - \left(
    \frac{P(D'|m_B)}{P(D|m_B)} \right)^2
  \end{split}
\end{equation}
where $D' = D \cup \{y_b\}$.

\section*{Break Probabilities}
\begin{equation}
  \begin{split}
    \sum_{B\in \mathcal{I}^i_{m_B}(\mathcal{X})}P(B|D,m_B) &=
      \frac{\sum_{B\in
          \mathcal{I}^i_{m_B}(\mathcal{X})} P(D|B,m_B)\pi(B|m_B)}{\sum_{B\in
               \mathcal{P}_{m_B}(\mathcal{X})} P(D|B,m_B)\pi(B|m_B)}\\
      &= \frac{\sum_{B\in
          \mathcal{I}^i_{m_B}(\mathcal{X})} P(D|B,m_B)\pi(B|m_B)}{P(D|m_B)},
  \end{split}
\end{equation}
where $\mathcal{I}^i_{m_B}(\mathcal{X}) \subseteq
\mathcal{P}_{m_B}(\mathcal{X})$ denotes the set of multi-bins that
have a border at position $i$.

\section*{Model Posterior and Model Average}
\begin{equation}
  \begin{split}
    P(m_B|D)
    &= \frac{P(D|m_B)\pi(m_B)}{P(D)}\\
    &= \frac{P(D|m_B)\pi(m_B)}{\sum_{m_B\in M}P(D|m_B)\pi(m_B)},
  \end{split}
\end{equation}

\begin{equation}
  \begin{split}
    \mathrm{E}[p_y^b|D] 
    = \frac{\sum_{m_B\in M}P(D'|m_B)\pi(m_B)}{\sum_{m_B\in M}P(D|m_B)\pi(m_B)},
  \end{split}
\end{equation}
with $M = \{1,2,\dots,L\}$.

\section*{Bayesian Hypothesis Testing}
Bayes factor
\begin{equation}
  \begin{split}
    K = \frac{P(D|m_1)}{P(D|m_2)}
  \end{split}
\end{equation}

\section*{Entropy}
The uncertainty about which multi-bin model describes the data best is
given by the Shannon entropy
\begin{equation}
  \begin{split}
    \mathrm{H}(&\mathcal{P}(\mathcal{X})|D)\\
    &= -\sum_{B\in \mathcal{P}(\mathcal{X})}P(B|D) \log P(B|D)\\
    &= -\sum_{B\in \mathcal{P}(\mathcal{X})}\frac{P(D|B)\pi(B)}{P(D)}
    \log
    \frac{P(D|B)\pi(B)}{P(D)}\\
    &= -\sum_{B\in \mathcal{P}(\mathcal{X})}
    \frac{P(D|B)\pi(B)}{\sum_{B\in
        \mathcal{P}(\mathcal{X})}P(D|B)\pi(B)}
    \log
    \frac{P(D|B)\pi(B)}{\sum_{B\in
        \mathcal{P}(\mathcal{X})}P(D|B)\pi(B)}\\
    &=
    -\frac{1}{P(D)}
    \sum_{B\in \mathcal{P}(\mathcal{X})}
    \pi(B)
    \prod_{b\in
      B}\frac{\Beta(\vec{n}^b+\vec{\alpha}^b)}{\Beta(\vec{\alpha}^b)}\\
    &\quad\quad\quad
    \left[
      \log
      \frac{\pi(B)}{P(D)} +
      \sum_{b \in B}
      \log
      \frac{\Beta(\vec{n}^b+\vec{\alpha}^b)}{\Beta(\vec{\alpha}^b)}
    \right]\\
    &=
    \frac{1}{P(D)}
    \sum_{B\in \mathcal{P}(\mathcal{X})}
    \pi(B)
    \prod_{b\in B}\frac{\Beta(\vec{n}^b+\vec{\alpha}^b)}{\Beta(\vec{\alpha}^b)}
    \sum_{b \in B}
    \left[-\log
    \frac{\Beta(\vec{n}^b+\vec{\alpha}^b)}{\Beta(\vec{\alpha}^b)}\right]\\
    &-
    \frac{1}{P(D)}
    \sum_{B\in \mathcal{P}(\mathcal{X})}
    \pi(B)
    \log\frac{\pi(B)}{P(D)}
    \prod_{b\in B}
    \frac{\Beta(\vec{n}^b+\vec{\alpha}^b)}{\Beta(\vec{\alpha}^b)}
  \end{split}
\end{equation}

\section*{Implementation}
\begin{equation}
  \begin{split}
    \sum_{B\in \mathcal{P}(\mathcal{X})}
    g_{m_B}
    \prod_{b\in B}
    f(b)
  \end{split}
\end{equation}

\begin{equation}
  \begin{split}
    \sum_{B\in \mathcal{P}(\mathcal{X})}
    g_{m_B}
    \prod_{b\in B}
    f(b)
    \sum_{b \in B}
    h(b)
  \end{split}
\end{equation}

\begin{equation}
  \begin{split}
    \lim\limits_{\epsilon \rightarrow \infty}
    \frac{\exp[\epsilon\sum_i h_i]-1}{\epsilon}
    =
    \frac{\mathrm{d}}{\mathrm{d}\epsilon}
    \exp[\epsilon\sum_i h_i]\bigg|_{\epsilon=0}
    =
    \sum_ih_i
  \end{split}
\end{equation}

\section*{Estimation of Multinomial Probabilities}
If $m_T$ is the number of trials, then the estimated probabilities
of $p^b_y$ in bin $b$ are given by
\begin{equation}
  \begin{split}
    P(\vec{p}^b|D,m_T)
    &= \frac{P(D|\vec{p}^b,m_T)\pi(\vec{p}^b|m_T)}{P(D|m_T)}\\
    &=
    \frac{P(D|\vec{p}^b,m_T)\pi(\vec{p}^b|m_T)}{
      \int_{\Delta^K}
      P(D|\vec{p}^b,m_T)
      \pi(\vec{p}^b|m_T)
      \mathrm{d}\vec{p}^b}\\
    &=
    \Dir(\vec{p}^b; \vec{n}^b+\vec{\alpha}^b),
  \end{split}
\end{equation}
where $\pi(\vec{p}^b|m_T) = \Dir(\vec{p}^b; \vec{\alpha}^b)$. Hence
the expectation for $\vec{p}^b \sim \Dir(\vec{p}^b;
\vec{n}^b+\vec{\alpha}^b)$ is given by
\begin{equation}
  \begin{split}
    \hat{p}^b_y = \mathrm{E}[p^b_y|D,m_T]
    &= \frac{n^b_y + \alpha^b_y}{\sum_{y \in \mathcal{Y}} n^b_y + \alpha^b_y}\\
    &= \frac{n^b_y + \alpha^b_y}{m_T + \sum_{y^* \in \mathcal{Y}} \alpha^b_{y^*}},
  \end{split}
\end{equation}
which is also known as the \textit{extended Bayes-Laplace rule} and is
often used whenever only a small number of observations is
available. This estimate is equal to the frequentist's estimate,
i.e. $\mathrm{E}[p^b_y|D,m_T] = \frac{n^b_y}{m_T}$, if $\alpha^b_y = 0
~ \forall y \in \mathcal{Y}$. For $\alpha^b_y = 1 ~ \forall y \in
\mathcal{Y}$ we get \textit{Laplace's rule of succession}, which
prevents that prior probabilities of zero or one are used. In Bayesian
statistics a probability of zero or one should only be used for
logical statements, called \textit{Cromwell's rule}.

\begin{lemma}[The Proximal Multi-Bin Summation (ProMBS) Algorithm] \label{prombslemma}
%
Let $f:C(\Xc)\rightarrow\mathbb{R}$ be any function mapping
consecutive segments to the real numbers and
$\gf=(g_1,\ldots,g_K)\in\mathbb{R}^K$ be any real valued
vector. Define for $l=1,\dots,L$ the upper triangular matrices
$A_l=(a^l_{ij})_{L\times L}$ recursively by
%
$$
%
			a^1_{ij}
		=
	\begin{cases}
%
				f(b_{i,j}) 	& \text{ if } i\geq j	
			\\
				0  & \text{ otherwise }
%
	\end{cases}
% 
\text{  and  }
%
 \ \
		 a^{l+1}_{ij}
		=
	\begin{cases}
				a^l_{i+1,j+1}	& i,j < L 			
			\\
				1							& i=j=L
			\\
				0							& \text{else}
%
	\end{cases} \ .
% 
$$
%
The sum
$$
		S[f]
	=
		\sum_{B\in\Pc(\Xc)}
		g_{m_B}
		\prod_{b\in B}
		f(b)	
$$
%
can be turned out as follows: The first $N\leq L$ partial sums
$$
		S_m[f]
	=
		\sum_{B\in\mathcal{P}_m(\Xc)}
		\prod_{b\in B}
		f(b)
$$
%
can be evaluated by iterative matrix multiplications 
%
$$
		(0,\ldots,S_N[f],\ldots,S_1[f])
	=
		(1,0,\ldots,0)
	\prod_{m=1}^N
		A_m .
$$
%
Full evaluation of all partial sums delivers
%
$$
		S[f,\gf]
	=
		\sum_{m=1}^K g_m S_m[f] \ .
$$
%
\end{lemma}
%
\begin{lemma}[The extended ProMBS-Algorithm]\label{eprombslemma}
%
Consider the same conditions and definitions made in lemma
(\ref{prombslemma}). Let $h:C(\Xc)\rightarrow\mathbb{R}$ and
define for any $\epsilon>0$
%
$$
		SP_\epsilon[f,\gf,h]
	:=
			\frac{			
							S[f_\epsilon,\gf]
						-
							S[f,\gf]
					}{
						\epsilon
					}
			\ , ~ \text{where} ~
			f_\epsilon(b)
		:=
			f(b)e^{\epsilon h(b)} \ .
$$
%
For asymptotically small $\epsilon>0$, it holds that
$$
	\sum_{B\in\Pc(\Xc)}
		g_{m_B}
		\prod_{b\in B}
		f(b)	
			\sum_{b\in B}
				h(b)
	=
			SP_\epsilon[f,\gf,h] 
		+
		\Oc(\epsilon)		
			\ 
			.
$$
%
%
%
\end{lemma}

\section*{Todo}
\begin{itemize}
\item comparison of NSB method with binning to estimate entropies
\item numerically compute iso-entropic lines on the probability simplex
\end{itemize}