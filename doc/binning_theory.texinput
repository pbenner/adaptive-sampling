% -*- mode: latex -*-
%
\providecommand{\Mult}{\ensuremath{\mathrm{Mult}}}
\providecommand{\Dir}{\ensuremath{\mathrm{Dir}}}
\providecommand{\Beta}{\ensuremath{\mathrm{Beta}}}

\newcommand{\Yf}{\boldsymbol{Y}}
\newcommand{\Xf}{\boldsymbol{X}}
\newcommand{\pf}{\boldsymbol{p}}
\newcommand{\Df}{\boldsymbol{D}}
\newcommand{\Ef}{\boldsymbol{E}}
\newcommand{\xf}{\boldsymbol{x}}
\newcommand{\yf}{\boldsymbol{y}}
\newcommand{\zf}{\boldsymbol{z}}
\newcommand{\cf}{\boldsymbol{c}}
\newcommand{\gf}{\boldsymbol{g}}
\newcommand{\alphaf}{\boldsymbol{\alpha}}
\newcommand{\betaf}{\boldsymbol{\beta}}
\newcommand{\gammaf}{\boldsymbol{\gamma}}

\newcommand{\nf}{\boldsymbol{n}}

\newcommand{\xt}{\tilde{x}}
\newcommand{\yt}{\tilde{y}}

\newcommand{\Xc}{\mathcal{X}}
\newcommand{\Yc}{\mathcal{Y}}
\newcommand{\Fc}{\mathcal{F}}
\newcommand{\Gc}{\mathcal{G}}
\newcommand{\Hc}{\mathcal{H}}
\newcommand{\Oc}{\mathcal{O}}
\newcommand{\Pc}{\mathcal{P}}
\newcommand{\Cc}{\mathcal{C}}

\section*{Introduction}
\label{introduction}

\section*{Inference}
Given a sequence of $m_T$ observations $D = (X_1, X_2, \dots,
X_{m_T})$ how can we make predictions about future
observations\footnote{This is known as \textit{Hume's problem of
induction} and was answered by de Finetti's representation theorem.}?
That is, how can we formulate a hypothesis $\mathcal{H}$ about the
laws behind the sequence of observations and compute the probability
of our hypothesis conditional on all the evidence that we gathered?
From \textit{Bayes's rule of conditioning}
\begin{equation}
  P(\mathcal{H}|D) = \frac{P(D,\mathcal{H})}{P(D)}
\end{equation}
we know how to incorporate our observations into our probability
function, but to fully answer the question of induction we have to
clearly define the patterns or symmetries that we expect behing the
sequence of observations. This will lead us to a theorem by de Finetti
that tells us how the joint probability mass function for $P(D)$ can
be represented. A corresponding mode of inductive inference is then
given by the \textit{Johnson-Carnap continuum}.
\begin{definition}[Infinite exchangeability]
  Let $X_1, X_2, \dots$ denote an infinite sequence of random events,
  where each $X_i$ is a random variable from a probability space
  $(\Omega, \mathcal{F}, P)$ to a measure space $(\mathcal{X},
  \mathcal{F}')$. The sequence is said to be infinitely exchangeable
  with respect to the probability measure $P$ if for every subsequence
  of length $n$
  \begin{equation}
    P(X_1 = x_1, \dots, X_{m_T} = x_{m_T}) = P(X_{1} = x_{\pi(1)},
    \dots, X_{m_T} = x_{\pi(m_T)})
  \end{equation}
  with $x_i \in \mathcal{X}$ and for all permutations $\pi$ defined on
  the set $\{1, \dots, m_T\}$. (cf. \cite{Finetti1974})
\end{definition}

\begin{theorem}[Generalized de Finetti's representation theorem]
  \label{thr:frm}
  Let $X_1, X_2, \dots$ denote an infinitely exchangeable sequence of
  random variables from a probability space $(\Omega, \mathcal{F}, P)$
  to a measure space $(\mathcal{X}, \mathcal{F}')$. There exists a
  cumulative distribution $\Pi$ such that the joint mass function
  $p(x_1, \dots, x_{m_T}) = P(X_1 = x_1, \dots, X_{m_T} = x_{m_T})$ has
  the form
  \begin{equation}
    \begin{split}
      \label{eq:frt}
      p(x_1, \dots, x_{m_T}) &=
      \int_{\Delta^K} p(x_1, \dots, x_{m_T}|\vec{\theta})
      \mathrm{d} \Pi(\vec{\theta})\\
      &=
      \int_{\Delta^K} \Mult(\vec{n};\vec{\theta})
      \mathrm{d} \Pi(\vec{\theta})\\
      &=
      \int_{\Delta^K}
      \frac{m_T!}{\prod_{x \in \mathcal{X}}n_x!}
      \prod_{x \in \mathcal{X}}
      \theta_x^{n_x}
      \mathrm{d} \Pi(\vec{\theta}), ~ \text{where}\\
      \Pi(\vec{\theta}) &= \lim\limits_{m_T \rightarrow \infty}
      P \left\{\frac{n_x}{m_T} \le \theta_x : x \in \mathcal{X} \right\},
    \end{split}
  \end{equation}
  where $n_x$ denotes the count statistic for $x \in \mathcal{X}$, $K
  = |\mathcal{X}|$, and $\Delta^K := \{\vec{\theta} = (\theta_x)_{x \in
    \mathcal{X}} \in \mathbb{R}^K : \sum_{x \in \mathcal{X}} \theta_x = 1
  ~ \text{and} ~ \theta_x \ge 0~\forall x \in \mathcal{X} \}$ denotes
  the $K$-dimensional probability simplex. (cf. \cite{Finetti1974,
Hewitt1955})
\end{theorem}
In other words, conditional on $\vec{\theta}$ the joint probability
mass function $p(x_1, \dots, x_{m_T})$ can be written as an integral
mixture of multinomial probabilities. Note that for $K=1$ equation
$\ref{eq:frt}$ computes the $n_x$th moment of $\mathrm{d}\Pi$. In
Bayesian inference, the limiting distribution $\mathrm{d}\Pi$ is
interpreted as the prior distribution. A common choice for
$\mathrm{d}\Pi$ is a conjugate prior which substantially simplifies
the computation of the posterior distribution. In the following we
show how such a prior can be justified from inductive principles such
that theorem \ref{thr:frm} holds.

\begin{theorem}[Johnson-Carnap continuum]
  \label{thr:jcc}
  Let $X_1, X_2, \dots$ denote an infinite sequence of random
  variables from a probability space $(\Omega, \mathcal{F}, P)$ to a
  measure space $(\mathcal{X}, \mathcal{F}')$. Assume that
  \begin{itemize}
  \item[(i)  ] $K = |\mathcal{X}| \ge 3$,
  \item[(ii) ] $P(X_1 = x_1, \dots, X_{m_T} = x_{m_T}) > 0$ for all $x_i \in
    \mathcal{X}$, and
  \item[(iii)] there exists a sufficient statistic $f_x(n_x,n)$ such
    that $P(X_{m_T+1} = x_{m_T+1} | \vec{n}) = f_x(n_x,m_T)$, where $\vec{n} =
    (n_x)_{x \in \mathcal{X}}$.
  \end{itemize}
  Then either the outcomes are independent or there exist positive
  constants $\{\alpha_x | x \in \mathcal{X}\}$ such that for all $n \ge 1$
  \begin{equation}
    f_x(n_x, n) = \frac{n_x + \alpha_x}{m_T + \sum_{x^* \in \mathcal{X}}
      \alpha_{x^*}}.
  \end{equation}
  (cf. \cite{Johnson1932, Zabell1982})
\end{theorem}
Note that if $X_1, X_2, \dots$ is exchangeable then $P(X_{m_T+1} =
x_{m_T+1} | \vec{n}) = P(X_{m_T+1} = x_{m_T+1} | n_x) = f_x(n_x,m_T)$. The
Johnson-Carnap continuum of inductive methods not only provides us
with a sufficient statistic for inductive reasoning, but also
identifies a unique prior distribution for de Finetti's representation
theorem.
\begin{corollary}
  Under the conditions (i-iii) of theorem \ref{thr:jcc} the prior
  distribution $\mathrm{d}\Pi$ belongs to the \textit{Dirichlet
    familiy}, i.e.
  \begin{equation}
    \begin{split}
      \mathrm{d}\Pi(\vec{\theta}) =
      \Dir(\vec{\theta};\vec{\alpha})\mathrm{d}\vec{\theta} &=
      \frac{1}{\Beta(\vec{\alpha})}
      \prod_{x \in \mathcal{X}} \theta_x^{\alpha_x -1}
      \mathrm{d}\vec{\theta}, ~ \text{where}\\
      \Beta(\vec{\alpha}) &= \frac{\prod_{x \in \mathcal{X}}
        \Gamma(\alpha_x)}{\Gamma(\sum_{x
          \in \mathcal{X}}\alpha_x)} =
      \underbrace{\int_{\Delta^K}\prod_{x \in
          \mathcal{X}} \theta^{\alpha_x-1}
        \mathrm{d}\vec{\theta},}_{\text{Euler
          integral of the first kind}}
    \end{split}
  \end{equation}
  and $\Gamma(z) = \int_0^\infty t^{z-1}e^{-t}\mathrm{d}t$ denotes
  the Gamma function.
\end{corollary}
To summarize our results, under the symmetry assumptions that we
introduced about the sequence of observations $D = (X_1, X_2, \dots,
X_{m_T})$ we are able to state a hypothesis $\mathcal{H}$ in form of the
parameters $\vec{\theta}$ that represent our beliefs about the
underlying process. Any knowledge that we have before we look at the
data is represented in the parameters $\vec{\alpha}$. The probability
of a hypothesis is then given by
\begin{equation}
  \begin{split}
    P(\vec{\theta}|D)
    &= \frac{P(D,\vec{\theta})}{P(D)}
     =
    \frac{\Mult(D;\vec{\theta})\Dir(\vec{\theta};\vec{\alpha})}{
      \int_{\Delta^K}
      \Mult(D;\vec{\theta}')
      \mathrm{d}\Pi(\vec{\theta}')}\\
    &=
    \Dir(\vec{\theta}; \vec{n}+\vec{\alpha}).
  \end{split}
\end{equation}
The update of our beliefs after seeing the data is therefore a mere
shift in the parameters of the Dirichlet distribution
(i.e. $\Dir(\vec{\theta};\vec{\alpha}) \mapsto \Dir(\vec{\theta};
\vec{n}+\vec{\alpha})$) as it is a conjugate prior to the multinomial
distribution. Intuitively, the Dirichlet distribution states our
belief that the probabilities of $K$ rival events $x \in \mathcal{X}$
are $(\theta_x)_{x \in \mathcal{X}}$ given that each event has been
observed $\alpha_x - 1$ times. Since $\vec{\theta} \sim
\Dir(\vec{\theta}; \vec{n}+\vec{\alpha})$ we can easily computed the
expected value of $\vec{\theta}$ given our observations, i.e.
\begin{equation}
  \begin{split}
    \hat{\theta}_x = \mathrm{E}[\theta_x|D] &=
    \int_{\Delta^{K-1}}\theta_x \Mult(D;\vec{\theta})
    \mathrm{d}\Pi(\vec{\theta}) =
    \frac{
      \int_{\Delta^K} \Mult(D \cup \{x\};\vec{\theta})
      \mathrm{d}\Pi(\vec{\theta})
    }{
      \int_{\Delta^K} \Mult(D;\vec{\theta})
      \mathrm{d}\Pi(\vec{\theta})
    }\\
    &= f_x(n_x,m_T)
    = \frac{n_x + \alpha_x}{n + \sum_{x^* \in \mathcal{X}} \alpha_{x^*}},
  \end{split}
\end{equation}
as it was shown in theorem \ref{thr:jcc}. The estimate is known as the
\textit{extended Bayes-Laplace rule of succession}. It is equivalent
to the frequentist's estimate ($\hat{\theta}_x = \frac{n_x}{m_T}$) if
$\alpha_x = 0 ~ \forall x \in \mathcal{X}$. If $\alpha_x = 1 ~ \forall
x \in \mathcal{X}$ we get \textit{Laplace's rule of succession}.

\section*{Inductive inference for piecewise constant distributions}
We will now generalize the results of the previous section. 
\begin{equation}
  \begin{split}
    P(\vec{p},B|D,m_B) &= \frac{P(D,\vec{p},B|m_B)}{P(D|m_B)}\\
                       &= \frac{P(D|\vec{p},B,m_B)\pi(\vec{p},B|m_B)}{P(D|m_B)}
  \end{split}
\end{equation}
$\vec{p} = (p_y^b)_{K\times m_B}$, $m_B = |B|$, $\mathcal{X} = \{x_1, x_2, \dots, x_L\}$, $\mathcal{Y} =
\{y_1, y_2, \dots, y_K\}$ (discretized into $L$ intervals of equal
size) and $m_T$ is the number of trials.

Todo: De Finetti, n-exchangeability, from which it follows that the
likelihood function is given by the multinomial likelihood
\begin{equation}
  P(D|\vec{p},B,m_B) = \prod_{b\in B} \prod_{y\in \mathcal{Y}}(p_y^b)^{n^b_y},
\end{equation}
where $n^b_y$ denotes the count statistic for event $y$ in bin
$b$. The conjugate prior for a multinomial likelihood function is
given by the Dirichlet distribution $\Dir(\vec{p}^b;\vec{\alpha}^b)$, which
states the belief that the probabilities of $K$ rival events in bin
$b$ are $p_y^b$ given that each event has been observed $\alpha^b_y - 1$
times. The choice of this prior can be justified by \textit{Johnson's
  sufficiency postulate}. Hence
\begin{equation}
  P(\vec{p},B|m_B) = \prod_{b\in B}\frac{1}{\Beta(\vec{\alpha}^b)}
  \prod_{y\in \mathcal{Y}}(p_y^b)^{\alpha^b_y-1},
\end{equation}

\begin{equation}
  \Beta(z_1,z_2,\dots,z_m) =
  \frac{\prod_{i=1}^{m}\Gamma(z_i)}{\Gamma(\sum_{i=1}^{m}z_i)},~
  \text{with}~
  \Gamma(z) = \int_0^\infty t^{z-1}e^{-t}\mathrm{d}t
\end{equation}

For $\pi(\vec{p},B|m_B)$ a non-informative prior assumption is made, i.e.
\begin{equation}
  \pi(\vec{p},B|m_B) = \pi(\vec{p}|m_B)\pi(B|m_B),
\end{equation}
which means that the prior knowledge about $\vec{p}$ is independent of
$B$ and vice versa.

A non-informative prior is chosen for the multi-bins
\begin{equation}
  \pi(B|m_B) = {L \choose m_B}^{-1}
\end{equation}
Define $\pi(\vec{p}|m_B)$!

\section*{Evidence}
\begin{equation}
  \begin{split}
    P(D|m_B)
    &= \sum_{B\in \mathcal{P}_{m_B}(\mathcal{X})}
      P(D|B,m_B) \pi(B|m_B)\\
  \end{split}
\end{equation}
\begin{equation}
  \begin{split}
    P(D|B,m_B)
    &=
      \int_{\underbrace{\Delta^K \times \Delta^K \times \dots \times
          \Delta^K}_{m_B~\text{times}}}
      P(D|\vec{p},B,m_B) \pi(B|m_B)\\
    &=
      \int_{\Delta^K \times \Delta^K \times \dots \times \Delta^K}\mathrm{d}\vec{p}
      \prod_{b\in B}\prod_{y\in \mathcal{Y}}(p_y^b)^{n^b_y}
      \prod_{b\in B}\frac{1}{\Beta(\vec{\alpha}^b)}
      \prod_{y\in \mathcal{Y}}(p_y^b)^{\alpha^b_y-1}\\
    &=
      \int_{\Delta^K \times \Delta^K \times \dots \times \Delta^K}\mathrm{d}\vec{p}
      \prod_{b\in B}\frac{1}{\Beta(\vec{\alpha}^b)}
      \prod_{y\in \mathcal{Y}}(p_y^b)^{n^b_y+\alpha^b_y-1},
  \end{split}
\end{equation}
where $\Delta^K := \{(p_1,p_2,\dots,p_K)\in\mathbb{R} : \sum_{i=1}^{K}
p_i = 1 ~ \text{and} ~ p_i \ge 0~\forall i\}$ denotes the
$K$-dimensional probability simplex. The solution of the Euler
integral of the first kind is given by the multinomial Beta-function.

\begin{equation}
  \begin{split}
    \Beta(\vec{z}) =
    \underbrace{\int_{\Delta^K}\prod_{z\in\mathcal{Z}}t^{z-1}\mathrm{d}t}_{\text{Euler
      integral of the first kind}}=
    \underbrace{\frac{\prod_{z\in\mathcal{Z}}\Gamma(z)}{\Gamma(\sum_{z\in\mathcal{Z}}z)}}_{\text{multinomial
      Beta function}}
  \end{split}
\end{equation}

Hence
\begin{equation}
  \begin{split}
    P(D|m_B)
    &= \sum_{B\in \mathcal{P}_{m_B}(\mathcal{X})}
    \pi(B|m_B)
    \prod_{b\in B}\Beta(\vec{\alpha}^b)^{-1}
    \frac{\prod_{y\in
        \mathcal{Y}}\Gamma(n_y^b+\alpha^b_y)}{\Gamma(\sum_{y\in
        \mathcal{Y}}n_y^b+\alpha^b_y)}\\
    &=  \sum_{B\in \mathcal{P}_{m_B}(\mathcal{X})}
    \pi(B|m_B)
    \prod_{b\in B}\frac{\Beta(\vec{n}^b+\vec{\alpha}^b)}{\Beta(\vec{\alpha}^b)}
  \end{split}
\end{equation}

\section*{Expectation and Variance}
To characterize the inferred distribution, we will compute the first
three moments from the moment generating function
\begin{equation}
  \begin{split}
    \mu_i' = \int x^i\mathrm{d}P(x) = \frac{P(D^i|m_B)}{P(D|m_B)},
  \end{split}
\end{equation}
where $P(x)$ denotes the Lebesgue-Stieltjes probability density
function and $\mu_i'$ the $i$th raw moment. The central moments
$\mu_i$ are given by
\begin{equation}
  \begin{split}
    \mu_i = \int (x-\mu_1')^i\mathrm{d}P(x),
  \end{split}
\end{equation}
which can be computed from the raw moments by using the binomial
transform
\begin{equation}
  \begin{split}
    \mu_i = \sum_{k=0}^{i} {i \choose k} (-1)^{i-k}\mu_k'(\mu_1')^{i-k}.
  \end{split}
\end{equation}
Hence
\begin{equation}
  \begin{split}
    \mathrm{E}[p_y^b|D,m_b] 
    = P(y_b|D,m_B)
    = \frac{P(y_b,D|m_B)}{P(D|m_B)}
    = \frac{P(D'|m_B)}{P(D|m_B)},
  \end{split}
\end{equation}

\begin{equation}
  \begin{split}
    \mathrm{Var}[p_y^b|D,m_b]
    &= P(y_b,y_b|D,m_B) - (P(y_b|D,m_B))^2\\
    &= \frac{P(D''|m_B)}{P(D|m_B)} - \left(
    \frac{P(D'|m_B)}{P(D|m_B)} \right)^2
  \end{split}
\end{equation}
where $D' = D \cup \{y_b\}$.

\section*{Break Probabilities}
\begin{equation}
  \begin{split}
    \sum_{B\in \mathcal{I}^i_{m_B}(\mathcal{X})}P(B|D,m_B) &=
      \frac{\sum_{B\in
          \mathcal{I}^i_{m_B}(\mathcal{X})} P(D|B,m_B)\pi(B|m_B)}{\sum_{B\in
               \mathcal{P}_{m_B}(\mathcal{X})} P(D|B,m_B)\pi(B|m_B)}\\
      &= \frac{\sum_{B\in
          \mathcal{I}^i_{m_B}(\mathcal{X})} P(D|B,m_B)\pi(B|m_B)}{P(D|m_B)},
  \end{split}
\end{equation}
where $\mathcal{I}^i_{m_B}(\mathcal{X}) \subseteq
\mathcal{P}_{m_B}(\mathcal{X})$ denotes the set of multi-bins that
have a border at position $i$.

\section*{Model Posterior and Model Average}
\begin{equation}
  \begin{split}
    P(m_B|D)
    &= \frac{P(D|m_B)\pi(m_B)}{P(D)}\\
    &= \frac{P(D|m_B)\pi(m_B)}{\sum_{m_B\in M}P(D|m_B)\pi(m_B)},
  \end{split}
\end{equation}

\begin{equation}
  \begin{split}
    \mathrm{E}[p_y^b|D] 
    = \frac{\sum_{m_B\in M}P(D'|m_B)\pi(m_B)}{\sum_{m_B\in M}P(D|m_B)\pi(m_B)},
  \end{split}
\end{equation}
with $M = \{1,2,\dots,L\}$.

\section*{Bayesian Hypothesis Testing}
Bayes factor
\begin{equation}
  \begin{split}
    K = \frac{P(D|m_1)}{P(D|m_2)}
  \end{split}
\end{equation}

\section*{Entropy}
The uncertainty about which multi-bin model describes the data best is
given by the Shannon entropy
\begin{equation}
  \begin{split}
    \mathrm{H}(&\mathcal{P}(\mathcal{X})|D)\\
    &= -\sum_{B\in \mathcal{P}(\mathcal{X})}P(B|D) \log P(B|D)\\
    &= -\sum_{B\in \mathcal{P}(\mathcal{X})}\frac{P(D|B)\pi(B)}{P(D)}
    \log
    \frac{P(D|B)\pi(B)}{P(D)}\\
    &= -\sum_{B\in \mathcal{P}(\mathcal{X})}
    \frac{P(D|B)\pi(B)}{\sum_{B\in
        \mathcal{P}(\mathcal{X})}P(D|B)\pi(B)}
    \log
    \frac{P(D|B)\pi(B)}{\sum_{B\in
        \mathcal{P}(\mathcal{X})}P(D|B)\pi(B)}\\
    &=
    -\frac{1}{P(D)}
    \sum_{B\in \mathcal{P}(\mathcal{X})}
    \pi(B)
    \prod_{b\in
      B}\frac{\Beta(\vec{n}^b+\vec{\alpha}^b)}{\Beta(\vec{\alpha}^b)}\\
    &\quad\quad\quad
    \left[
      \log
      \frac{\pi(B)}{P(D)} +
      \sum_{b \in B}
      \log
      \frac{\Beta(\vec{n}^b+\vec{\alpha}^b)}{\Beta(\vec{\alpha}^b)}
    \right]\\
    &=
    \frac{1}{P(D)}
    \sum_{B\in \mathcal{P}(\mathcal{X})}
    \pi(B)
    \prod_{b\in B}\frac{\Beta(\vec{n}^b+\vec{\alpha}^b)}{\Beta(\vec{\alpha}^b)}
    \sum_{b \in B}
    \left[-\log
    \frac{\Beta(\vec{n}^b+\vec{\alpha}^b)}{\Beta(\vec{\alpha}^b)}\right]\\
    &-
    \frac{1}{P(D)}
    \sum_{B\in \mathcal{P}(\mathcal{X})}
    \pi(B)
    \log\frac{\pi(B)}{P(D)}
    \prod_{b\in B}
    \frac{\Beta(\vec{n}^b+\vec{\alpha}^b)}{\Beta(\vec{\alpha}^b)}
  \end{split}
\end{equation}

\section*{Implementation}
\begin{equation}
  \begin{split}
    \sum_{B\in \mathcal{P}(\mathcal{X})}
    g_{m_B}
    \prod_{b\in B}
    f(b)
  \end{split}
\end{equation}

\begin{equation}
  \begin{split}
    \sum_{B\in \mathcal{P}(\mathcal{X})}
    g_{m_B}
    \prod_{b\in B}
    f(b)
    \sum_{b \in B}
    h(b)
  \end{split}
\end{equation}

\begin{equation}
  \begin{split}
    \lim\limits_{\epsilon \rightarrow \infty}
    \frac{\exp[\epsilon\sum_i h_i]-1}{\epsilon}
    =
    \frac{\mathrm{d}}{\mathrm{d}\epsilon}
    \exp[\epsilon\sum_i h_i]\bigg|_{\epsilon=0}
    =
    \sum_ih_i
  \end{split}
\end{equation}

\section*{Estimation of Multinomial Probabilities}
If $m_T$ is the number of trials, then the estimated probabilities
of $p^b_y$ in bin $b$ are given by
\begin{equation}
  \begin{split}
    P(\vec{p}^b|D,m_T)
    &= \frac{P(D|\vec{p}^b,m_T)\pi(\vec{p}^b|m_T)}{P(D|m_T)}\\
    &=
    \frac{P(D|\vec{p}^b,m_T)\pi(\vec{p}^b|m_T)}{
      \int_{\Delta^K}
      P(D|\vec{p}^b,m_T)
      \pi(\vec{p}^b|m_T)
      \mathrm{d}\vec{p}^b}\\
    &=
    \Dir(\vec{p}^b; \vec{n}^b+\vec{\alpha}^b),
  \end{split}
\end{equation}
where $\pi(\vec{p}^b|m_T) = \Dir(\vec{p}^b; \vec{\alpha}^b)$. Hence
the expectation for $\vec{p}^b \sim \Dir(\vec{p}^b;
\vec{n}^b+\vec{\alpha}^b)$ is given by
\begin{equation}
  \begin{split}
    \hat{p}^b_y = \mathrm{E}[p^b_y|D,m_T]
    &= \frac{n^b_y + \alpha^b_y}{\sum_{y \in \mathcal{Y}} n^b_y + \alpha^b_y}\\
    &= \frac{n^b_y + \alpha^b_y}{m_T + \sum_{y^* \in \mathcal{Y}} \alpha^b_{y^*}},
  \end{split}
\end{equation}
which is also known as the \textit{extended Bayes-Laplace rule} and is
often used whenever only a small number of observations is
available. This estimate is equal to the frequentist's estimate,
i.e. $\mathrm{E}[p^b_y|D,m_T] = \frac{n^b_y}{m_T}$, if $\alpha^b_y = 0
~ \forall y \in \mathcal{Y}$. For $\alpha^b_y = 1 ~ \forall y \in
\mathcal{Y}$ we get \textit{Laplace's rule of succession}, which
prevents that prior probabilities of zero or one are used. In Bayesian
statistics a probability of zero or one should only be used for
logical statements, called \textit{Cromwell's rule}.

\begin{lemma}[The Proximal Multi-Bin Summation (ProMBS) Algorithm] \label{prombslemma}
%
Let $f:C(\Xc)\rightarrow\mathbb{R}$ be any function mapping
consecutive segments to the real numbers and
$\gf=(g_1,\ldots,g_K)\in\mathbb{R}^K$ be any real valued
vector. Define for $l=1,\dots,L$ the upper triangular matrices
$A_l=(a^l_{ij})_{L\times L}$ recursively by
%
$$
%
			a^1_{ij}
		=
	\begin{cases}
%
				f(b_{i,j}) 	& \text{ if } i\geq j	
			\\
				0  & \text{ otherwise }
%
	\end{cases}
% 
\text{  and  }
%
 \ \
		 a^{l+1}_{ij}
		=
	\begin{cases}
				a^l_{i+1,j+1}	& i,j < L 			
			\\
				1							& i=j=L
			\\
				0							& \text{else}
%
	\end{cases} \ .
% 
$$
%
The sum
$$
		S[f]
	=
		\sum_{B\in\Pc(\Xc)}
		g_{m_B}
		\prod_{b\in B}
		f(b)	
$$
%
can be turned out as follows: The first $N\leq L$ partial sums
$$
		S_m[f]
	=
		\sum_{B\in\mathcal{P}_m(\Xc)}
		\prod_{b\in B}
		f(b)
$$
%
can be evaluated by iterative matrix multiplications 
%
$$
		(0,\ldots,S_N[f],\ldots,S_1[f])
	=
		(1,0,\ldots,0)
	\prod_{m=1}^N
		A_m .
$$
%
Full evaluation of all partial sums delivers
%
$$
		S[f,\gf]
	=
		\sum_{m=1}^K g_m S_m[f] \ .
$$
%
\end{lemma}
%
\begin{lemma}[The extended ProMBS-Algorithm]\label{eprombslemma}
%
Consider the same conditions and definitions made in lemma
(\ref{prombslemma}). Let $h:C(\Xc)\rightarrow\mathbb{R}$ and
define for any $\epsilon>0$
%
$$
		SP_\epsilon[f,\gf,h]
	:=
			\frac{			
							S[f_\epsilon,\gf]
						-
							S[f,\gf]
					}{
						\epsilon
					}
			\ , ~ \text{where} ~
			f_\epsilon(b)
		:=
			f(b)e^{\epsilon h(b)} \ .
$$
%
For asymptotically small $\epsilon>0$, it holds that
$$
	\sum_{B\in\Pc(\Xc)}
		g_{m_B}
		\prod_{b\in B}
		f(b)	
			\sum_{b\in B}
				h(b)
	=
			SP_\epsilon[f,\gf,h] 
		+
		\Oc(\epsilon)		
			\ 
			.
$$
%
%
%
\end{lemma}
