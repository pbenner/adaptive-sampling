% -*- mode: latex -*-
%
\providecommand{\Dir}{\ensuremath{\mathrm{Dir}}}
\providecommand{\Beta}{\ensuremath{\mathrm{Beta}}}

\section*{Introduction}
\label{introduction}
\begin{equation}
  \begin{split}
    P(\vec{p},B|D,m_B) &= \frac{P(D,\vec{p},B|m_B)}{P(D|m_B)}\\
                       &= \frac{P(D|\vec{p},B,m_B)\pi(\vec{p},B|m_B)}{P(D|m_B)}
  \end{split}
\end{equation}
$\vec{p} = (p_y^b)_{K\times m_B}$, $m_B = |B|$, $\mathcal{X} = \{x_1, x_2, \dots, x_L\}$, $\mathcal{Y} =
\{y_1, y_2, \dots, y_K\}$ (discretized into $L$ intervals of equal
size) and $m_T$ is the number of trials.

\section*{Assumptions and Prior Knowledge}
Todo: De Finetti, n-exchangeability, from which it follows that the
likelihood function is given by the multinomial likelihood
\begin{equation}
  P(D|\vec{p},B,m_B) = \prod_{b\in B} \prod_{y\in \mathcal{Y}}(p_y^b)^{n^b_y},
\end{equation}
where $n^b_y$ denotes the count statistic for event $y$ in bin
$b$. The conjugate prior for a multinomial likelihood function is
given by the Dirichlet distribution $\Dir(\vec{p}^b;\vec{\alpha}^b)$, which
states the belief that the probabilities of $K$ rival events in bin
$b$ are $p_y^b$ given that each event has been observed $\alpha^b_y - 1$
times. The choice of this prior can be justified by \textit{Johnson's
  sufficiency postulate}. Hence
\begin{equation}
  P(\vec{p},B|m_B) = \prod_{b\in B}\frac{1}{\Beta(\vec{\alpha}^b)}
  \prod_{y\in \mathcal{Y}}(p_y^b)^{\alpha^b_y-1},
\end{equation}

\begin{equation}
  \Beta(z_1,z_2,\dots,z_m) =
  \frac{\prod_{i=1}^{m}\Gamma(z_i)}{\Gamma(\sum_{i=1}^{m}z_i)},~
  \text{with}~
  \Gamma(z) = \int_0^\infty t^{z-1}e^{-t}\mathrm{d}t
\end{equation}

For $\pi(\vec{p},B|m_B)$ a non-informative prior assumption is made, i.e.
\begin{equation}
  \pi(\vec{p},B|m_B) = \pi(\vec{p}|m_B)\pi(B|m_B),
\end{equation}
which means that the prior knowledge about $\vec{p}$ is independent of
$B$ and vice versa.

A non-informative prior is chosen for the multi-bins
\begin{equation}
  \pi(B|m_B) = {L \choose m_B}^{-1}
\end{equation}

\section*{Evidence}
\begin{equation}
  \begin{split}
    P(D|m_B)
    &= \sum_{B\in \mathcal{P}_{m_B}(\mathcal{X})}
      P(D|B,m_B) \pi(B|m_B)\\
  \end{split}
\end{equation}
\begin{equation}
  \begin{split}
    P(D|B,m_B)
    &=
      \int_{\underbrace{\Delta^K \times \Delta^K \times \dots \times
          \Delta^K}_{m_B~\text{times}}}\mathrm{d}\vec{p}
      P(D|\vec{p},B,m_B) \pi(\vec{p}|m_B)\\
    &=
      \int_{\Delta^K \times \Delta^K \times \dots \times \Delta^K}\mathrm{d}\vec{p}
      \prod_{b\in B}\prod_{y\in \mathcal{Y}}(p_y^b)^{n^b_y}
      \prod_{b\in B}\frac{1}{\Beta(\vec{\alpha}^b)}
      \prod_{y\in \mathcal{Y}}(p_y^b)^{\alpha^b_y-1}\\
    &=
      \int_{\Delta^K \times \Delta^K \times \dots \times \Delta^K}\mathrm{d}\vec{p}
      \prod_{b\in B}\frac{1}{\Beta(\vec{\alpha}^b)}
      \prod_{y\in \mathcal{Y}}(p_y^b)^{n^b_y+\alpha^b_y-1},
  \end{split}
\end{equation}
where $\Delta^K := \{(p_1,p_2,\dots,p_K)\in\mathbb{R} : \sum_{i=1}^{K}
p_i = 1 ~ \text{and} ~ p_i \ge 0~\forall i\}$ denotes the
$K$-dimensional probability simplex. The solution of the Euler
integral of the first kind is given by the multinomial Beta-function.

\begin{equation}
  \begin{split}
    \Beta(\vec{z}) =
    \underbrace{\int_{\Delta^K}\prod_{z\in\mathcal{Z}}t^{z-1}\mathrm{d}t}_{\text{Euler
      integral of the first kind}}=
    \underbrace{\frac{\prod_{z\in\mathcal{Z}}\Gamma(z)}{\Gamma(\sum_{z\in\mathcal{Z}}z)}}_{\text{multinomial
      Beta function}}
  \end{split}
\end{equation}

Hence
\begin{equation}
  \begin{split}
    P(D|m_B)
    &= \sum_{B\in \mathcal{P}_{m_B}(\mathcal{X})}
    \pi(B|m_B)
    \prod_{b\in B}\Beta(\vec{\alpha}^b)^{-1}
    \frac{\prod_{y\in
        \mathcal{Y}}\Gamma(n_y^b+\alpha^b_y)}{\Gamma(\sum_{y\in
        \mathcal{Y}}n_y^b+\alpha^b_y)}\\
    &=  \sum_{B\in \mathcal{P}_{m_B}(\mathcal{X})}
    \pi(B|m_B)
    \prod_{b\in B}\frac{\Beta(\vec{n}^b+\vec{\alpha}^b)}{\Beta(\vec{\alpha}^b)}
  \end{split}
\end{equation}

\section*{Expectation and Variance}
\begin{equation}
  \begin{split}
    \mathrm{E}[p_y^b|D,m_b] 
    = P(y_b|D,m_B)
    = \frac{P(y_b,D|m_B)}{P(D|m_B)}
    = \frac{P(D'|m_B)}{P(D|m_B)},
  \end{split}
\end{equation}

\begin{equation}
  \begin{split}
    \mathrm{Var}[p_y^b|D,m_b]
    &= P(y_b,y_b|D,m_B) - (P(y_b|D,m_B))^2\\
    &= \frac{P(D''|m_B)}{P(D|m_B)} - \left(
    \frac{P(D'|m_B)}{P(D|m_B)} \right)^2
  \end{split}
\end{equation}
where $D' = D \cup \{y_b\}$.

\section*{Break Probabilities}
\begin{equation}
  \begin{split}
    \sum_{B\in \mathcal{I}^i_{m_B}(\mathcal{X})}P(B|D,m_B) &=
      \frac{\sum_{B\in
          \mathcal{I}^i_{m_B}(\mathcal{X})} P(D|B,m_B)\pi(B|m_B)}{\sum_{B\in
               \mathcal{P}_{m_B}(\mathcal{X})} P(D|B,m_B)\pi(B|m_B)}\\
      &= \frac{\sum_{B\in
          \mathcal{I}^i_{m_B}(\mathcal{X})} P(D|B,m_B)\pi(B|m_B)}{P(D|m_B)},
  \end{split}
\end{equation}
where $\mathcal{I}^i_{m_B}(\mathcal{X}) \subseteq
\mathcal{P}_{m_B}(\mathcal{X})$ denotes the set of multi-bins that
have a border at position $i$.

\section*{Model Posterior and Model Average}
\begin{equation}
  \begin{split}
    P(m_B|D)
    &= \frac{P(D|m_B)\pi(m_B)}{P(D)}\\
    &= \frac{P(D|m_B)\pi(m_B)}{\sum_{m_B\in M}P(D|m_B)\pi(m_B)},
  \end{split}
\end{equation}

\begin{equation}
  \begin{split}
    \mathrm{E}[p_y^b|D] 
    = \frac{\sum_{m_B\in M}P(D'|m_B)\pi(m_B)}{\sum_{m_B\in M}P(D|m_B)\pi(m_B)},
  \end{split}
\end{equation}
with $M = \{1,2,\dots,L\}$.

\section*{Bayesian Hypothesis Testing}
Bayes factor
\begin{equation}
  \begin{split}
    K = \frac{P(D|m_1)}{P(D|m_2)}
  \end{split}
\end{equation}

\section*{Entropy}
The uncertainty about which multi-bin model describes the data best is
given by the Shannon entropy
\begin{equation}
  \begin{split}
    \mathrm{H}(&\mathcal{P}(\mathcal{X})|D)\\
    &= -\sum_{B\in \mathcal{P}(\mathcal{X})}P(B|D) \log P(B|D)\\
    &= -\sum_{B\in \mathcal{P}(\mathcal{X})}\frac{P(D|B)\pi(B)}{P(D)}
    \log
    \frac{P(D|B)\pi(B)}{P(D)}\\
    &= -\sum_{B\in \mathcal{P}(\mathcal{X})}
    \frac{P(D|B)\pi(B)}{\sum_{B\in
        \mathcal{P}(\mathcal{X})}P(D|B)\pi(B)}
    \log
    \frac{P(D|B)\pi(B)}{\sum_{B\in
        \mathcal{P}(\mathcal{X})}P(D|B)\pi(B)}\\
    &=
    -\frac{1}{P(D)}
    \sum_{B\in \mathcal{P}(\mathcal{X})}
    \pi(B)
    \prod_{b\in
      B}\frac{\Beta(\vec{n}^b+\vec{\alpha}^b)}{\Beta(\vec{\alpha}^b)}\\
    &\quad\quad\quad
    \left[
      \log
      \frac{\pi(B)}{P(D)} +
      \sum_{b \in B}
      \log
      \frac{\Beta(\vec{n}^b+\vec{\alpha}^b)}{\Beta(\vec{\alpha}^b)}
    \right]\\
    &=
    \frac{1}{P(D)}
    \sum_{B\in \mathcal{P}(\mathcal{X})}
    \pi(B)
    \prod_{b\in B}\frac{\Beta(\vec{n}^b+\vec{\alpha}^b)}{\Beta(\vec{\alpha}^b)}
    \sum_{b \in B}
    \left[-\log
    \frac{\Beta(\vec{n}^b+\vec{\alpha}^b)}{\Beta(\vec{\alpha}^b)}\right]\\
    &-
    \frac{1}{P(D)}
    \sum_{B\in \mathcal{P}(\mathcal{X})}
    \pi(B)
    \log\frac{\pi(B)}{P(D)}
    \prod_{b\in B}
    \frac{\Beta(\vec{n}^b+\vec{\alpha}^b)}{\Beta(\vec{\alpha}^b)}
  \end{split}
\end{equation}

\section*{Implementation}
\begin{equation}
  \begin{split}
    \sum_{B\in \mathcal{P}(\mathcal{X})}
    g_{m_B}
    \prod_{b\in B}
    f(b)
  \end{split}
\end{equation}

\begin{equation}
  \begin{split}
    \sum_{B\in \mathcal{P}(\mathcal{X})}
    g_{m_B}
    \prod_{b\in B}
    f(b)
    \sum_{b \in B}
    h(b)
  \end{split}
\end{equation}

\begin{equation}
  \begin{split}
    \lim\limits_{\epsilon \rightarrow \infty}
    \frac{\exp[\epsilon\sum_i h_i]-1}{\epsilon}
    =
    \frac{\mathrm{d}}{\mathrm{d}\epsilon}
    \exp[\epsilon\sum_i h_i]\bigg|_{\epsilon=0}
    =
    \sum_ih_i
  \end{split}
\end{equation}

\section*{Estimation of Multinomial Probabilities}
If $m_T$ is the number of trials, then the estimated probabilities
of $p^b_y$ in bin $b$ are given by
\begin{equation}
  \begin{split}
    P(\vec{p}^b|D,m_T)
    &= \frac{P(D|\vec{p}^b,m_T)\pi(\vec{p}^b|m_T)}{P(D|m_T)}\\
    &=
    \frac{P(D|\vec{p}^b,m_T)\pi(\vec{p}^b|m_T)}{
      \int_{\Delta^K}
      P(D|\vec{p}^b,m_T)
      \pi(\vec{p}^b|m_T)
      \mathrm{d}\vec{p}^b}\\
    &=
    \Dir(\vec{p}^b; \vec{n}^b+\vec{\alpha}^b),
  \end{split}
\end{equation}
where $\pi(\vec{p}^b|m_T) = \Dir(\vec{p}^b; \vec{\alpha}^b)$. Hence
the expectation for $\vec{p}^b \sim \Dir(\vec{p}^b;
\vec{n}^b+\vec{\alpha}^b)$ is given by
\begin{equation}
  \begin{split}
    \hat{p}^b_y = \mathrm{E}[p^b_y|D,m_T]
    &= \frac{n^b_y + \alpha^b_y}{\sum_{y \in \mathcal{Y}} n^b_y + \alpha^b_y}\\
    &= \frac{n^b_y + \alpha^b_y}{m_T + \sum_{y^* \in \mathcal{Y}} \alpha^b_{y^*}},
  \end{split}
\end{equation}
which is also known as the \textit{extended Bayes-Laplace rule} and is
often used whenever only a small number of observations is
available. This estimate is equal to the frequentist's estimate,
i.e. $\mathrm{E}[p^b_y|D,m_T] = \frac{n^b_y}{m_T}$, if $\alpha^b_y = 0
~ \forall y \in \mathcal{Y}$. For $\alpha^b_y = 1 ~ \forall y \in
\mathcal{Y}$ we get \textit{Laplace's rule of succession}, which
prevents that prior probabilities of zero or one are used. In Bayesian
statistics a probability of zero or one should only be used for
logical statements, called \textit{Cromwell's rule}.

\section*{Todo}
\begin{itemize}
\item comparison of NSB method with binning to estimate entropies
\item numerically compute iso-entropic lines on the probability simplex
\end{itemize}