% -*- mode: latex -*-
%
\providecommand{\Mult}{\ensuremath{\mathrm{Mult}}}
\providecommand{\Dir}{\ensuremath{\mathrm{Dir}}}
\providecommand{\Beta}{\ensuremath{\mathrm{Beta}}}

\newcommand{\Yf}{\boldsymbol{Y}}
\newcommand{\Xf}{\boldsymbol{X}}
\newcommand{\pf}{\boldsymbol{p}}
\newcommand{\Df}{\boldsymbol{D}}
\newcommand{\Ef}{\boldsymbol{E}}
\newcommand{\xf}{\boldsymbol{x}}
\newcommand{\yf}{\boldsymbol{y}}
\newcommand{\zf}{\boldsymbol{z}}
\newcommand{\cf}{\boldsymbol{c}}
\newcommand{\gf}{\boldsymbol{g}}
\newcommand{\alphaf}{\boldsymbol{\alpha}}
\newcommand{\betaf}{\boldsymbol{\beta}}
\newcommand{\gammaf}{\boldsymbol{\gamma}}

\newcommand{\nf}{\boldsymbol{n}}

\newcommand{\xt}{\tilde{x}}
\newcommand{\yt}{\tilde{y}}

\newcommand{\Xc}{\mathcal{X}}
\newcommand{\Yc}{\mathcal{Y}}
\newcommand{\Fc}{\mathcal{F}}
\newcommand{\Gc}{\mathcal{G}}
\newcommand{\Hc}{\mathcal{H}}
\newcommand{\Oc}{\mathcal{O}}
\newcommand{\Pc}{\mathcal{P}}
\newcommand{\Cc}{\mathcal{C}}

\section*{Introduction}
\label{introduction}

\section*{Inference}
Given a sequence of $m_T$ observations $D = (X_1, X_2, \dots,
X_{m_T})$ how can we make predictions about future
observations\footnote{This is known as \textit{Hume's problem of
induction} and was answered by de Finetti's representation theorem.}?
That is, how can we formulate a hypothesis $\mathcal{H}$ about the
laws behind the sequence of observations and compute the probability
of our hypothesis conditional on all the evidence that we gathered?
From \textit{Bayes's rule of conditioning}
\begin{equation}
  P(\mathcal{H}|D) = \frac{P(D,\mathcal{H})}{P(D)}
\end{equation}
we know how to incorporate our observations into our probability
function, but to fully answer the question of induction we have to
clearly define the patterns or symmetries that we expect behing the
sequence of observations. This will lead us to a theorem by de Finetti
that tells us how the joint probability mass function for $P(D)$ can
be represented. A corresponding mode of inductive inference is then
given by the \textit{Johnson-Carnap continuum}. Several drawbacks of
the Johnson-Carnap continuum have been extensively discussed in the
literature, one if which is the assumption that the number of possible
outcomes $K$ is known in advance. Several generalizations were
developed, for instance the one by Pitman which is especially 
\begin{definition}[Infinite exchangeability]
  Let $X_1, X_2, \dots$ denote an infinite sequence of random events,
  where each $X_i$ is a random variable from a probability space
  $(\Omega, \mathcal{F}, P)$ to a measure space $(\mathcal{X},
  \mathcal{F}')$. The sequence is said to be infinitely exchangeable
  with respect to the probability measure $P$ if for every subsequence
  of length $n$
  \begin{equation}
    P(X_1 = x_1, \dots, X_{m_T} = x_{m_T}) = P(X_{1} = x_{\pi(1)},
    \dots, X_{m_T} = x_{\pi(m_T)})
  \end{equation}
  with $x_i \in \mathcal{X}$ and for all permutations $\pi$ defined on
  the set $\{1, \dots, m_T\}$. (cf. \cite{Finetti1974})
\end{definition}

\begin{theorem}[Generalized de Finetti's representation theorem]
  \label{thr:frm}
  Let $X_1, X_2, \dots$ denote an infinitely exchangeable sequence of
  random variables from a probability space $(\Omega, \mathcal{F}, P)$
  to a measure space $(\mathcal{X}, \mathcal{F}')$. There exists a
  cumulative distribution $\Pi$ such that the joint mass function
  $p(x_1, \dots, x_{m_T}) = P(X_1 = x_1, \dots, X_{m_T} = x_{m_T})$ has
  the form
  \begin{equation}
    \begin{split}
      \label{eq:frt}
      p(x_1, \dots, x_{m_T}) &=
      \int_{\Delta^K} p(x_1, \dots, x_{m_T}|\vec{\theta})
      \mathrm{d} \Pi(\vec{\theta})\\
      &=
      \int_{\Delta^K} \Mult(\vec{n};\vec{\theta})
      \mathrm{d} \Pi(\vec{\theta})\\
      &=
      \int_{\Delta^K}
      \frac{m_T!}{\prod_{x \in \mathcal{X}}n_x!}
      \prod_{x \in \mathcal{X}}
      \theta_x^{n_x}
      \mathrm{d} \Pi(\vec{\theta}), ~ \text{where}\\
      \Pi(\vec{\theta}) &= \lim\limits_{m_T \rightarrow \infty}
      P \left\{\frac{n_x}{m_T} \le \theta_x : x \in \mathcal{X} \right\},
    \end{split}
  \end{equation}
  where $n_x$ denotes the count statistic for $x \in \mathcal{X}$, $K
  = |\mathcal{X}|$, and $\Delta^K := \{\vec{\theta} = (\theta_x)_{x \in
    \mathcal{X}} \in \mathbb{R}^K : \sum_{x \in \mathcal{X}} \theta_x = 1
  ~ \text{and} ~ \theta_x \ge 0~\forall x \in \mathcal{X} \}$ denotes
  the $K$-dimensional probability simplex. (cf. \cite{Finetti1974,
Hewitt1955})
\end{theorem}
In other words, conditional on $\vec{\theta}$ the joint probability
mass function $p(x_1, \dots, x_{m_T})$ can be written as an integral
mixture of multinomial probabilities. Note that for $K=1$ equation
$\ref{eq:frt}$ computes the $n_x$th moment of $\mathrm{d}\Pi$. In
Bayesian inference, the limiting distribution $\mathrm{d}\Pi$ is
interpreted as the prior distribution. A common choice for
$\mathrm{d}\Pi$ is a conjugate prior which substantially simplifies
the computation of the posterior distribution. In the following we
show how such a prior can be justified from inductive principles such
that theorem \ref{thr:frm} holds.

\begin{theorem}[Johnson-Carnap continuum]
  \label{thr:jcc}
  Let $X_1, X_2, \dots$ denote an infinite sequence of random
  variables from a probability space $(\Omega, \mathcal{F}, P)$ to a
  measure space $(\mathcal{X}, \mathcal{F}')$. Assume that
  \begin{itemize}
  \item[(i)  ] $K = |\mathcal{X}| \ge 3$,
  \item[(ii) ] $P(X_1 = x_1, \dots, X_{m_T} = x_{m_T}) > 0$ for all $x_i \in
    \mathcal{X}$, and
  \item[(iii)] there exists a sufficient statistic $f_x(n_x,n)$ such
    that $P(X_{m_T+1} = x_{m_T+1} | \vec{n}) = f_x(n_x,m_T)$, where $\vec{n} =
    (n_x)_{x \in \mathcal{X}}$.
  \end{itemize}
  Then either the outcomes are independent or there exist positive
  constants $\{\alpha_x | x \in \mathcal{X}\}$ such that for all $n \ge 1$
  \begin{equation}
    f_x(n_x, n) = \frac{n_x + \alpha_x}{m_T + \sum_{x^* \in \mathcal{X}}
      \alpha_{x^*}}.
  \end{equation}
  (cf. \cite{Johnson1932, Zabell1982})
\end{theorem}
Note that if $X_1, X_2, \dots$ is exchangeable then $P(X_{m_T+1} =
x_{m_T+1} | \vec{n}) = P(X_{m_T+1} = x_{m_T+1} | n_x) = f_x(n_x,m_T)$. The
Johnson-Carnap continuum of inductive methods not only provides us
with a sufficient statistic for inductive reasoning, but also
identifies a unique prior distribution for de Finetti's representation
theorem.
\begin{corollary}
  Under the conditions (i-iii) of theorem \ref{thr:jcc} the prior
  distribution $\mathrm{d}\Pi$ belongs to the \textit{Dirichlet
    familiy}, i.e.
  \begin{equation}
    \begin{split}
      \mathrm{d}\Pi(\vec{\theta}) =
      \Dir(\vec{\theta};\vec{\alpha})\mathrm{d}\vec{\theta} &=
      \frac{1}{\Beta(\vec{\alpha})}
      \prod_{x \in \mathcal{X}} \theta_x^{\alpha_x -1}
      \mathrm{d}\vec{\theta}, ~ \text{where}\\
      \Beta(\vec{\alpha}) &= \frac{\prod_{x \in \mathcal{X}}
        \Gamma(\alpha_x)}{\Gamma(\sum_{x
          \in \mathcal{X}}\alpha_x)} =
      \underbrace{\int_{\Delta^K}\prod_{x \in
          \mathcal{X}} \theta^{\alpha_x-1}
        \mathrm{d}\vec{\theta},}_{\text{multinomial Euler
          integral}}
    \end{split}
  \end{equation}
  and $\Gamma(z) = \int_0^\infty t^{z-1}e^{-t}\mathrm{d}t$ denotes
  the Gamma function.
\end{corollary}
To summarize our results, under the symmetry assumptions that we
introduced about the sequence of observations $D = (X_1, X_2, \dots,
X_{m_T})$ we are able to state a hypothesis $\mathcal{H}$ in form of the
parameters $\vec{\theta}$ that represent our beliefs about the
underlying process. Any knowledge that we have before we look at the
data is represented in the parameters $\vec{\alpha}$. The probability
of a hypothesis is then given by
\begin{equation}
  \begin{split}
    P(\vec{\theta}|D)
    &= \frac{P(D,\vec{\theta})}{P(D)}
     =
    \frac{\Mult(D;\vec{\theta})\Dir(\vec{\theta};\vec{\alpha})}{
      \int_{\Delta^K}
      \Mult(D;\vec{\theta}')
      \mathrm{d}\Pi(\vec{\theta}')}\\
    &=
    \Dir(\vec{\theta}; \vec{n}+\vec{\alpha}).
  \end{split}
\end{equation}
The update of our beliefs after seeing the data is therefore a mere
shift in the parameters of the Dirichlet distribution
(i.e. $\Dir(\vec{\theta};\vec{\alpha}) \mapsto \Dir(\vec{\theta};
\vec{n}+\vec{\alpha})$) as it is a conjugate prior to the multinomial
distribution. Intuitively, the Dirichlet distribution states our
belief that the probabilities of $K$ rival events $x \in \mathcal{X}$
are $(\theta_x)_{x \in \mathcal{X}}$ given that each event has been
observed $\alpha_x - 1$ times. Since $\vec{\theta} \sim
\Dir(\vec{\theta}; \vec{n}+\vec{\alpha})$ we can easily compute the
expected value of $\vec{\theta}$ given our observations, i.e.
\begin{equation}
  \begin{split}
    \hat{\theta}_x = \mathrm{E}[\theta_x|D] &=
    \int_{\Delta^{K-1}}\theta_x \Mult(D;\vec{\theta})
    \mathrm{d}\Pi(\vec{\theta}; \vec{\alpha}) =
    \frac{
      \int_{\Delta^K} \Mult(D \cup \{x\};\vec{\theta})
      \mathrm{d}\Pi(\vec{\theta}; \vec{\alpha})
    }{
      \int_{\Delta^K} \Mult(D;\vec{\theta})
      \mathrm{d}\Pi(\vec{\theta}; \vec{\alpha})
    }\\
    &= f_x(n_x,m_T)
    = \frac{n_x + \alpha_x}{n + \sum_{x^* \in \mathcal{X}} \alpha_{x^*}},
  \end{split}
\end{equation}
as it was shown in theorem \ref{thr:jcc}. The estimate is known as the
\textit{extended Bayes-Laplace rule of succession}. It is equivalent
to the frequentist's estimate ($\hat{\theta}_x = \frac{n_x}{m_T}$) if
$\alpha_x = 0 ~ \forall x \in \mathcal{X}$. If $\alpha_x = 1 ~ \forall
x \in \mathcal{X}$ we get \textit{Laplace's rule of succession}.

\section*{Inductive inference for piecewise constant distributions}
We will now generalize the results of the previous section. Suppose
that we have $m_T$ measurements (trials) where each of them is a
sequence $X_i^1 = x_i^1, \dots, X_i^L = x_i^L$ of $L$ discrete events,
with $x_i^j \in \mathcal{X}$. The sequence could for instance be a
spike train where time has been discretized into intervals
$\mathcal{T} = \{t_1, t_2, \dots, t_L\}$ of size $\Delta t$. Then
$X_i^j$ would indicate whether or not a spike was recorded in the
$j$th interval (with $\mathcal{X} = \{0,1\}$). We expect that the
probabilities for an event in general varies over time. However, we
assume that we can join some proximal intervals because the
probabilites are nearly the same. The advantage of this strategy is
that in case we have few measurements we can use the data from several
intervals to get a better estimate, which we will call sharing
strength. A well-ordered partition $B$ of $\mathcal{T}$ is a division
of $\mathcal{T}$ into $m_B$ non-empty, non-overlapping consecutive
subsets called bins, such that their union covers $\mathcal{T}$. The
partition class $\Pc(\mathcal{T})$ denotes the collection of all
well-ordered partitions and $\Pc_{m_B}(\mathcal{T}) \subset
\Pc(\mathcal{T})$ the subclass of well-ordererd partitions in
$\Pc(\mathcal{T})$ consisting of exactly $m_B$ different subsets. For
simplicity we will first assume that we know the number of partitions
$m_B$. We are interested in the joint probability of the parameters
$\vec{\Theta} = (\vec{\theta}^b)_{b \in B}$ and a partition $B$ given
our observations and the size of the partition $m_B$, i.e. we want to
compute
\begin{equation}
  \begin{split}
    P(\vec{\Theta},B|D,m_B)
    &= \frac{P(D,\vec{\Theta},B|m_B)}{P(D|m_B)}\\
    &= \frac{P(D|\vec{\Theta},B,m_B)P(\vec{\Theta},B|m_B)}{P(D|m_B)}.
  \end{split}
\end{equation}
We assume that events are independent between the bins and therefore
we can factorize the likelihood and prior such that
\begin{equation}
  P(D,\vec{\Theta},B|m_B) =
  \prod_{b\in B} P(D|\vec{\theta}^b,B,m_B)P(\vec{\theta}^b,B|m_B).
\end{equation}
where $\vec{\theta}^b = (\theta^b_x)_{x \in \mathcal{X}}$ denote the
probabilities of observing event $x \in \mathcal{X}$ in bin $b$.
For $P(\vec{\theta}^b,B|m_B)$ we choose a non-informative prior
assumption such that
\begin{equation}
  P(\vec{\theta}^b,B|m_B) = P(\vec{\theta}^b|m_B)P(B|m_B),
\end{equation}
which means that the prior knowledge about $\vec{\theta}^b$ is
independent of $B$ and vice versa. We also choose a flat prior for the
partitions, i.e.
\begin{equation}
  P(B|m_B) = {L \choose m_B}^{-1}
\end{equation}
because we have no information a priori which of the partitions is
more probable. $P(\vec{\theta}^b|m_B)$ is represented by a distribution of
the Dirichlet familiy as discussed earlier. To compute the evidence
\begin{equation}
  \begin{split}
    P(D|m_B)
    &= \sum_{B\in \mathcal{P}_{m_B}(\mathcal{X})}
      P(D|B,m_B) P(B|m_B)\\
  \end{split}
\end{equation}
we have to sum over all possible partitions $B \in
\mathcal{P}_{m_B}$. Furthermore we have
\begin{equation}
  \begin{split}
    P(D|B,m_B)
    &=
      \int_{\underbrace{\Delta^K \times \Delta^K \times \dots \times
          \Delta^K}_{m_B~\text{times}}}
      P(D|\vec{\Theta},B,m_B) P(\vec{\Theta}|B,m_B)\mathrm{d}\vec{\Theta}\\
    &=
      \prod_{b\in B} \int_{\Delta^K}
      \Mult(\vec{\theta}^b;\vec{n}^b)
      \mathrm{d}\Pi(\vec{\theta}^b; \vec{\alpha}^b).
  \end{split}
\end{equation}
By solving the integrals we get
\begin{equation}
  \begin{split}
    P(D|m_B)
    &= \sum_{B\in \mathcal{P}_{m_B}(\mathcal{T})}
    P(B|m_B)
    \prod_{b\in B}\Beta(\vec{\alpha}^b)^{-1}
    \frac{\prod_{x \in
        \mathcal{X}}\Gamma(n_x^b+\alpha^b_x)}{\Gamma(\sum_{x\in
        \mathcal{X}}n^b_x+\alpha^b_x)}\\
    &=  \sum_{B\in \mathcal{P}_{m_B}(\mathcal{T})}
    P(B|m_B)
    \prod_{b\in B}\frac{\Beta(\vec{n}^b+\vec{\alpha}^b)}{\Beta(\vec{\alpha}^b)}
  \end{split}
\end{equation}
which we can compute with the algorithm sketched in lemma
\ref{prombslemma}. To compute the full model we still need to average
over all possible $m_B$, i.e.
\begin{equation}
  P(\vec{\Theta},B|D) = \sum_{m_B = 1}^L P(\vec{\Theta},B|D,m_B) P(m_B|D),
\end{equation}
where $P(m_B|D)$ denotes the model posterior, which is given by
\begin{equation}
  \begin{split}
    P(m_B|D)
    &= \frac{P(D|m_B) P(m_B)}{P(D)}\\
    &= \frac{P(D|m_B) P(m_B)}{\sum_{m_B\in M}P(D|m_B) P(m_B)}.
  \end{split}
\end{equation}
$P(m_B)$ is the prior for a given model size $m_B$. We will usually
choose a non-informative prior $1/L$ unless we want to select
specific model sizes.

\section*{Expectation and Variance}
To characterize the inferred distribution, we will compute the first
three moments from the moment generating function
\begin{equation}
  \begin{split}
    \mu_i' = \int x^i\mathrm{d}P(x) = \frac{P(D^i|m_B)}{P(D|m_B)},
  \end{split}
\end{equation}
where $P(x)$ denotes the Lebesgue-Stieltjes probability density
function and $\mu_i'$ the $i$th raw moment. The central moments
$\mu_i$ are given by
\begin{equation}
  \begin{split}
    \mu_i = \int (x-\mu_1')^i\mathrm{d}P(x),
  \end{split}
\end{equation}
which can be computed from the raw moments by using the binomial
transform
\begin{equation}
  \begin{split}
    \mu_i = \sum_{k=0}^{i} {i \choose k} (-1)^{i-k}\mu_k'(\mu_1')^{i-k}.
  \end{split}
\end{equation}
Hence
\begin{equation}
  \begin{split}
    \mathrm{E}[\theta_x^b|D,m_b] 
    = P(y_b|D,m_B)
    = \frac{P(x_b,D|m_B)}{P(D|m_B)}
    = \frac{P(D'|m_B)}{P(D|m_B)},
  \end{split}
\end{equation}

\begin{equation}
  \begin{split}
    \mathrm{Var}[p_x^b|D,m_b]
    &= P(x_b,x_b|D,m_B) - (P(x_b|D,m_B))^2\\
    &= \frac{P(D''|m_B)}{P(D|m_B)} - \left(
    \frac{P(D'|m_B)}{P(D|m_B)} \right)^2
  \end{split}
\end{equation}
where $D' = D \cup \{x_b\}$.

\begin{equation}
  \begin{split}
    \mathrm{E}[p_y^b|D] 
    = \frac{\sum_{m_B\in M}P(D'|m_B) P(m_B)}{\sum_{m_B\in M}P(D|m_B) P(m_B)},
  \end{split}
\end{equation}
with $M = \{1,2,\dots,L\}$.

\section*{Break Probabilities}
\begin{equation}
  \begin{split}
    \sum_{B\in \mathcal{I}^i_{m_B}(\mathcal{X})}P(B|D,m_B) &=
      \frac{\sum_{B\in
          \mathcal{I}^i_{m_B}(\mathcal{X})} P(D|B,m_B) P(B|m_B)}{\sum_{B\in
               \mathcal{P}_{m_B}(\mathcal{X})} P(D|B,m_B) P(B|m_B)}\\
      &= \frac{\sum_{B\in
          \mathcal{I}^i_{m_B}(\mathcal{X})} P(D|B,m_B) P(B|m_B)}{P(D|m_B)},
  \end{split}
\end{equation}
where $\mathcal{I}^i_{m_B}(\mathcal{X}) \subseteq
\mathcal{P}_{m_B}(\mathcal{X})$ denotes the set of multi-bins that
have a border at position $i$.

\section*{Bayesian Hypothesis Testing}
Bayes factor
\begin{equation}
  \begin{split}
    K = \frac{P(D|m_1)}{P(D|m_2)}
  \end{split}
\end{equation}

\section*{Information-theoretic quantities}
By intriducing a random variable $\mathcal{B}$ that represents the
outcome of a certain partition, we can compute our uncertainty about
which partition describes the data best, which is given by the Shannon
entropy
\begin{equation}
  \begin{split}
    \mathrm{H}(&\mathcal{B}|D)\\
    &= -\sum_{B\in \mathcal{P}(\mathcal{X})}P(B|D) \log P(B|D)\\
    &= -\sum_{B\in \mathcal{P}(\mathcal{X})}\frac{P(D|B)P(B)}{P(D)}
    \log
    \frac{P(D|B)P(B)}{P(D)}\\
%     &= -\sum_{B\in \mathcal{P}(\mathcal{X})}
%     \frac{P(D|B)P(B)}{\sum_{B\in
%         \mathcal{P}(\mathcal{X})}P(D|B)P(B)}
%     \log
%     \frac{P(D|B)P(B)}{\sum_{B\in
%         \mathcal{P}(\mathcal{X})}P(D|B)P(B)}\\
    &=
    -\frac{1}{P(D)}
    \sum_{B\in \mathcal{P}(\mathcal{X})}
    P(B)
    \prod_{b\in
      B}\frac{\Beta(\vec{n}^b+\vec{\alpha}^b)}{\Beta(\vec{\alpha}^b)}\\
    &\quad\quad\quad
    \left[
      \log
      \frac{P(B)}{P(D)} +
      \sum_{b \in B}
      \log
      \frac{\Beta(\vec{n}^b+\vec{\alpha}^b)}{\Beta(\vec{\alpha}^b)}
    \right]\\
    &=
    \frac{1}{P(D)}
    \sum_{B\in \mathcal{P}(\mathcal{X})}
    P(B)
    \prod_{b\in B}\frac{\Beta(\vec{n}^b+\vec{\alpha}^b)}{\Beta(\vec{\alpha}^b)}
    \sum_{b \in B}
    \left[-\log
    \frac{\Beta(\vec{n}^b+\vec{\alpha}^b)}{\Beta(\vec{\alpha}^b)}\right]\\
    &-
    \frac{1}{P(D)}
    \sum_{B\in \mathcal{P}(\mathcal{X})}
    P(B)
    \log\frac{P(B)}{P(D)}
    \prod_{b\in B}
    \frac{\Beta(\vec{n}^b+\vec{\alpha}^b)}{\Beta(\vec{\alpha}^b)}
  \end{split}
\end{equation}

\begin{lemma}[The Proximal Multi-Bin Summation (ProMBS) Algorithm] \label{prombslemma}
%
Let $f:C(\Xc)\rightarrow\mathbb{R}$ be any function mapping
consecutive segments to the real numbers and
$\gf=(g_1,\ldots,g_K)\in\mathbb{R}^K$ be any real valued
vector. Define for $l=1,\dots,L$ the upper triangular matrices
$A_l=(a^l_{ij})_{L\times L}$ recursively by
%
$$
%
			a^1_{ij}
		=
	\begin{cases}
%
				f(b_{i,j}) 	& \text{ if } i\geq j	
			\\
				0  & \text{ otherwise }
%
	\end{cases}
% 
\text{  and  }
%
 \ \
		 a^{l+1}_{ij}
		=
	\begin{cases}
				a^l_{i+1,j+1}	& i,j < L 			
			\\
				1							& i=j=L
			\\
				0							& \text{else}
%
	\end{cases} \ .
% 
$$
%
The sum
$$
		S[f]
	=
		\sum_{B\in\Pc(\Xc)}
		g_{m_B}
		\prod_{b\in B}
		f(b)	
$$
%
can be turned out as follows: The first $N\leq L$ partial sums
$$
		S_m[f]
	=
		\sum_{B\in\mathcal{P}_m(\Xc)}
		\prod_{b\in B}
		f(b)
$$
%
can be evaluated by iterative matrix multiplications 
%
$$
		(0,\ldots,S_N[f],\ldots,S_1[f])
	=
		(1,0,\ldots,0)
	\prod_{m=1}^N
		A_m .
$$
%
Full evaluation of all partial sums delivers
%
$$
		S[f,\gf]
	=
		\sum_{m=1}^K g_m S_m[f] \ .
$$
%
\end{lemma}
%
\begin{lemma}[The extended ProMBS-Algorithm]\label{eprombslemma}
%
Consider the same conditions and definitions made in lemma
(\ref{prombslemma}). Let $h:C(\Xc)\rightarrow\mathbb{R}$ and
define for any $\epsilon>0$
%
$$
		SP_\epsilon[f,\gf,h]
	:=
			\frac{			
							S[f_\epsilon,\gf]
						-
							S[f,\gf]
					}{
						\epsilon
					}
			\ , ~ \text{where} ~
			f_\epsilon(b)
		:=
			f(b)e^{\epsilon h(b)} \ ,
$$
where we use
\begin{equation}
  \begin{split}
    \sum_ih_i =
    \lim\limits_{\epsilon \rightarrow \infty}
    \frac{\exp[\epsilon\sum_i h_i]-1}{\epsilon}
    =
    \frac{\mathrm{d}}{\mathrm{d}\epsilon}
    \exp[\epsilon\sum_i h_i]\bigg|_{\epsilon=0}
  \end{split}
\end{equation}
for the approximation.
%
For asymptotically small $\epsilon>0$, it holds that
$$
	\sum_{B\in\Pc(\Xc)}
		g_{m_B}
		\prod_{b\in B}
		f(b)	
			\sum_{b\in B}
				h(b)
	=
			SP_\epsilon[f,\gf,h] 
		+
		\Oc(\epsilon)		
			\ 
			.
$$
%
%
%
\end{lemma}
