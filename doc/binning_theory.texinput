% -*- mode: latex -*-
%
\providecommand{\Dir}{\ensuremath{\mathrm{Dir}}}
\providecommand{\Beta}{\ensuremath{\mathrm{Beta}}}

\newcommand{\Yf}{\boldsymbol{Y}}
\newcommand{\Xf}{\boldsymbol{X}}
\newcommand{\pf}{\boldsymbol{p}}
\newcommand{\Df}{\boldsymbol{D}}
\newcommand{\Ef}{\boldsymbol{E}}
\newcommand{\xf}{\boldsymbol{x}}
\newcommand{\yf}{\boldsymbol{y}}
\newcommand{\zf}{\boldsymbol{z}}
\newcommand{\cf}{\boldsymbol{c}}
\newcommand{\gf}{\boldsymbol{g}}
\newcommand{\alphaf}{\boldsymbol{\alpha}}
\newcommand{\betaf}{\boldsymbol{\beta}}
\newcommand{\gammaf}{\boldsymbol{\gamma}}

\newcommand{\nf}{\boldsymbol{n}}

\newcommand{\xt}{\tilde{x}}
\newcommand{\yt}{\tilde{y}}

\newcommand{\Xc}{\mathcal{X}}
\newcommand{\Yc}{\mathcal{Y}}
\newcommand{\Fc}{\mathcal{F}}
\newcommand{\Gc}{\mathcal{G}}
\newcommand{\Hc}{\mathcal{H}}
\newcommand{\Oc}{\mathcal{O}}
\newcommand{\Pc}{\mathcal{P}}
\newcommand{\Cc}{\mathcal{C}}

\section*{Introduction}
\label{introduction}

\section*{Bayesian inference and the meaning of probability}
As an extension of logic, Bayesian theory provides us with a framework
for optimal reasoning about uncertain statements\footnote{Whether or
not humans do reason in a Bayesian optimal way is for instance debated
by Gigerenzer et al.}. For an axiomatic justification of the rules of
probability we use Cox's theorem (cf. \cite{Cox1946, Cox1961,
Jaynes2003}) as opposed to the axioms proposed by Kolmogorov. In
Bayesian probability theory, the meaning of probability is of
epistemic nature. That is, probabilities represent a degree of belief
about a statement or hypothesis rather than a physical property, which
also means that probabilities are considered to be subjective. An
extreme view, introduced by de Finetti, states that probabilities are
purely subjective and do not exist as physical properties. Other
viewpoints are less extreme and accept the existence of such
probabilities. A new approach called objective Bayesian inference,
which started with \cite{Jaynes1957a, Jaynes1957b}, made the
assignment of probabilities less subjective in the sense that one
chooses the maximum entropy distribution given a set of
observations. From a technical point of view it is of course not
always easy to compute this distribution called a Gibbs measure
(cf. \cite{Jaynes1957a}).

\section*{Basic Definitions}
A probability space is given as a triple $(\Omega, \mathcal{F}, P)$,
where $\Omega$ denotes the set of outcomes (or elementary events),
$\mathcal{F} \subseteq \mathcal{P}(\Omega)$ a set of possible events
and $P : \mathcal{F} \rightarrow [0,1]$ a probability measure with
$P(\Omega) = 1$. We define a random variable $X : \Omega \rightarrow
\mathcal{X} \subseteq \mathbb{R}$ as a measurable map from $(\Omega,
\mathcal{F})$ to $(\mathcal{X}, \mathcal{F}')$ with
\begin{equation}
  X^{-1}(B) = \{ \omega : X(\omega) \in B \} \in \mathcal{F} ~
  \text{for all} ~ B \in \mathcal{F}'.
\end{equation}
For a random variable $X$ from a probability space $(\Omega,
\mathcal{F}, P)$ to a measure space $(\mathcal{X}, \mathcal{F}')$ we
define the expected value as
\begin{equation}
  E[X] = \int_\Omega X(\omega) \mathrm{d}P(\omega),
\end{equation}
but to compute the integral we need to move to a different space. For
a measurable function $f$ from $(\mathcal{X}, \mathcal{F}')$ to
$(\mathbb{R}, \mathcal{R})$ and a distribution $\mu(A) = P(\{\omega :
X(\omega) \in A\}) = P(X \in A)$, i.e. $X \sim \mu$, with $A
\subseteq \mathcal{F}'$ we define
\begin{equation}
  \begin{split}
  E_f[X] &= \int_\Omega f(X(\omega)) \mathrm{d}P(\omega)\\
         &= \int_{\mathcal{F}'} f(y) \mathrm{d}(P \circ X^{-1})\\
         &= \int_{\mathcal{F}'} f(y) \mu(\mathrm{d}y).
  \end{split}
\end{equation}

\section*{Inference}
Given a sequence of $m_T$ observations $E = \{X_1, X_2, \dots,
X_{m_T}\}$ how can we make predictions about future
observations?
%\footnote{This is known as \textit{Hume's problem of
%induction} and was first answered by de Finetti's representation
%theorem for problems where sequences of events are exchangeable.}
That is, how can we formulate a hypothesis $\mathcal{H}$ about the
laws behind the sequence of observations and compute the probability
of our hypothesis conditional on all the evidence $E$ that we
gathered? From \textit{Bayes's rule of conditioning}
\begin{equation}
  P(\mathcal{H}|E) = \frac{P(E,\mathcal{H})}{P(E)},
\end{equation}
we know how to incorporate our observations into our
probability function. But to fully answer the question of induction we
have to clearly define the patterns or symmetries that we expect
behing the sequence of observations. This will lead us to a theorem by
de Finetti that tells us how the joint probability mass function for
$P(E)$ can be represented. A corresponding mode of inductive inference
is then given by the \textit{Johnson-Carnap continuum}. Several
drawbacks of the Johnson-Carnap continuum have been extensively
discussed in literature, one of which is the assumption that the
number of possible outcomes $K$ is known in advance. Several
generalizations were developed, for instance the one by Pitman which
overcomes this limitation. However, we will first focus on the
Johnson-Carnap continuum. More general forms will become useful later
when we want to estimate entropies from severely undersampled
probability spaces.
\begin{definition}[Finite and infinite exchangeability]
  Let $X_1, X_2, \dots$ denote sequence of random events,
  where each $X_i$ is a random variable from a probability space
  $(\Omega, \mathcal{F}, P)$ to a measure space $(\mathcal{X},
  \mathcal{F}')$. A finite sequence of $m_T$ events is said to be
  $m_T$-exchangeable with respect to the probability measure $P$ if
  \begin{equation}
    P(X_1 = x_1, \dots, X_{m_T} = x_{m_T}) = P(X_{1} = x_{\pi(1)},
    \dots, X_{m_T} = x_{\pi(m_T)})
  \end{equation}
  with $x_i \in \mathcal{X}$ and for all permutations $\pi$ defined on
  the set $\{1, \dots, m_T\}$. An infinite sequence is exchangeable if
  it is $m_T$-exchangeable for all $m_T \in \mathbb{N}$.
  (cf. \cite{Finetti1974})
\end{definition}

\begin{lemma}
  Exchangeable sequences are strictly stationary, i.e.
  \begin{equation}
    P(X_1 = x_1, \dots, X_{m_T} = x_{m_T}) = P(X_{1} = x_{1+\tau},
    \dots, X_{m_T} = x_{m_T+\tau})
  \end{equation}
  for all $\tau \in \mathbb{N}$.
\end{lemma}

Note that de Finetti's representation theorem is a form of ergodic
decomposition for stationary (non-ergodic) random processes.

\begin{example}[Exchangeability]
  Let $X_1, X_2, \dots$ be an infinite sequence of binary random
  events with $\mathcal{X} = \{0, 1 \}$ and $X_i \sim p$, where we
  define $p(0) = 1-p^*$ and $p(1) = p^*$. We will draw $p^*$ from a
  discrete uniform distribution on $\{p_1, p_2\}$. Hence we have
  \begin{equation}
    \lim\limits_{m_T \rightarrow \infty}
    \frac{1}{m_T}\sum_i^{m_T} X_i =
    \begin{cases}
      p_1 & \text{with probability} ~ 1/2\\
      p_2 & \text{with probability} ~ 1/2,
    \end{cases}
  \end{equation}
  from the law of large numbers. The sequence is exchangeable and
  therefore independent conditional on $p$, i.e.
  \begin{equation}
    P(X_1 = x_1, X_2 = x_2, \dots, X_{m_T} = x_{m_T}) =
    \prod_{i=1}^{m_T}P(X_i = x_i | p).
  \end{equation}
\end{example}
Assuming exchangeability for a sequence of observations makes the
assignment of probabilites a tractable problem and still allows
inductive reasoning. Note that this is not the case when we assume
that observations are independently generated, i.e.
\begin{equation}
    P(X_1 = x_1, \dots, X_{m_T} = x_{m_T}) = P(X_{1} = x_{\pi(1)})
    \dots P(X_{m_T} = x_{\pi(m_T)}),
\end{equation}
as it is often assumed in orthodox statistics. If independence were
truly the case we would not be able to draw any conclusions about
future events given our observations.

\begin{theorem}[Generalized de Finetti's representation theorem]
  \label{thr:frm}
  Let $X_1, X_2, \dots$ denote an infinitely exchangeable sequence of
  random variables from a probability space $(\Omega, \mathcal{F}, P)$
  to a measure space $(\mathcal{X}, \mathcal{F}')$. There exists a
  cumulative distribution $\Pi_{\vec{\alpha}}$ such that the joint mass
  function $p(x_1, \dots, x_{m_T}) = P(X_1 = x_1, \dots, X_{m_T} =
  x_{m_T})$ has the form
  \begin{equation}
    \begin{split}
      \label{eq:frt}
      p(x_1, \dots, x_{m_T}) &=
      \int_{\Delta^K} p(x_1, \dots, x_{m_T}|\vec{\theta})
      \mathrm{d} \Pi_{\vec{\alpha}}(\vec{\theta})\\
      &=
      \int_{\Delta^K}
      \prod_{x \in \mathcal{X}}
      \theta_x^{n_x}
      \mathrm{d} \Pi_{\vec{\alpha}}(\vec{\theta}), ~ \text{with}\\
      \Pi_{\vec{\alpha}}(\vec{\theta}) &= \lim\limits_{m_T \rightarrow \infty}
      \Pr \left\{\frac{n_x}{m_T} \le \theta_x : x \in \mathcal{X} \right\},
    \end{split}
  \end{equation}
  where $\vec{n}(E) = (n_x)_{x \in \mathcal{X}}$ denotes the count
  statistic for every occurence of $x \in \mathcal{X}$ in $E$, $K =
  |\mathcal{X}|$, and $\Delta^K := \{\vec{\theta} = (\theta_x)_{x \in
    \mathcal{X}} \in \mathbb{R}^K : \sum_{x \in \mathcal{X}} \theta_x
  = 1 ~ \text{and} ~ \theta_x \ge 0~\forall x \in \mathcal{X} \}$
  denotes the $K$-dimensional probability simplex. If
  $\Pi_{\vec{\alpha}}$ is a non-decreasing real function we get the
  Riemann-Stieltjes integral.
  (cf. \cite{Finetti1974, Hewitt1955})
\end{theorem}
\begin{proof}
  Todo
\end{proof}
In other words, conditional on $\vec{\theta}$ the joint probability
mass function $p(x_1, \dots, x_{m_T})$ can be written as an integral
mixture of multinomial probabilities. Note that for $K=1$ equation
$\ref{eq:frt}$ computes the $n_x$th moment of
$\mathrm{d}\Pi_{\vec{\alpha}}$. In Bayesian inference, the limiting
distribution $\pi_{\vec{\alpha}} = \mathrm{d}\Pi_{\vec{\alpha}} /
\mathrm{d}\vec{\theta}$ is interpreted as a prior distribution. A
common choice for $\mathrm{d}\Pi_{\vec{\alpha}}$ is a conjugate prior
which substantially simplifies the computation of the posterior
distribution. In the following we show how such a prior can be
justified from inductive principles such that theorem \ref{thr:frm}
holds for a proper choice of $\vec{\alpha}$.

\begin{theorem}[Johnson-Carnap continuum]
  \label{thr:jcc}
  Let $X_1, X_2, \dots$ denote an infinite sequence of random
  variables from a probability space $(\Omega, \mathcal{F}, P)$ to a
  measure space $(\mathcal{X}, \mathcal{F}')$. Assume that
  \begin{itemize}
  \item[(i)  ] $K = |\mathcal{X}| \ge 3$,
  \item[(ii) ] $P(X_1 = x_1, \dots, X_{m_T} = x_{m_T}) > 0$ for all $x_i \in
    \mathcal{X}$, and
  \item[(iii)] there exists a sufficient statistic $f_x(n_x,n)$ such
    that $P(X_{m_T+1} = x_{m_T+1} | \vec{n}(E)) = f_x(n_x,m_T)$, where $\vec{n}(E) =
    (n_x)_{x \in \mathcal{X}}$.
  \end{itemize}
  Then either the outcomes are independent or there exist positive
  constants $\{\alpha_x : x \in \mathcal{X}\}$ such that for all $n \ge 1$
  \begin{equation}
    f_x(n_x, m_T) = \frac{n_x + \alpha_x}{m_T + \sum_{x^* \in \mathcal{X}}
      \alpha_{x^*}}.
  \end{equation}
  (cf. \cite{Johnson1932, Zabell1982})
\end{theorem}
\begin{proof}
  Todo
\end{proof}
Note that if $X_1, X_2, \dots$ is exchangeable then $P(X_{m_T+1} =
x_{m_T+1} | \vec{n}(E)) = P(X_{m_T+1} = x_{m_T+1} | n_x) = f_x(n_x,m_T)$. The
Johnson-Carnap continuum of inductive methods not only provides us
with a sufficient statistic for inductive reasoning, but also
identifies a unique prior distribution for de Finetti's representation
theorem.
\begin{corollary}
  Under the conditions (i-iii) of theorem \ref{thr:jcc} the prior
  distribution $\mathrm{d}\Pi_{\vec{\alpha}}$ belongs to the \textit{Dirichlet
    familiy}, i.e.
  \begin{equation}
    \begin{split}
      \mathrm{d}\Pi_{\vec{\alpha}}(\vec{\theta}) =
      \Dir(\vec{\theta};\vec{\alpha})\mathrm{d}\vec{\theta} &=
      \frac{1}{\Beta(\vec{\alpha})}
      \prod_{x \in \mathcal{X}} \theta_x^{\alpha_x -1}
      \mathrm{d}\vec{\theta}, ~ \text{where}\\
      \Beta(\vec{\alpha}) &= \frac{\prod_{x \in \mathcal{X}}
        \Gamma(\alpha_x)}{\Gamma(\sum_{x
          \in \mathcal{X}}\alpha_x)} =
      \underbrace{\int_{\Delta^K}\prod_{x \in
          \mathcal{X}} \theta^{\alpha_x-1}
        \mathrm{d}\vec{\theta},}_{\text{multinomial Euler
          integral}}
    \end{split}
  \end{equation}
  and $\Gamma(z) = \int_0^\infty t^{z-1}e^{-t}\mathrm{d}t$ denotes
  the Gamma function.
\end{corollary}
\begin{proof}
  Todo
\end{proof}
To summarize our results, under the symmetry assumptions that we
introduced about the sequence of observations $E = (X_1, X_2, \dots,
X_{m_T})$ we are able to state a hypothesis $\mathcal{H}$ in form of the
parameters $\vec{\theta}$ that represent our beliefs about the
underlying process. Any knowledge that we have before we look at the
data is represented in the parameters $\vec{\alpha}$. The probability
of a hypothesis is then given by
\begin{equation}
  \begin{split}
    P(\vec{\theta}|E)
    &= \frac{P(E,\vec{\theta})}{P(E)}
     =
    \frac{P(E|\vec{\theta})
      \mathrm{d}\Pi_{\vec{\alpha}}(\vec{\theta}) /
      \mathrm{d}\vec{\theta}}{
      \int_{\Delta^K}
      P(E|\vec{\theta}')
      \mathrm{d}\Pi_{\vec{\alpha}}(\vec{\theta}')}\\
    &=
    \Dir(\vec{\theta}; \vec{n}(E)+\vec{\alpha}).
  \end{split}
\end{equation}
The update of our beliefs after seeing the data is therefore a mere
shift in the parameters of the Dirichlet distribution
(i.e. $\Dir(\vec{\theta};\vec{\alpha}) \mapsto \Dir(\vec{\theta};
\vec{n}(E)+\vec{\alpha})$) as it is a conjugate prior to the multinomial
distribution. Intuitively, the Dirichlet distribution states our
belief that the probabilities of $K$ rival events $x \in \mathcal{X}$
are $(\theta_x)_{x \in \mathcal{X}}$ given that each event has been
observed $\alpha_x - 1$ times. Since $\vec{\theta} \sim
\Dir(\vec{\theta}; \vec{n}(E)+\vec{\alpha})$ we can easily compute the
expected value of $\vec{\theta}$ given our observations, i.e.
\begin{equation}
  \begin{split}
    \hat{\theta}_x = \mathrm{E}[\theta_x|E] &=
    \int_{\Delta^{K-1}}\theta_x P(E|\vec{\theta})
    \mathrm{d}\Pi_{\vec{\alpha}}(\vec{\theta})\\
    &=
    \frac{
      \int_{\Delta^K} P(X_{m_T+1} = x,E|\vec{\theta})
      \mathrm{d}\Pi_{\vec{\alpha}}(\vec{\theta})
    }{
      \int_{\Delta^K} P(E|\vec{\theta})
      \mathrm{d}\Pi_{\vec{\alpha}}(\vec{\theta})
    }\\
    &= f_x(n_x,m_T)
    = \frac{n_x + \alpha_x}{m_T + \sum_{x' \in \mathcal{X}} \alpha_{x'}},
  \end{split}
\end{equation}
as it was shown in theorem \ref{thr:jcc}. The estimate is known as the
\textit{extended Bayes-Laplace rule of succession}. It is equivalent
to the frequentist's estimate ($\hat{\theta}_x = \frac{n_x}{m_T}$) if
$\alpha_x = 0 ~ \forall x \in \mathcal{X}$. If $\alpha_x = 1 ~ \forall
x \in \mathcal{X}$ we get \textit{Laplace's rule of succession}.

\begin{example}[Polya's Urn]
  Suppose we draw white and black balls at random from an urn
  (i.e. $\Omega = \{ \text{white}, \text{black} \}$). Each time we
  draw a ball we put $k$ balls of the same color back into the
  urn. Thus we have a sequence of exchangeable events $X_1, X_2,
  \dots$ with $X_i( \text{white} ) = 0$ and $X_i( \text{black} ) =
  1$. Hence
  \begin{equation}
    \lim\limits_{m_T \rightarrow \infty} \frac{1}{m_T}
    \sum_{i=1}^{m_T} X_i = \theta \sim \Beta \left(\frac{b}{k},
      \frac{w}{k} \right),
  \end{equation}
  where $b$ is the initial number of black balls and $w$ the number of
  white balls. Conditional on $\theta$ the sequence $X_1, X_2, \dots$ is
  independent.
\end{example}

The concept of exchangeability was generalized by de Finetti to the
notion of $L$-fold partial exchangeablility, where we allow events of
a sequence $X_1, X_2, \dots$ to belong to $L$ different classes.
We can therefore split the sequence into $L$ sequences $(\vec{X^1},
\vec{X^2}, \dots,  \vec{X^L})$ where each $\vec{X^i}$ represents the
events that we assign to the $i$th class. Within the classes the
ordering provides no relevant information. This leads us to the
following definition:

\begin{definition}[Partial exchangeability]
  We call a tuple $(\vec{X^l})_{l=1}^L$ of infinitely exchangeable
  sequences $\vec{X^l} = X^l_1, X^l_2, \dots$ $L$-fold partially
  exchangeable.
\end{definition}

\begin{theorem}[Representation theorem for partially exchangeable
  sequences]
  Let $(\vec{X}^l)_{l=1}^L$ denote an $L$-fold partially exchangeable
  sequence of random variables from a probability space $(\Omega,
  \mathcal{F}, P)$ to a measure space $(\mathcal{X},
  \mathcal{F}')$. There exists a cumulative distribution
  $\Pi_{\vec{\alpha}}^L$ such that the joint mass function $p(\vec{x}^1,
  \dots, \vec{x}^{L}) = P(\vec{X}^1 = \vec{x}^1, \dots, \vec{X}^L =
  \vec{x}^L)$ has the form
  \begin{equation}
    \begin{split}
      p(\vec{x}^1, \dots, \vec{x}^{L}) &=
      \int_{\Delta_L^K}
      \left\{
      \prod_{l=1}^L
      p(x^l_1, \dots, x^l_{m_T}|\vec{\theta}^l)
      \right\}
      \mathrm{d} \Pi_{\vec{\alpha}}^L(\vec{\Theta})\\
      &=
      \int_{\Delta_L^K}
      \left\{
      \prod_{l=1}^L
      \prod_{x \in \mathcal{X}}
      (\theta_x^l)^{n_x^l}
      \right\}
      \mathrm{d} \Pi_{\vec{\alpha}}^L(\vec{\Theta}), ~ \text{with}\\
      \Pi_{\vec{\alpha}}^L(\vec{\Theta}) &= \lim\limits_{m_T \rightarrow \infty}
      \Pr \left\{\frac{n_x^l}{m_T} \le \theta^l_x : x \in \mathcal{X}
        ~ \text{and} ~ l=1,2, \dots, L \right\},
    \end{split}
  \end{equation}
  where $\Delta^K_L = \varprod_{i=1}^L \Delta^K$. We have strong
  partial exchangeability if we can write the joint probability mass
  function as
  \begin{equation}
      p(\vec{x}_1, \dots, \vec{x}_{L}) =
      \prod_{l=1}^L
      \int_{\Delta^K}
      \prod_{x \in \mathcal{X}}
      \theta_x^{n_x^l}
      \mathrm{d} \Pi_{\vec{\alpha}}(\vec{\theta}),
  \end{equation}
  i.e. the classes are independent.
\end{theorem}

\section*{Inductive inference of piecewise constant distributions}
We will now generalize the results of the previous section. Suppose
that we have $m_T$ measurements (trials) where each of them is a
sequence $X_i^1 = x_i^1, \dots, X_i^L = x_i^L$ of $L$ discrete events,
with $x_i^j \in \mathcal{X}$. The sequence could for instance be a
spike train where time has been discretized into intervals
$\mathcal{T} = \{t_1, t_2, \dots, t_L\}$ of size $\Delta t$. Then
$X_i^j$ would indicate whether or not a spike was recorded in the
$j$th interval (with $\mathcal{X} = \{0,1\}$). We expect that the
probabilities for an event in general varies over time. However, we
assume that we can join some proximal intervals because the
probabilites are nearly the same. The advantage of this strategy is
that in case we have few measurements we can use the data from several
intervals to get a better estimate, which we will call sharing
strength. A well-ordered partition $B$ of $\mathcal{T}$ is a division
of $\mathcal{T}$ into $m_B$ non-empty, non-overlapping consecutive
subsets called bins, such that their union covers $\mathcal{T}$. The
partition class $\Pc(\mathcal{T})$ denotes the collection of all
well-ordered partitions and $\Pc_{m_B}(\mathcal{T}) \subset
\Pc(\mathcal{T})$ the subclass of well-ordererd partitions in
$\Pc(\mathcal{T})$ consisting of exactly $m_B$ different subsets. For
simplicity we will first assume that we know the number of partitions
$m_B$. We are interested in the joint probability of the parameters
$\vec{\Theta} = (\vec{\theta}^b)_{b \in B}$ and a partition $B$ given
our observations, i.e. we want to compute
\begin{equation}
  \begin{split}
    P(\vec{\Theta},B|E)
    &= \frac{P(E,\vec{\Theta},B)}{P(E)}\\
    &= \frac{P(E|\vec{\Theta},B)P(\vec{\Theta},B)}{P(E)}.
  \end{split}
\end{equation}
We assume that events are independent between the bins and therefore
we can factorize the likelihood and prior such that
\begin{equation}
  P(E,\vec{\Theta},B) =
  \prod_{b\in B} P(E|\vec{\theta}^b,B)P(\vec{\theta}^b,B).
\end{equation}
where $\vec{\theta}^b = (\theta^b_x)_{x \in \mathcal{X}}$ denote the
probabilities of observing event $x \in \mathcal{X}$ in bin $b$.
For $P(\vec{\theta}^b,B|m_B)$ we choose a non-informative prior
assumption such that
\begin{equation}
  P(\vec{\theta}^b,B) = P(\vec{\theta}^b)P(B),
\end{equation}
which means that the prior knowledge about $\vec{\theta}^b$ is
independent of $B$ and vice versa. Since we have no a priori
information about the partitions, we choose a prior in an objective
Bayesian fashion that maximizes the entropy (cf. \cite{Jaynes1957a,
  Jaynes1957b, Jaynes2003}), i.e.
\begin{equation}
  P(B) = \sum_{m_B \in \mathcal{M}} P(B|m_B)P(m_B)
       = \sum_{m_B \in \mathcal{M}} {L \choose m_B}^{-1}P(m_B),
\end{equation}
where $P(m_B)$ denotes the prior for a given model size $m_B$. We will
usually choose a non-informative prior $1/|\mathcal{M}|$ with
$\mathcal{M} = \{1, 2, \dots, L\}$ unless we want to select specific
model sizes. $P(\vec{\theta}^b|m_B)$ is represented by a distribution
of the Dirichlet familiy as discussed earlier. To compute the evidence
\begin{equation}
  \begin{split}
    P(E)
    &= \sum_{B\in \mathcal{P}(\mathcal{X})}
      P(E|B) P(B)\\
  \end{split}
\end{equation}
we have to sum over all possible partitions $B \in
\mathcal{P}$. Furthermore we have
\begin{equation}
  \begin{split}
    P(E|B)
    &=
      \int_{\underbrace{\Delta^K \times \Delta^K \times \dots \times
          \Delta^K}_{m_B~\text{times}}}
      P(E|\vec{\Theta},B) P(\vec{\Theta}|B)\mathrm{d}\vec{\Theta}\\
    &=
      \prod_{b\in B} \int_{\Delta^K}
      P(\vec{\theta}^b|\vec{n}^b)
      \mathrm{d}\Pi_{\vec{\alpha}}(\vec{\theta}^b).
  \end{split}
\end{equation}
By solving the integrals we get
\begin{equation}
  \begin{split}
    P(E)
    &= \sum_{B\in \mathcal{P}(\mathcal{T})}
    P(B)
    \prod_{b\in B}\Beta(\vec{\alpha}^b)^{-1}
    \frac{\prod_{x \in
        \mathcal{X}}\Gamma(n_x^b+\alpha^b_x)}{\Gamma(\sum_{x\in
        \mathcal{X}}n^b_x+\alpha^b_x)}\\
    &=
    \sum_{B\in \mathcal{P}(\mathcal{T})}
    P(B)
    \prod_{b\in B}\frac{\Beta(\vec{n}^b+\vec{\alpha}^b)}{
      \Beta(\vec{\alpha}^b)}
  \end{split}
\end{equation}
which we can compute with the algorithm sketched in lemma
\ref{prombslemma}. We are also interested in computing the model posterior
\begin{equation}
  \begin{split}
    P(m_B|E)
    &= \frac{P(E|m_B) P(m_B)}{P(E)}\\
    &= \frac{P(E|m_B) P(m_B)}{\sum_{m_B \in \mathcal{M}}P(E|m_B) P(m_B)},
  \end{split}
\end{equation}
that tells us how probable a partition into $m_B$ bins is.

\section*{The moment problem and maximum entropy distributions}
From the Hausdorff moment theorem we know that a probability measure
on a compact set $[0,1]$ is fully characterized by its moments.  Hence
we would like to compute the first $n$ raw moments $\mu_i^b(x|E)$ for
each bin $b \in B$ and event $x \in \mathcal{X}$ by averaging over
\begin{equation}
  \begin{split}
    \mu_i^b(x|E,B)
    &=
    \int_{\Delta^{K-1}} (\theta^b_x)^i P(\vec{\theta}^b|E,B)
    \mathrm{d}\vec{\theta}^b \\
    &= \frac{P(X_{m_T+1}^b = x, \dots, X_{m_T+i}^b = x,E|B)}{P(E|B)}.
  \end{split}
\end{equation}
That is, we simply have to add $i$ events to our observations and
recompute the evidence to get the $i$th moment. We therefore get
\begin{equation}
  \begin{split}
    \mu_i^j(x|E)
    = \frac{\sum_{m_B\in \mathcal{M}}P(X_{m_T+1}^j = x, \dots, X_{m_T+i}^j =
      x,E|m_B) P(m_B)}{\sum_{m_B\in \mathcal{M}}P(E|m_B) P(m_B)},
  \end{split}
\end{equation}
for the full model where we add an event into the respective bin at
position $j$. The central moments $\bar{\mu}_i^b$ are given by
\begin{equation}
  \begin{split}
    \bar{\mu}^b_i(x|B) = \int_{\Delta^{K-1}}
    (\theta^b_x-\mu^b_1(x|E,B))^i
    P(\vec{\theta}^b|E,B)\mathrm{d}
    \vec{\theta}^b,
  \end{split}
\end{equation}
which can be computed from the raw moments with the binomial
transform
\begin{equation}
  \begin{split}
    \bar{\mu}_i = \sum_{k=0}^{i} {i \choose k} (-1)^{i-k}\mu_k(\mu_1)^{i-k}.
  \end{split}
\end{equation}

Having computed the first $n$ moments we would like to find the
corresponding maximum entropy (ME) distribution $p(x)$. Hence we would
like to compute the following optimization problem
\begin{equation}
  \begin{split}
    \operatorname*{maximize}_{p(x)} ~ H(X) &= - \int p(x)\log p(x) \mathrm{d}x, ~
    \text{subject to}\\
    \int x^ip(x) \mathrm{d}x &= \mu'_{i} ~
    \text{for} ~ i = 1, \dots, n
  \end{split}
\end{equation}
where $\mu'_0 = 1$ is the normalization constraint.
\begin{equation}
  L = \int p(x)\log p(x) \mathrm{d}x +
  \sum_{i=0}^{n}\lambda_0\left[ \int x^ip(x) \mathrm{d}x - \mu'_i \right]
\end{equation}

\begin{equation}
  p(x|\vec{\lambda}) = \exp(-1 -\sum_{i=0}^n\lambda_ix^i)
\end{equation}

\section*{Break Probabilities}
For the analysis of certain data it is often useful to know where a
new partition begins. For instance when we want to analyse
psychometric functions such knowledge could help to clearly define
changepoints. Also for the analysis of spike trains one often wants to
distinguish between tonic and phasic regimes of a stimulus response.
Hence we would like to compute the probability that a break $\Rsh_i$
occurs at the $i$th interval.
\begin{equation}
  \begin{split}
    P(\Rsh_i|E) =
    \sum_{B\in \mathcal{I}^i(\mathcal{T})}P(B|E) &=
      \frac{\sum_{B\in
          \mathcal{I}^i(\mathcal{T})} P(E|B) P(B)}{\sum_{B\in
               \mathcal{P}(\mathcal{T})} P(E|B) P(B)}\\
      &= \frac{\sum_{B\in
          \mathcal{I}^i(\mathcal{T})} P(E|B) P(B)}{P(E)},
  \end{split}
\end{equation}
where $\mathcal{I}^i(\mathcal{T}) \subseteq
\mathcal{P}(\mathcal{T})$ denotes the set of partitions that
have a break at the $i$th interval.

% \section*{Bayesian Hypothesis Testing}
% Bayes factor
% \begin{equation}
%   \begin{split}
%     K = \frac{P(E|m_1)}{P(E|m_2)}
%   \end{split}
% \end{equation}

\section*{Information-theoretic quantities}
By introducing a random variable $\mathcal{B}$ that represents the
outcome of a certain partition, we can compute our uncertainty about
which partition describes the data best, which is given by the Shannon
entropy
\begin{equation}
  \begin{split}
    \mathrm{H}(&\mathcal{B}|E)\\
    &= -\sum_{B\in \mathcal{P}(\mathcal{X})}P(B|E) \log P(B|E)\\
    &= -\sum_{B\in \mathcal{P}(\mathcal{X})}\frac{P(E|B)P(B)}{P(E)}
    \log
    \frac{P(E|B)P(B)}{P(E)}\\
%     &= -\sum_{B\in \mathcal{P}(\mathcal{X})}
%     \frac{P(E|B)P(B)}{\sum_{B\in
%         \mathcal{P}(\mathcal{X})}P(E|B)P(B)}
%     \log
%     \frac{P(E|B)P(B)}{\sum_{B\in
%         \mathcal{P}(\mathcal{X})}P(E|B)P(B)}\\
    &=
    -\frac{1}{P(E)}
    \sum_{B\in \mathcal{P}(\mathcal{X})}
    P(B)
    \prod_{b\in
      B}\frac{\Beta(\vec{n}^b+\vec{\alpha}^b)}{\Beta(\vec{\alpha}^b)}\\
    &\quad\quad\quad
    \left[
      \log
      \frac{P(B)}{P(E)} +
      \sum_{b \in B}
      \log
      \frac{\Beta(\vec{n}^b+\vec{\alpha}^b)}{\Beta(\vec{\alpha}^b)}
    \right]\\
    &=
    \frac{1}{P(E)}
    \sum_{B\in \mathcal{P}(\mathcal{X})}
    P(B)
    \prod_{b\in B}\frac{\Beta(\vec{n}^b+\vec{\alpha}^b)}{\Beta(\vec{\alpha}^b)}
    \sum_{b \in B}
    \left[-\log
    \frac{\Beta(\vec{n}^b+\vec{\alpha}^b)}{\Beta(\vec{\alpha}^b)}\right]\\
    &-
    \frac{1}{P(E)}
    \sum_{B\in \mathcal{P}(\mathcal{X})}
    P(B)
    \log\frac{P(B)}{P(E)}
    \prod_{b\in B}
    \frac{\Beta(\vec{n}^b+\vec{\alpha}^b)}{\Beta(\vec{\alpha}^b)}
  \end{split}
\end{equation}
We use the algorithm sketched in lemma \ref{eprombslemma} to
approximate the first term of the last expression.

\section*{Finite and partial exchangeability}
\cite{Pitman1995}, \cite{Jaynes1986}

\section*{Information theoretic analysis of biological systems}
Shannon's original work on information theory (cf. \cite{Shannon1948})
has shown us how to optimally design communication systems and might
also provide us with a unifying framework to understand information
processing in biological systems\dots

\section*{Spike train analysis}
\begin{figure}[ht]
  \centering
  \includegraphics[width=.9\textwidth]{../data/data8.pdf}
  \caption{
    Bayesian analysis of 30 repeated recordings from a neuron ($m_T =
    30$). Time was discretized into intervals of length $\Delta t =
    1ms$. Models with up to 20 bins were included in the computation,
    i.e. $\mathcal{M} = \{1, 2, \dots, 20\}$ and $P(m_B) =
    1/|\mathcal{M}|$. For the prior we chose $\vec{\alpha} = (1,32)$ for
    all intervals. The red line in the second plot shows the estimated
    probability to observe a spike $S_i$ within the $i$th bin of size
    1ms. The standard deviation is shown as a dashed line. Skews were
    computed but turned out to be very small (max. $10^{-7}$). Break
    probabilities $\Rsh_i$ were computed for each interval $i$, shown in
    green. The third plot shows the model posterior for each $m_B \in
    \mathcal{M}$. Source code:
    \url{http://git.debian.org/?p=users/philipp/bayes-toolbox;a=summary}.
  }
  \label{fig:1}
\end{figure}
When analysing spike trains with the framework discussed above we make
the assumption that we deal with a Poisson process, i.e. spikes are
independently generated from a renewal process with exponential
interspike interval distribution. This assumption is of course
violated for most recordings, especially for those from higher
cortical areas such as FEF where interspike intervals are clearly
gamma distributed\footnote{Since gamma distributed interspike
intervals can be constructed from a Poisson process by removing every
$i$th spike it is self-evident that they are a result of inhibition.}
\footnote{This is another argument against the standard Poisson neuron
model, which clearly does not capture the essentials of neuronal
processing.  The main argument is however that although the interspike
interval distribution in sensory areas looks exponential, it is not a
characteristic of neuronal processing but of the input statistics.}.
To at least capture refractory periods the notion of Markov or partial
exchangeability introduced by de Finetti might be useful.

For an information theoretic analysis of spike trains
(e.g. \cite{Strong1998}) and receptive field models
(e.g. \cite{Adelman2003}) it is important to have a good estimate for
entropies and mutual information. Assuming a Poisson process greatly
affects the estimate, which is why most approaches try to sample the
joint probability distribution instead of assuming independence. For
instance the method presented by \cite{Nemenman2002} (also
\cite{Nemenman2004}) is based on a Bayesian analysis where a flat
prior for the entropy distribution is derived. Other methods
(e.g. \cite{Grassberger1988, Grassberger2008}) merely use a correction
term for the biased maximum-likelihood estimate and are reported to be
less successful. An interesting approach to the estimation of
entropies from the joint distribution could result from the Pitman
continuum of inductive methods. There might also be a way to represent
spike trains in form of interspike intervals such that we can use the
Johnson-Carnap continuum as discussed above.

\section*{Markov exchangeability on spike trains}
For the analysis of spike trains we made the assumption that events
between the invervals $\mathcal{T} = \{t_1, t_2, \dots, t_L\}$ of size
$\Delta t$ are independent. This is true if we assume that spike
trains are generated from a Poisson process. To account for refractory
periods we will extend the model to include Markov dependencies between
the intervals. Hence we want to infer the transition probabilities for

\begin{center}
  \begin{tikzpicture}
    \tikzstyle{vertex}=[circle,minimum size=25pt,draw]
    \node[vertex] (A-1) at (1,0) {$0$};
    \node[vertex] (A-2) at (4,0) {$1$};
    %% connections between both nodes
    \draw[->,thick] (A-1) .. controls +(+1.5,+0.5) .. node[above,sloped] {$p_{01}$} (A-2);
    \draw[->,thick] (A-2) .. controls +(-1.5,-0.5) .. node[above,sloped] {$p_{10}$} (A-1);
    %% self-connections
    \draw[->,thick] (A-1) .. controls +(-1.5,0.5) and +(-1.5,-0.5) .. node[left]  {$p_{00}$} (A-1);
    \draw[->,thick] (A-2) .. controls +(1.5,-0.5) and +(1.5,0.5)   .. node[right] {$p_{11}$} (A-2);
  \end{tikzpicture}
\end{center}
where the state $1$ denotes a spike. We assume that the refractory
period affects only a single proximal interval and set $p_{11} = 0$,
hence $p_{01} = p_{10}$. The sufficient statistic is given by 
\begin{equation}
  \vec{n}(E) =
  \begin{pmatrix}
    n_{00} & n_{01}\\
    n_{10} & n_{11}
  \end{pmatrix},
\end{equation}
where $n_{ij}$ counts the number of transitions from $i$ to $j$.
\begin{equation}
  p(x_1, \dots, x_{L}) =
  \int_{\Delta^L}
  \theta_{00}^{n_{00}}
  \theta_{01}^{n_{01}}
  \theta_{10}^{n_{10}}
  \theta_{11}^{n_{11}}
  \mathrm{d} \Pi_{\vec{\alpha}} (\vec{\theta})
\end{equation}

\section*{Rate-distortion theory and the information bottleneck
  method for efficient neural coding}
\cite{Schneidman2001}, \cite{Tishby1999}

\section*{Self-organized criticality in biological systems}
\cite{Mora2010}
\cite{Monteforte2010}

\section*{Bayesian spiking neurons}

\section*{Relations between adaptive sampling and reinforcement
  learning}
Multi-armed bandit

\section*{de Finetti extension of typical sets}
Shannon's theory relies on the notion of typical sets where an
i.i.d. and ergodic sequence $X_1, X_2, \dots, X_{m_T}$ is drawn from a
distribution with entropy $H(X)$ over a finite alphabet
$\mathcal{X}$. To define typical sets we need a generalization of the
strong law of large numbers called the asymptotic equipartition
property (AEP), given by
\begin{equation}
  \begin{split}
    -\frac{1}{m_T} \log p(x_1, \dots, x_{m_T})
    &= - \frac{1}{m_T} \log \prod_{i=1}^{m_T} p(x_i)\\
    &= - \frac{1}{m_T} \sum_{i=1}^{m_T} \log p(x_i)\\
    &\rightarrow - E[\log p(x)] = H(X),
  \end{split}
\end{equation}
almost surely as $m_T \rightarrow \infty$.  Hence we have $p(x_1,
\dots, x_{m_T}) \approx 2^{-n H(X)}$. We can now define the typical
set as
\begin{equation}
  \begin{split}
    A^{(m_T)}_\epsilon
    &= \left\{ (x_1, \dots, x_{m_T}) : \left| \frac{n_i}{m_T} - p_i \right| < \epsilon \right\}\\
    &= \left\{ (x_1, \dots, x_{m_T}) : \left| \log \frac{1}{p(x_1, \dots, x_{m_T})} - n H(X) \right| < \epsilon \right\},
  \end{split}
\end{equation}
which includes all sequences that satisfy
\begin{equation}
  2^{-n (H(X) + \epsilon)} \le p(x_1, \dots, x_{m_T}) \le 2^{-n (H(X) - \epsilon)}.
\end{equation}
We can generalize this definition with de Finetti's notion of
exchangeable (non-ergodic) sequences, we then have
\begin{equation}
  \begin{split}
    -\frac{1}{m_T} \log p(x_1, \dots, x_{m_T})
    &=
    - \frac{1}{m_T} \log
    \int_{\Delta^K} p(x_1, \dots, x_{m_T}|\vec{\theta})
    \mathrm{d} \Pi_{\vec{\alpha}}(\vec{\theta})\\
    &=
    - \frac{1}{m_T} \log
    \int_{\Delta^K}
    \prod_{x \in \mathcal{X}}
    \theta_x^{n_x}
    \mathrm{d} \Pi_{\vec{\alpha}}(\vec{\theta})\\
    &=
    - \frac{1}{m_T} \log
    \frac{1}{\Beta(\vec{\alpha})}
    \int_{\Delta^K}
    \prod_{x \in \mathcal{X}}
    \theta_x^{n_x+\alpha_x-1}
    \mathrm{d} \vec{\theta}\\
    &=
    - \frac{1}{m_T} \log
    \frac{\Beta(\vec{n}+\vec{\alpha})}{\Beta(\vec{\alpha})}\\
    &\rightarrow
    - \lim\limits_{m_T \rightarrow \infty}
    \frac{1}{m_T} \log \Beta(\vec{n}+\vec{\alpha})\\
  \end{split}
\end{equation}
%What does this result mean?

\section*{Adaptive sampling}
\begin{figure}[ht]
  \centering
  \includegraphics[width=.48\textwidth,page=1]{../data/data12.pdf}
  \includegraphics[width=.48\textwidth,page=2]{../data/data12.pdf}\\
  \includegraphics[width=.48\textwidth,page=3]{../data/data12.pdf}
  \includegraphics[width=.48\textwidth,page=4]{../data/data12.pdf}
  \caption{
    Adaptive sampling from a given ground truth.
  }
  \label{fig:2}
\end{figure}
