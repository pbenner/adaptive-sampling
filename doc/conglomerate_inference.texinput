% -*- mode: latex -*-
%
\section*{Bayesian inference and the meaning of probability}
As an extension of logic, Bayesian theory provides us with a framework
for optimal reasoning about uncertain statements\footnote{Whether or
not humans do reason in a Bayesian optimal way is for instance debated
by Gigerenzer et al.}. We use Cox's theorem (cf. \cite{Cox1946,
Cox1961, Jaynes2003}) to interpret Kolmogorov's axioms
of probability (cf. \cite{Kolmogorov1933}) as a formal system of
logic\footnote{\cite{Renyi1955} generalized Kolmogorov's
axioms such that probability spaces are directly defined through
conditional probability measure. Is it possible to rewrite Cox's
theorem to support R\'eyni probability spaces?}. In Bayesian
probability theory, the meaning of probability is of epistemic
nature. That is, probabilities represent a degree of belief about a
statement or hypothesis rather than a physical property, which also
means that probabilities are considered to be subjective. An extreme
view, introduced by de Finetti, states that probabilities are purely
subjective and do not exist as physical properties. Other viewpoints
are less extreme and accept the existence of such probabilities. A new
approach called objective Bayesian inference, which started with
\cite{Jaynes1957a, Jaynes1957b}, made the assignment of probabilities
less subjective in the sense that one chooses the maximum entropy
distribution given a set of observations. From a technical point of
view it is of course not always easy to compute this distribution
called a Gibbs measure (cf. \cite{Jaynes1957a}).

\section*{Bayesic Definitions}
A probability space is given as a triple $(\Omega, \mathcal{F}, P)$,
where $\Omega$ denotes the set of outcomes (or elementary events),
$\mathcal{F} = \mathcal{B}(\Omega)$ the Borel $\sigma$-algebra of
possible events and $P : \mathcal{F} \rightarrow [0,1]$ a probability
measure with $P(\Omega) = 1$. We define a random variable $X : \Omega
\rightarrow \mathcal{X} \subseteq \mathbb{R}$ as a measurable map from
$(\Omega, \mathcal{F})$ to $(\mathcal{X}, \mathcal{G})$ with
\begin{equation}
  X^{-1}(B) = \{ \omega : X(\omega) \in B \} \in \mathcal{F} ~
  \text{for all} ~ B \in \mathcal{G}.
\end{equation}
For a random variable $X$ from a probability space $(\Omega,
\mathcal{F}, P)$ to a measure space $(\mathcal{X}, \mathcal{G})$ we
define the expected value as
\begin{equation}
  E[X] = \int_\Omega X(\omega) \dd P(\omega),
\end{equation}
but to compute the integral we need to move to a different space. We
define the distribution $\mu_X = P \circ X^{-1}$ as the measure
induced by the random variable $X$, denoted as $X \sim \mu_X$. For a
measurable function $f$ from $(\mathcal{X}, \mathcal{G})$ to
$(\mathbb{R}, \mathcal{R})$ the expectation is given by
\begin{equation}
  \begin{split}
  E_f[X] &= \int_\Omega f(X(\omega)) P(\dd\omega)\\
         &= \int_{\mathcal{G}} f(y) (P \circ X^{-1})(\dd y)\\
         &= \int_{\mathcal{G}} f(y) \mu_X(\dd y).
%          = \int_{\mathcal{G}} f(y) \dd \mu(y).
  \end{split}
\end{equation}
The distribution function of a random variable is given by
\begin{equation}
  F(x) = \mu_X(-\infty, x],
\end{equation}
for which we define the Stieltjes integral as
\begin{equation}
  \int_{\mathcal{G}} f(y) \dd F(y)
  = \int_{\mathcal{G}} f(y) \mu(\dd y).
\end{equation}

\begin{theorem}[Radon-Nikodym]
  Any absolute continuous measure $\mu_X$ with respect to some
  positive measure $\mu$  ($\mu_X(A) = 0$ whenever $\mu(A) =0$) can be
  represented as
  \begin{equation}
    \mu_X(A) = \int_A f_X(x) \mu(\dd x),
  \end{equation}
  where $f_X$ is the density for the distribution $\mu_X$. We
  call $f_X$ the Radon-Nikodym derivative of $\mu_X$ with respect to
  the carrier measure $\mu$.
\end{theorem}

\begin{corollary}[Radon-Nikodym representation of Bayes' formula]
  Given that the  posterior distribution $\mu_{\Theta|E}$ is
  absolutely continuous with respect to the prior distribution
  $\mu_{\Theta}$, we can represent Bayes' rule of conditioning as
  \begin{equation}
    \frac{\dd \mu_{\Theta|E}}{\dd \mu_{\Theta}} (\theta|E) =
    \frac{f(E|\theta)}{f(E)}.
  \end{equation}
\end{corollary}

\section*{Inference}
Given a sequence of $m_T$ observations $E = \{X_1, X_2, \dots,
X_{m_T}\}$ how can we make predictions about future
observations?
%\footnote{This is known as \textit{Hume's problem of
%induction} and was first answered by de Finetti's representation
%theorem for problems where sequences of events are exchangeable.}
That is, how can we formulate a hypothesis $\mathcal{H}$ about the
laws behind the sequence of observations and compute the probability
of our hypothesis conditional on all the evidence $E$ that we
gathered? From \textit{Bayes's rule of conditioning}
\begin{equation}
  P(\mathcal{H}|E) = \frac{P(E,\mathcal{H})}{P(E)},
\end{equation}
we know how to incorporate our observations into our
probability function. But to fully answer the question of induction we
have to clearly define the patterns or symmetries that we expect
behing the sequence of observations. This will lead us to a theorem by
de Finetti that tells us how the joint probability mass function for
$P(E)$ can be represented. A corresponding mode of inductive inference
is then given by the \textit{Johnson-Carnap continuum}. Several
drawbacks of the Johnson-Carnap continuum have been extensively
discussed in literature, one of which is the assumption that the
number of possible outcomes $K$ is known in advance. Several
generalizations were developed, for instance the one by Pitman which
overcomes this limitation. However, we will first focus on the
Johnson-Carnap continuum. More general forms will become useful later
when we want to estimate entropies from severely undersampled
probability spaces.
\begin{definition}[Finite and infinite exchangeability]
  Let $X_1, X_2, \dots$ denote sequence of random events,
  where each $X_i$ is a random variable from a probability space
  $(\Omega, \mathcal{F}, P)$ to a measure space $(\mathcal{X},
  \mathcal{G})$. A finite sequence of $m_T$ events is said to be
  $m_T$-exchangeable with respect to the probability measure $P$ if
  \begin{equation}
    P(X_1 = x_1, \dots, X_{m_T} = x_{m_T}) = P(X_{1} = x_{\pi(1)},
    \dots, X_{m_T} = x_{\pi(m_T)})
  \end{equation}
  with $x_i \in \mathcal{X}$ and for all permutations $\pi$ defined on
  the set $\{1, \dots, m_T\}$. An infinite sequence is exchangeable if
  it is $m_T$-exchangeable for all $m_T \in \mathbb{N}$.
  (cf. \cite{Finetti1974})
\end{definition}

\begin{lemma}
  Exchangeable sequences are strictly stationary, i.e.
  \begin{equation}
    P(X_1 = x_1, \dots, X_{m_T} = x_{m_T}) = P(X_{1} = x_{1+\tau},
    \dots, X_{m_T} = x_{m_T+\tau})
  \end{equation}
  for all $\tau \in \mathbb{N}$.
\end{lemma}

Note that de Finetti's representation theorem is a form of ergodic
decomposition for stationary (non-ergodic) random processes.

\begin{example}[Exchangeability]
  Let $X_1, X_2, \dots$ be an infinite sequence of binary random
  events with $\mathcal{X} = \{0, 1 \}$ and $X_i \sim p$, where we
  define $p(0) = 1-p^*$ and $p(1) = p^*$. We will draw $p^*$ from a
  discrete uniform distribution on $\{p_1, p_2\}$. Hence we have
  \begin{equation}
    \lim\limits_{m_T \rightarrow \infty}
    \frac{1}{m_T}\sum_i^{m_T} X_i =
    \begin{cases}
      p_1 & \text{with probability} ~ 1/2\\
      p_2 & \text{with probability} ~ 1/2,
    \end{cases}
  \end{equation}
  from the law of large numbers. The sequence is exchangeable and
  therefore independent conditional on $p$, i.e.
  \begin{equation}
    P(X_1 = x_1, X_2 = x_2, \dots, X_{m_T} = x_{m_T}) =
    \prod_{i=1}^{m_T}P(X_i = x_i | p).
  \end{equation}
\end{example}
Assuming exchangeability for a sequence of observations makes the
assignment of probabilites a tractable problem and still allows
inductive reasoning. Note that this is not the case when we assume
that observations are independently generated, i.e.
\begin{equation}
    P(X_1 = x_1, \dots, X_{m_T} = x_{m_T}) = P(X_{1} = x_{\pi(1)})
    \dots P(X_{m_T} = x_{\pi(m_T)}),
\end{equation}
as it is often assumed in orthodox statistics. If independence were
truly the case we would not be able to draw any conclusions about
future events given our observations.

\begin{theorem}[Generalized de Finetti's representation theorem]
  \label{inf:frm}
  Let $X_1, X_2, \dots$ denote an infinitely exchangeable sequence of
  random variables from a probability space $(\Omega, \mathcal{F}, P)$
  to a measure space $(\mathcal{X}, \mathcal{G})$. There exists a
  distribution function $\Pi_{\valpha}$ such that the joint mass
  function $p(x_1, \dots, x_{m_T}) = P(X_1 = x_1, \dots, X_{m_T} =
  x_{m_T})$ has the form
  \begin{equation}
    \begin{split}
      \label{eq:frt}
      p(x_1, \dots, x_{m_T}) &=
      \int_{\Delta^K} p(x_1, \dots, x_{m_T}|\vec{\theta})
      \dd \Pi_{\valpha}(\vec{\theta})\\
      &=
      \int_{\Delta^K}
      \prod_{x \in \mathcal{X}}
      \theta_x^{n_x}
      \dd \Pi_{\valpha}(\vec{\theta}), ~ \text{with}\\
      \Pi_{\valpha}(\vec{\theta}) &= \lim\limits_{m_T \rightarrow \infty}
      \Pr \left\{\frac{n_x}{m_T} \le \theta_x : x \in \mathcal{X} \right\},
    \end{split}
  \end{equation}
  where $\vec{n}(E) = (n_x)_{x \in \mathcal{X}}$ denotes the count
  statistic for every occurence of $x \in \mathcal{X}$ in $E$, $K =
  |\mathcal{X}|$, and $\Delta^K := \{\vec{\theta} = (\theta_x)_{x \in
    \mathcal{X}} \in \mathbb{R}^K : \sum_{x \in \mathcal{X}} \theta_x
  = 1 ~ \text{and} ~ \theta_x \ge 0~\forall x \in \mathcal{X} \}$
  denotes the $K-1$-dimensional probability simplex. If
  $\Pi_{\valpha}$ is a non-decreasing real function we get the
  Riemann-Stieltjes integral and we denote its density
  $f_{\Pi_{\valpha}}$. (cf. \cite{Finetti1974, Hewitt1955})
\end{theorem}
\begin{proof}
  Todo
\end{proof}
In other words, conditional on $\vec{\theta}$ the joint probability
mass function $p(x_1, \dots, x_{m_T})$ can be written as an integral
mixture of multinomial probabilities. Note that for $K=1$ equation
$\ref{eq:frt}$ computes the $n_x$th moment of $f_{\Pi_{\valpha}}$. In
Bayesian inference, the limiting distribution function $\Pi_{\valpha}$
is interpreted as a prior distribution. A common choice for
$\Pi_{\valpha}$ is a conjugate prior which substantially simplifies
the computation of the posterior distribution. In the following we
show how such a prior can be justified from inductive principles such
that theorem \ref{inf:frm} holds for a proper choice of $\valpha$.

\begin{theorem}[Johnson-Carnap continuum]
  \label{inf:jcc}
  Let $X_1, X_2, \dots$ denote an infinite sequence of random
  variables from a probability space $(\Omega, \mathcal{F}, P)$ to a
  measure space $(\mathcal{X}, \mathcal{G})$. Assume that
  \begin{itemize}
  \item[(i)  ] $K = |\mathcal{X}| \ge 3$,
  \item[(ii) ] $P(X_1 = x_1, \dots, X_{m_T} = x_{m_T}) > 0$ for all $x_i \in
    \mathcal{X}$, and
  \item[(iii)] there exists a sufficient statistic $g_x(n_x,n)$ such
    that $P(X_{m_T+1} = x_{m_T+1} | \vec{n}(E)) = g_x(n_x,m_T)$, where $\vec{n}(E) =
    (n_x)_{x \in \mathcal{X}}$.
  \end{itemize}
  Then either the outcomes are independent or there exist positive
  constants $\{\alpha_x : x \in \mathcal{X}\}$ such that for all $n \ge 1$
  \begin{equation}
    g_x(n_x, m_T) = \frac{n_x + \alpha_x}{m_T + \sum_{x^* \in \mathcal{X}}
      \alpha_{x^*}}.
  \end{equation}
  (cf. \cite{Johnson1932, Zabell1982})
\end{theorem}
\begin{proof}
  Todo
\end{proof}
Note that if $X_1, X_2, \dots$ is exchangeable then $P(X_{m_T+1} =
x_{m_T+1} | \vec{n}(E)) = P(X_{m_T+1} = x_{m_T+1} | n_x) = g_x(n_x,m_T)$. The
Johnson-Carnap continuum of inductive methods not only provides us
with a sufficient statistic for inductive reasoning, but also
identifies a unique prior distribution for de Finetti's representation
theorem.
\begin{corollary}
  Under the conditions (i-iii) of theorem \ref{inf:jcc} the prior
  distribution function $\Pi_{\valpha}$ belongs to the \textit{Dirichlet
    familiy} $\Dir(\valpha)$ with density $f_{\Dir_{\valpha}}$, i.e.
  \begin{equation}
    \begin{split}
      \dd \Pi_{\valpha}(\vec{\theta}) =
      f_{\Dir(\valpha)}(\vec{\theta})\dd \vec{\theta} &=
      \frac{1}{\Beta(\valpha)}
      \prod_{x \in \mathcal{X}} \theta_x^{\alpha_x -1}
      \dd \vec{\theta}, ~ \text{where}\\
      \Beta(\valpha) &= \frac{\prod_{x \in \mathcal{X}}
        \Gamma(\alpha_x)}{\Gamma(\sum_{x
          \in \mathcal{X}}\alpha_x)} =
      \underbrace{\int_{\Delta^K}\prod_{x \in
          \mathcal{X}} \theta^{\alpha_x-1}
        \dd \vec{\theta},}_{\text{multinomial Euler
          integral}}
    \end{split}
  \end{equation}
  and $\Gamma(z) = \int_0^\infty t^{z-1}e^{-t}\dd t$ denotes
  the Gamma function.
\end{corollary}
\begin{proof}
  Todo
\end{proof}
The Dirichlet prior can also be obtained by assuming that a
sequence of observations $X_1, \dots, X_{m_T}$ is neutral
(cf. \cite{James1980, Walker1999}).
\begin{lemma}[Properties of the Dirichlet distribution]
  Let $\Xf = (X_1, \dots, X_{m_T}) \sim \Dir(\valpha)$ then
  \begin{enumerate}
  \item (Expectation and variance)
    \begin{equation}
      \begin{split}
        \E[X_i]   &= \frac{\alpha_i}{\sum_{j=1}^{m_T}\alpha_j}, ~ \text{and}\\
        \Var[X_i] &= \frac{\E[X_i](1-E[X_i])}{\sum_{j=1}^{m_T}\alpha_j
          + 1}
      \end{split}
    \end{equation}
  \item (Predictive distribution)
    \begin{equation}
      P(X_{m_T+1} = x| X_1, X_2 \dots, X_{m_T}) =
      \frac{n_x + \alpha_x}{m_T + \sum_{x^* \in \mathcal{X}} \alpha_{x^*}}
    \end{equation}
  \item (Aggregation) If $r_1, \dots, r_n$ are integers with $0 < r_1
    < \dots < r_k = m_T$, then
    \begin{equation}
      \left(\sum_{i=1}^{r_1}X_1, \sum_{i=r_1+1}^{r_2}X_2, \dots, \sum_{i=r_{n-1}+1}^{r_n}X_{m_T}\right) \sim
      \Dir\left(\sum_{i=1}^{r_1}\alpha_1, \sum_{i=r_1+1}^{r_2}\alpha_2, \dots, \sum_{i=r_{n-1}+1}^{r_n}\alpha_{m_T}\right)
    \end{equation}
  \item (Multinomial conjugacy) Let $X \sim \Dis(\theta_1, \dots,
    \theta_K)$ be a random variable that takes value $i \in \{1,
    \dots, K\}$ with probability $\theta_i$ and $(\theta_1, \dots,
    \theta_K) \sim \Dir(\alpha_1, \dots, \alpha_K)$, then
    \begin{equation}
      \begin{split}
        X &\sim \Dis\left(\frac{\alpha_1}{\sum_{i=1}^{K} \alpha_i},
        \dots, \frac{\alpha_K}{\sum_{i=1}^{K} \alpha_i} \right),\\
        (\theta_1, \dots, \theta_K) | X &\sim \Dir(\alpha_1 +
        \delta_1(X), \dots, \alpha_K + \delta_K(X)),
      \end{split}
    \end{equation}
    with $\delta_i$ denoting the Dirac measure.
  \end{enumerate}
\end{lemma}
To summarize our results, under the symmetry assumptions that we
introduced about the sequence of observations $E = (X_1, X_2, \dots,
X_{m_T})$ we are able to state a hypothesis $\mathcal{H}$ in form of the
parameters $\vec{\theta}$ that represent our beliefs about the
underlying process. Any knowledge that we have before we look at the
data is represented in the parameters $\valpha$. The probability
of a hypothesis is then given by
\begin{equation}
  \begin{split}
    P(\vec{\theta}|E)
    &= \frac{P(E,\vec{\theta})}{P(E)}
     =
    \frac{
      P(E|\vec{\theta})
      f_{\Pi_{\valpha}}(\vec{\theta})}{
      \int_{\Delta^K}
      P(E|\vec{\theta}')
      \dd \Pi_{\valpha}(\vec{\theta}')}\\
    &=
    f_{\Dir(\vec{n}(E)+\valpha)}(\vec{\theta}).
  \end{split}
\end{equation}
The update of our beliefs after seeing the data is therefore a mere
shift in the parameters of the Dirichlet distribution
(i.e. $\Dir(\vec{\theta};\valpha) \mapsto \Dir(\vec{\theta};
\vec{n}(E)+\valpha)$) as it is a conjugate prior to the multinomial
distribution. Intuitively, the Dirichlet distribution states our
belief that the probabilities of $K$ rival events $x \in \mathcal{X}$
are $(\theta_x)_{x \in \mathcal{X}}$ given that each event has been
observed $\alpha_x - 1$ times. Since $\vec{\theta} \sim
\Dir(\vec{n}(E)+\valpha)$ we can easily compute the
expected value of $\vec{\theta}$ given our observations, i.e.
\begin{equation}
  \begin{split}
    \hat{\theta}_x = \mathrm{E}[\theta_x|E] &=
    \int_{\Delta^{K-1}}\theta_x P(E|\vec{\theta})
    \dd \Pi_{\valpha}(\vec{\theta})\\
    &=
    \frac{
      \int_{\Delta^K} P(X_{m_T+1} = x,E|\vec{\theta})
      \dd \Pi_{\valpha}(\vec{\theta})
    }{
      \int_{\Delta^K} P(E|\vec{\theta})
      \dd \Pi_{\valpha}(\vec{\theta})
    }\\
    &= g_x(n_x,m_T)
    = \frac{n_x + \alpha_x}{m_T + \sum_{x' \in \mathcal{X}} \alpha_{x'}},
  \end{split}
\end{equation}
as it was shown in theorem \ref{inf:jcc}. The estimate is known as the
\textit{extended Bayes-Laplace rule of succession}. It is equivalent
to the frequentist's estimate ($\hat{\theta}_x = \frac{n_x}{m_T}$) if
$\alpha_x = 0 ~ \forall x \in \mathcal{X}$. If $\alpha_x = 1 ~ \forall
x \in \mathcal{X}$ we get \textit{Laplace's rule of succession}.

\begin{example}[Radon-Nikodym derivative for Dirichlet priors]
  For any $A \subseteq \Delta^K$ the measure of the Dirichlet family is
  given by
  \begin{equation}
    \mu_{Dir(\alpha)} = \frac{1}{\Beta(\alpha)} \int_A \prod_{x \in
      \mathcal{X}} \theta_x^{\alpha - 1} \dd \vtheta.
  \end{equation}
  We can compute the Radon-Nikodym derivative of the Dirichlet
  posterior with respect to the Dirichlet prior as a carrier measure,
  given by
  \begin{equation}
    \begin{split}
    \frac{\dd\mu_{Dir(\vec{n}+\valpha)}}{\dd\mu_{Dir(\valpha)}}
    (\vtheta|E)
    &=
    \frac{\prod_{x \in \mathcal{X}} \theta_x^{n_x}}{
      \frac{1}{\Beta(\valpha)} \int_{\Delta^K} \prod_{x \in \mathcal{X}}
      \theta_x'^{n_x + \alpha_x - 1}\dd \vtheta'}\\
    &=
    \frac{\Beta(\valpha)}{\Beta(\vec{n}+\valpha)} \prod_{x \in
      \mathcal{X}} \theta_x^{n_x}.
    \end{split}
  \end{equation}
\end{example}

\begin{example}[P\'olya's Urn]
  Suppose we draw white and black balls at random from an urn
  (i.e. $\Omega = \{ \text{white}, \text{black} \}$). Each time we
  draw a ball we put $k$ balls of the same color back into the
  urn. Thus we have a sequence of exchangeable events $X_1, X_2,
  \dots$ with $X_i( \text{white} ) = 0$ and $X_i( \text{black} ) =
  1$. Hence
  \begin{equation}
    \lim\limits_{m_T \rightarrow \infty} \frac{1}{m_T}
    \sum_{i=1}^{m_T} X_i = \theta \sim \Beta \left(\frac{b}{k},
      \frac{w}{k} \right),
  \end{equation}
  where $b$ is the initial number of black balls and $w$ the number of
  white balls. Conditional on $\theta$ the sequence $X_1, X_2, \dots$ is
  independent.
\end{example}

The concept of exchangeability was generalized by de Finetti to the
notion of $L$-fold partial exchangeablility, where we allow events of
a sequence $X_1, X_2, \dots$ to belong to $L$ different classes.
We can therefore split the sequence into $L$ sequences $(\vec{X^1},
\vec{X^2}, \dots,  \vec{X^L})$ where each $\vec{X^i}$ represents the
events that we assign to the $i$th class. Within the classes the
ordering provides no relevant information. This leads us to the
following definition:

\begin{definition}[Partial exchangeability]
  We call a tuple $(\vec{X^l})_{l=1}^L$ of infinitely exchangeable
  sequences $\vec{X^l} = X^l_1, X^l_2, \dots$ $L$-fold partially
  exchangeable.
\end{definition}

\begin{theorem}[Representation theorem for partially exchangeable
  sequences]
  Let $(\vec{X}^l)_{l=1}^L$ denote an $L$-fold partially exchangeable
  sequence of random variables from a probability space $(\Omega,
  \mathcal{F}, P)$ to a measure space $(\mathcal{X},
  \mathcal{G})$. There exists a distribution function
  $\Pi_{\valpha}^L$ such that the joint mass function $p(\vec{x}^1,
  \dots, \vec{x}^{L}) = P(\vec{X}^1 = \vec{x}^1, \dots, \vec{X}^L =
  \vec{x}^L)$ has the form
  \begin{equation}
    \begin{split}
      p(\vec{x}^1, \dots, \vec{x}^{L}) &=
      \int_{\Delta_L^K}
      \left\{
      \prod_{l=1}^L
      p(x^l_1, \dots, x^l_{m_T}|\vec{\theta}^l)
      \right\}
      \dd \Pi_{\valpha}^L(\vec{\Theta})\\
      &=
      \int_{\Delta_L^K}
      \left\{
      \prod_{l=1}^L
      \prod_{x \in \mathcal{X}}
      (\theta_x^l)^{n_x^l}
      \right\}
      \dd \Pi_{\valpha}^L(\vec{\Theta}), ~ \text{with}\\
      \Pi_{\valpha}^L(\vec{\Theta}) &= \lim\limits_{m_T \rightarrow \infty}
      \Pr \left\{\frac{n_x^l}{m_T} \le \theta^l_x : x \in \mathcal{X}
        ~ \text{and} ~ l=1,2, \dots, L \right\},
    \end{split}
  \end{equation}
  where $\Delta^K_L = \varprod_{i=1}^L \Delta^K$. We have strong
  partial exchangeability if we can write the joint probability mass
  function as
  \begin{equation}
      p(\vec{x}_1, \dots, \vec{x}_{L}) =
      \prod_{l=1}^L
      \int_{\Delta^K}
      \prod_{x \in \mathcal{X}}
      \theta_x^{n_x^l}
      \dd \Pi_{\valpha}(\vec{\theta}),
  \end{equation}
  i.e. the classes are independent.
\end{theorem}
