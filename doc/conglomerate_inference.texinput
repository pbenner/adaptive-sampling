% -*- mode: latex -*-
%
\section*{Bayesian inference and the meaning of probability}
As an extension of logic, Bayesian theory provides us with a framework
for optimal reasoning about uncertain statements\footnote{Whether or
not humans do reason in a Bayesian optimal way is for instance debated
by Gigerenzer et al.}. For an axiomatic justification of the rules of
probability we use Cox's theorem (cf. \cite{Cox1946, Cox1961,
Jaynes2003}) as opposed to the axioms proposed by Kolmogorov. In
Bayesian probability theory, the meaning of probability is of
epistemic nature. That is, probabilities represent a degree of belief
about a statement or hypothesis rather than a physical property, which
also means that probabilities are considered to be subjective. An
extreme view, introduced by de Finetti, states that probabilities are
purely subjective and do not exist as physical properties. Other
viewpoints are less extreme and accept the existence of such
probabilities. A new approach called objective Bayesian inference,
which started with \cite{Jaynes1957a, Jaynes1957b}, made the
assignment of probabilities less subjective in the sense that one
chooses the maximum entropy distribution given a set of
observations. From a technical point of view it is of course not
always easy to compute this distribution called a Gibbs measure
(cf. \cite{Jaynes1957a}).

\section*{Bayesic Definitions}
A probability space is given as a triple $(\Omega, \mathcal{F}, P)$,
where $\Omega$ denotes the set of outcomes (or elementary events),
$\mathcal{F} \subseteq \mathcal{P}(\Omega)$ a set of possible events
and $P : \mathcal{F} \rightarrow [0,1]$ a probability measure with
$P(\Omega) = 1$. We define a random variable $X : \Omega \rightarrow
\mathcal{X} \subseteq \mathbb{R}$ as a measurable map from $(\Omega,
\mathcal{F})$ to $(\mathcal{X}, \mathcal{F}')$ with
\begin{equation}
  X^{-1}(B) = \{ \omega : X(\omega) \in B \} \in \mathcal{F} ~
  \text{for all} ~ B \in \mathcal{F}'.
\end{equation}
For a random variable $X$ from a probability space $(\Omega,
\mathcal{F}, P)$ to a measure space $(\mathcal{X}, \mathcal{F}')$ we
define the expected value as
\begin{equation}
  E[X] = \int_\Omega X(\omega) \mathrm{d}P(\omega),
\end{equation}
but to compute the integral we need to move to a different space. For
a measurable function $f$ from $(\mathcal{X}, \mathcal{F}')$ to
$(\mathbb{R}, \mathcal{R})$ and a distribution $\mu(A) = P(\{\omega :
X(\omega) \in A\}) = P(X \in A)$, i.e. $X \sim \mu$, with $A
\subseteq \mathcal{F}'$ we define
\begin{equation}
  \begin{split}
  E_f[X] &= \int_\Omega f(X(\omega)) \mathrm{d}P(\omega)\\
         &= \int_{\mathcal{F}'} f(y) \mathrm{d}(P \circ X^{-1})\\
         &= \int_{\mathcal{F}'} f(y) \mu(\mathrm{d}y).
  \end{split}
\end{equation}

\section*{Inference}
Given a sequence of $m_T$ observations $E = \{X_1, X_2, \dots,
X_{m_T}\}$ how can we make predictions about future
observations?
%\footnote{This is known as \textit{Hume's problem of
%induction} and was first answered by de Finetti's representation
%theorem for problems where sequences of events are exchangeable.}
That is, how can we formulate a hypothesis $\mathcal{H}$ about the
laws behind the sequence of observations and compute the probability
of our hypothesis conditional on all the evidence $E$ that we
gathered? From \textit{Bayes's rule of conditioning}
\begin{equation}
  P(\mathcal{H}|E) = \frac{P(E,\mathcal{H})}{P(E)},
\end{equation}
we know how to incorporate our observations into our
probability function. But to fully answer the question of induction we
have to clearly define the patterns or symmetries that we expect
behing the sequence of observations. This will lead us to a theorem by
de Finetti that tells us how the joint probability mass function for
$P(E)$ can be represented. A corresponding mode of inductive inference
is then given by the \textit{Johnson-Carnap continuum}. Several
drawbacks of the Johnson-Carnap continuum have been extensively
discussed in literature, one of which is the assumption that the
number of possible outcomes $K$ is known in advance. Several
generalizations were developed, for instance the one by Pitman which
overcomes this limitation. However, we will first focus on the
Johnson-Carnap continuum. More general forms will become useful later
when we want to estimate entropies from severely undersampled
probability spaces.
\begin{definition}[Finite and infinite exchangeability]
  Let $X_1, X_2, \dots$ denote sequence of random events,
  where each $X_i$ is a random variable from a probability space
  $(\Omega, \mathcal{F}, P)$ to a measure space $(\mathcal{X},
  \mathcal{F}')$. A finite sequence of $m_T$ events is said to be
  $m_T$-exchangeable with respect to the probability measure $P$ if
  \begin{equation}
    P(X_1 = x_1, \dots, X_{m_T} = x_{m_T}) = P(X_{1} = x_{\pi(1)},
    \dots, X_{m_T} = x_{\pi(m_T)})
  \end{equation}
  with $x_i \in \mathcal{X}$ and for all permutations $\pi$ defined on
  the set $\{1, \dots, m_T\}$. An infinite sequence is exchangeable if
  it is $m_T$-exchangeable for all $m_T \in \mathbb{N}$.
  (cf. \cite{Finetti1974})
\end{definition}

\begin{lemma}
  Exchangeable sequences are strictly stationary, i.e.
  \begin{equation}
    P(X_1 = x_1, \dots, X_{m_T} = x_{m_T}) = P(X_{1} = x_{1+\tau},
    \dots, X_{m_T} = x_{m_T+\tau})
  \end{equation}
  for all $\tau \in \mathbb{N}$.
\end{lemma}

Note that de Finetti's representation theorem is a form of ergodic
decomposition for stationary (non-ergodic) random processes.

\begin{example}[Exchangeability]
  Let $X_1, X_2, \dots$ be an infinite sequence of binary random
  events with $\mathcal{X} = \{0, 1 \}$ and $X_i \sim p$, where we
  define $p(0) = 1-p^*$ and $p(1) = p^*$. We will draw $p^*$ from a
  discrete uniform distribution on $\{p_1, p_2\}$. Hence we have
  \begin{equation}
    \lim\limits_{m_T \rightarrow \infty}
    \frac{1}{m_T}\sum_i^{m_T} X_i =
    \begin{cases}
      p_1 & \text{with probability} ~ 1/2\\
      p_2 & \text{with probability} ~ 1/2,
    \end{cases}
  \end{equation}
  from the law of large numbers. The sequence is exchangeable and
  therefore independent conditional on $p$, i.e.
  \begin{equation}
    P(X_1 = x_1, X_2 = x_2, \dots, X_{m_T} = x_{m_T}) =
    \prod_{i=1}^{m_T}P(X_i = x_i | p).
  \end{equation}
\end{example}
Assuming exchangeability for a sequence of observations makes the
assignment of probabilites a tractable problem and still allows
inductive reasoning. Note that this is not the case when we assume
that observations are independently generated, i.e.
\begin{equation}
    P(X_1 = x_1, \dots, X_{m_T} = x_{m_T}) = P(X_{1} = x_{\pi(1)})
    \dots P(X_{m_T} = x_{\pi(m_T)}),
\end{equation}
as it is often assumed in orthodox statistics. If independence were
truly the case we would not be able to draw any conclusions about
future events given our observations.

\begin{theorem}[Generalized de Finetti's representation theorem]
  \label{thr:frm}
  Let $X_1, X_2, \dots$ denote an infinitely exchangeable sequence of
  random variables from a probability space $(\Omega, \mathcal{F}, P)$
  to a measure space $(\mathcal{X}, \mathcal{F}')$. There exists a
  cumulative distribution $\Pi_{\vec{\alpha}}$ such that the joint mass
  function $p(x_1, \dots, x_{m_T}) = P(X_1 = x_1, \dots, X_{m_T} =
  x_{m_T})$ has the form
  \begin{equation}
    \begin{split}
      \label{eq:frt}
      p(x_1, \dots, x_{m_T}) &=
      \int_{\Delta^K} p(x_1, \dots, x_{m_T}|\vec{\theta})
      \mathrm{d} \Pi_{\vec{\alpha}}(\vec{\theta})\\
      &=
      \int_{\Delta^K}
      \prod_{x \in \mathcal{X}}
      \theta_x^{n_x}
      \mathrm{d} \Pi_{\vec{\alpha}}(\vec{\theta}), ~ \text{with}\\
      \Pi_{\vec{\alpha}}(\vec{\theta}) &= \lim\limits_{m_T \rightarrow \infty}
      \Pr \left\{\frac{n_x}{m_T} \le \theta_x : x \in \mathcal{X} \right\},
    \end{split}
  \end{equation}
  where $\vec{n}(E) = (n_x)_{x \in \mathcal{X}}$ denotes the count
  statistic for every occurence of $x \in \mathcal{X}$ in $E$, $K =
  |\mathcal{X}|$, and $\Delta^K := \{\vec{\theta} = (\theta_x)_{x \in
    \mathcal{X}} \in \mathbb{R}^K : \sum_{x \in \mathcal{X}} \theta_x
  = 1 ~ \text{and} ~ \theta_x \ge 0~\forall x \in \mathcal{X} \}$
  denotes the $K-1$-dimensional probability simplex. If
  $\Pi_{\vec{\alpha}}$ is a non-decreasing real function we get the
  Riemann-Stieltjes integral.
  (cf. \cite{Finetti1974, Hewitt1955})
\end{theorem}
\begin{proof}
  Todo
\end{proof}
In other words, conditional on $\vec{\theta}$ the joint probability
mass function $p(x_1, \dots, x_{m_T})$ can be written as an integral
mixture of multinomial probabilities. Note that for $K=1$ equation
$\ref{eq:frt}$ computes the $n_x$th moment of
$\mathrm{d}\Pi_{\vec{\alpha}}$. In Bayesian inference, the limiting
distribution $\pi_{\vec{\alpha}} = \mathrm{d}\Pi_{\vec{\alpha}} /
\mathrm{d}\vec{\theta}$ is interpreted as a prior distribution. A
common choice for $\mathrm{d}\Pi_{\vec{\alpha}}$ is a conjugate prior
which substantially simplifies the computation of the posterior
distribution. In the following we show how such a prior can be
justified from inductive principles such that theorem \ref{thr:frm}
holds for a proper choice of $\vec{\alpha}$.

\begin{theorem}[Johnson-Carnap continuum]
  \label{thr:jcc}
  Let $X_1, X_2, \dots$ denote an infinite sequence of random
  variables from a probability space $(\Omega, \mathcal{F}, P)$ to a
  measure space $(\mathcal{X}, \mathcal{F}')$. Assume that
  \begin{itemize}
  \item[(i)  ] $K = |\mathcal{X}| \ge 3$,
  \item[(ii) ] $P(X_1 = x_1, \dots, X_{m_T} = x_{m_T}) > 0$ for all $x_i \in
    \mathcal{X}$, and
  \item[(iii)] there exists a sufficient statistic $f_x(n_x,n)$ such
    that $P(X_{m_T+1} = x_{m_T+1} | \vec{n}(E)) = f_x(n_x,m_T)$, where $\vec{n}(E) =
    (n_x)_{x \in \mathcal{X}}$.
  \end{itemize}
  Then either the outcomes are independent or there exist positive
  constants $\{\alpha_x : x \in \mathcal{X}\}$ such that for all $n \ge 1$
  \begin{equation}
    f_x(n_x, m_T) = \frac{n_x + \alpha_x}{m_T + \sum_{x^* \in \mathcal{X}}
      \alpha_{x^*}}.
  \end{equation}
  (cf. \cite{Johnson1932, Zabell1982})
\end{theorem}
\begin{proof}
  Todo
\end{proof}
Note that if $X_1, X_2, \dots$ is exchangeable then $P(X_{m_T+1} =
x_{m_T+1} | \vec{n}(E)) = P(X_{m_T+1} = x_{m_T+1} | n_x) = f_x(n_x,m_T)$. The
Johnson-Carnap continuum of inductive methods not only provides us
with a sufficient statistic for inductive reasoning, but also
identifies a unique prior distribution for de Finetti's representation
theorem.
\begin{corollary}
  Under the conditions (i-iii) of theorem \ref{thr:jcc} the prior
  distribution $\mathrm{d}\Pi_{\vec{\alpha}}$ belongs to the \textit{Dirichlet
    familiy}, i.e.
  \begin{equation}
    \begin{split}
      \mathrm{d}\Pi_{\vec{\alpha}}(\vec{\theta}) =
      \Dir(\vec{\theta};\vec{\alpha})\mathrm{d}\vec{\theta} &=
      \frac{1}{\Beta(\vec{\alpha})}
      \prod_{x \in \mathcal{X}} \theta_x^{\alpha_x -1}
      \mathrm{d}\vec{\theta}, ~ \text{where}\\
      \Beta(\vec{\alpha}) &= \frac{\prod_{x \in \mathcal{X}}
        \Gamma(\alpha_x)}{\Gamma(\sum_{x
          \in \mathcal{X}}\alpha_x)} =
      \underbrace{\int_{\Delta^K}\prod_{x \in
          \mathcal{X}} \theta^{\alpha_x-1}
        \mathrm{d}\vec{\theta},}_{\text{multinomial Euler
          integral}}
    \end{split}
  \end{equation}
  and $\Gamma(z) = \int_0^\infty t^{z-1}e^{-t}\mathrm{d}t$ denotes
  the Gamma function.
\end{corollary}
\begin{proof}
  Todo
\end{proof}
To summarize our results, under the symmetry assumptions that we
introduced about the sequence of observations $E = (X_1, X_2, \dots,
X_{m_T})$ we are able to state a hypothesis $\mathcal{H}$ in form of the
parameters $\vec{\theta}$ that represent our beliefs about the
underlying process. Any knowledge that we have before we look at the
data is represented in the parameters $\vec{\alpha}$. The probability
of a hypothesis is then given by
\begin{equation}
  \begin{split}
    P(\vec{\theta}|E)
    &= \frac{P(E,\vec{\theta})}{P(E)}
     =
    \frac{P(E|\vec{\theta})
      \mathrm{d}\Pi_{\vec{\alpha}}(\vec{\theta}) /
      \mathrm{d}\vec{\theta}}{
      \int_{\Delta^K}
      P(E|\vec{\theta}')
      \mathrm{d}\Pi_{\vec{\alpha}}(\vec{\theta}')}\\
    &=
    \Dir(\vec{\theta}; \vec{n}(E)+\vec{\alpha}).
  \end{split}
\end{equation}
The update of our beliefs after seeing the data is therefore a mere
shift in the parameters of the Dirichlet distribution
(i.e. $\Dir(\vec{\theta};\vec{\alpha}) \mapsto \Dir(\vec{\theta};
\vec{n}(E)+\vec{\alpha})$) as it is a conjugate prior to the multinomial
distribution. Intuitively, the Dirichlet distribution states our
belief that the probabilities of $K$ rival events $x \in \mathcal{X}$
are $(\theta_x)_{x \in \mathcal{X}}$ given that each event has been
observed $\alpha_x - 1$ times. Since $\vec{\theta} \sim
\Dir(\vec{\theta}; \vec{n}(E)+\vec{\alpha})$ we can easily compute the
expected value of $\vec{\theta}$ given our observations, i.e.
\begin{equation}
  \begin{split}
    \hat{\theta}_x = \mathrm{E}[\theta_x|E] &=
    \int_{\Delta^{K-1}}\theta_x P(E|\vec{\theta})
    \mathrm{d}\Pi_{\vec{\alpha}}(\vec{\theta})\\
    &=
    \frac{
      \int_{\Delta^K} P(X_{m_T+1} = x,E|\vec{\theta})
      \mathrm{d}\Pi_{\vec{\alpha}}(\vec{\theta})
    }{
      \int_{\Delta^K} P(E|\vec{\theta})
      \mathrm{d}\Pi_{\vec{\alpha}}(\vec{\theta})
    }\\
    &= f_x(n_x,m_T)
    = \frac{n_x + \alpha_x}{m_T + \sum_{x' \in \mathcal{X}} \alpha_{x'}},
  \end{split}
\end{equation}
as it was shown in theorem \ref{thr:jcc}. The estimate is known as the
\textit{extended Bayes-Laplace rule of succession}. It is equivalent
to the frequentist's estimate ($\hat{\theta}_x = \frac{n_x}{m_T}$) if
$\alpha_x = 0 ~ \forall x \in \mathcal{X}$. If $\alpha_x = 1 ~ \forall
x \in \mathcal{X}$ we get \textit{Laplace's rule of succession}.

\begin{example}[Polya's Urn]
  Suppose we draw white and black balls at random from an urn
  (i.e. $\Omega = \{ \text{white}, \text{black} \}$). Each time we
  draw a ball we put $k$ balls of the same color back into the
  urn. Thus we have a sequence of exchangeable events $X_1, X_2,
  \dots$ with $X_i( \text{white} ) = 0$ and $X_i( \text{black} ) =
  1$. Hence
  \begin{equation}
    \lim\limits_{m_T \rightarrow \infty} \frac{1}{m_T}
    \sum_{i=1}^{m_T} X_i = \theta \sim \Beta \left(\frac{b}{k},
      \frac{w}{k} \right),
  \end{equation}
  where $b$ is the initial number of black balls and $w$ the number of
  white balls. Conditional on $\theta$ the sequence $X_1, X_2, \dots$ is
  independent.
\end{example}

The concept of exchangeability was generalized by de Finetti to the
notion of $L$-fold partial exchangeablility, where we allow events of
a sequence $X_1, X_2, \dots$ to belong to $L$ different classes.
We can therefore split the sequence into $L$ sequences $(\vec{X^1},
\vec{X^2}, \dots,  \vec{X^L})$ where each $\vec{X^i}$ represents the
events that we assign to the $i$th class. Within the classes the
ordering provides no relevant information. This leads us to the
following definition:

\begin{definition}[Partial exchangeability]
  We call a tuple $(\vec{X^l})_{l=1}^L$ of infinitely exchangeable
  sequences $\vec{X^l} = X^l_1, X^l_2, \dots$ $L$-fold partially
  exchangeable.
\end{definition}

\begin{theorem}[Representation theorem for partially exchangeable
  sequences]
  Let $(\vec{X}^l)_{l=1}^L$ denote an $L$-fold partially exchangeable
  sequence of random variables from a probability space $(\Omega,
  \mathcal{F}, P)$ to a measure space $(\mathcal{X},
  \mathcal{F}')$. There exists a cumulative distribution
  $\Pi_{\vec{\alpha}}^L$ such that the joint mass function $p(\vec{x}^1,
  \dots, \vec{x}^{L}) = P(\vec{X}^1 = \vec{x}^1, \dots, \vec{X}^L =
  \vec{x}^L)$ has the form
  \begin{equation}
    \begin{split}
      p(\vec{x}^1, \dots, \vec{x}^{L}) &=
      \int_{\Delta_L^K}
      \left\{
      \prod_{l=1}^L
      p(x^l_1, \dots, x^l_{m_T}|\vec{\theta}^l)
      \right\}
      \mathrm{d} \Pi_{\vec{\alpha}}^L(\vec{\Theta})\\
      &=
      \int_{\Delta_L^K}
      \left\{
      \prod_{l=1}^L
      \prod_{x \in \mathcal{X}}
      (\theta_x^l)^{n_x^l}
      \right\}
      \mathrm{d} \Pi_{\vec{\alpha}}^L(\vec{\Theta}), ~ \text{with}\\
      \Pi_{\vec{\alpha}}^L(\vec{\Theta}) &= \lim\limits_{m_T \rightarrow \infty}
      \Pr \left\{\frac{n_x^l}{m_T} \le \theta^l_x : x \in \mathcal{X}
        ~ \text{and} ~ l=1,2, \dots, L \right\},
    \end{split}
  \end{equation}
  where $\Delta^K_L = \varprod_{i=1}^L \Delta^K$. We have strong
  partial exchangeability if we can write the joint probability mass
  function as
  \begin{equation}
      p(\vec{x}_1, \dots, \vec{x}_{L}) =
      \prod_{l=1}^L
      \int_{\Delta^K}
      \prod_{x \in \mathcal{X}}
      \theta_x^{n_x^l}
      \mathrm{d} \Pi_{\vec{\alpha}}(\vec{\theta}),
  \end{equation}
  i.e. the classes are independent.
\end{theorem}
