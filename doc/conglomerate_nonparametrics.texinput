% -*- mode: latex -*-
%
\section*{Nonparametric Bayesian Inference}
\cite{Antoniak1974}
\cquote{A strictly Bayesian approach might be to start out with an
  \textit{a priori} distribution of probability over the class
  $\mathcal{F}$ of all possible distributions $F$ $[\dots]$ this
  could possibly be converted into an \textit{a posteriori}
  distribution after $X_1, \dots, X_{m_T}$ have been observed. Whether
  anyone will espouse this view and recommend a procedure for carrying
  it out remains to be seen.}{\cite{Robbins1963}}

\section*{Dirichlet Process}
The Dirichlet process was first described with the notion of tail-free
measures (cf. \cite{Fabius1964}) by \cite{Freedman1963} and it was
later analyzed in detail by \cite{Ferguson1973, Blackwell1973}.
\cite{Antoniak1974} introduced Dirichlet process mixtures that
convolve the random measure with a continuous distribution since
random measures drawn from a Dirichlet process are discrete. A good
summary on the topic is provided by \cite{Ferguson1992}.
\begin{definition}[Tail-free measure]
  Todo (cf. \cite{Fabius1964, Freedman1963, Doksum1974})
\end{definition}

\begin{definition}[Dirichlet Process]
  Let $\Theta$ denote a measurable set, $\mathcal{F}$ a
  $\sigma$-algebra of subsets of $\Theta$, and let $G_0$ be a
  probability measure on $(\Theta, \mathcal{F})$. A random probability
  measure $G$ is drawn from a Dirichlet process $\DP(G_0, \alpha)$
  with parameter $\alpha \in \mathbb{R}$ if for any finite partition
  $A_1, \dots A_K$ of $\Theta$
  \begin{equation}
    (G(A_1), \dots, G(A_K)) \sim \Dir(\alpha G_0(A_1), \dots, \alpha G_0(A_K)),
  \end{equation}
  and $P(\theta \in A_i | G) = G(A_i)$. (cf. \cite{Ferguson1973})
\end{definition}

We can extend Theorem~\ref{inf:jcc} to continuous spaces to get a
sufficient characterization of the Dirichlet process:
\begin{theorem} Let $\Theta$ denote a measurable set, $A \subset
  \Theta$ and $G$ any finite measure on $\Theta$ then from
  \begin{equation}
    P(X_{m_T+1} \in A | X_1, X_2, \dots, X_{m_T}) = \frac{G(A) +
      \sum_{i=1}^{m_T} \delta_{X_i}(A)}{G(\Theta) + m_T}
  \end{equation}
  it follows that $G$ is a Dirichlet process
  prior. (cf. \cite{Regazzini1978, Lo1991})
\end{theorem}

\begin{lemma}[Properties of the Dirichlet process]\hfill
  \begin{enumerate}
  \item
    Given a set of observations $\theta_1, \dots, \theta_{m_T}$ then the
    posterior $G|\theta_1, \dots, \theta_{m_T}$ is as well a Dirichlet
    process, i.e.
    \begin{equation}
      \begin{split}
        G~ | ~\theta_1, \dots, \theta_{m_T} &\sim
        \DP\left(
        \frac{1}{\alpha + m_T} \sum_{i=1}^{m_T}\delta_{\theta_i} +
        \frac{\alpha}{\alpha + m_T} G_0,
        \alpha + m_T
        \right).
      \end{split}
    \end{equation}
  \item For any $A_k \subset \Theta$ we have
    \begin{equation}
      \begin{split}
        \E[G(A_k)]   &= G_0(A_k),\\
        \Var[G(A_k)] &= \frac{G_0(A_k)(1-G_0(A_k))}{\alpha + 1}.
      \end{split}
    \end{equation}
  \item If $G \sim DP(G_0, \alpha)$ then $G$ is almost surely discrete.
  \end{enumerate}
\end{lemma}

De Finetti representation
\begin{equation}
  P(\theta_{m_T} | \theta_1, \dots, \theta_{m_T-1}, G_o, \alpha) \propto
  \int \prod_{i=1}^{m_T-1} P(\theta_i|G)P(G|G_0,\alpha)\dd G
\end{equation}

\begin{align*}
  1
  &\sim \Dir(\alpha)\\
%
  (\pi_1, \pi_2)
  &\sim \Dir(\alpha/2, \alpha/2)
  & \pi_1 + \pi_2 &= 1\\
%
  (\pi_{11}, \pi_{12}, \pi_{21}, \pi_{22})
  &\sim \Dir(\alpha/4, \alpha/4, \alpha/4, \alpha/4)
  &\pi_{i1} + \pi_{i2} &= \pi_i, i = 1,2\\
%
  &\,~\vdots\\
%
  (\pi_k)_{k=1}^K
  &\sim \Dir(\alpha/K, \dots, \alpha/K)
  &\sum_{k=1}^K \pi_k &= 1\\
%
  G(\theta) &= \sum_{k=1}^{\infty} \pi_k \delta_{\theta^*_k}(\theta)
\end{align*}
\begin{theorem}
  Let
  \begin{equation}
    \begin{aligned}
      (\pi_k)_{k=1}^K &\sim \Dir(\alpha/K, \dots, \alpha/K)\\
      \theta^*_k ~ | ~ G_0 &\sim G_0
    \end{aligned}
    \quad\quad G^K = \sum_{k=1}^{K} \pi_k \delta_{\theta^*_k}
  \end{equation}
  then $G^K \codist G$ as $K \rightarrow \infty$, where $G \sim
  DP(G_0, \alpha)$.
  (cf. \cite{Ishwaran2002})
\end{theorem}

\section*{Dirichlet Process Mixtures}
\cite{Antoniak1974}
\begin{equation}
  \begin{split}
    x_k ~ | ~ \theta_k  &\sim F(\theta_k)\\
    \theta_k ~ | ~ G    &\sim G\\
    G ~ | ~ G_0, \alpha &\sim \DP(G_0, \alpha)
  \end{split}
\end{equation}
which can also be expressed as the limit of a finite mixture model
\begin{equation}
  \begin{split}
    x_k ~ | ~ c_k, \vec{\theta}^*   &\sim F(\theta^*_{c_k})\\
    c_k ~ | ~ \vec{p}               &\sim \Dis(p_1, \dots, p_K)\\
    \theta^*_c                  &\sim G_0\\
    \vec{p}                     &\sim \Dir(\alpha/K, \dots, \alpha/K)
  \end{split}
\end{equation}
as $K \rightarrow \infty$.

\begin{equation}
  \begin{split}
    P(c_k | c_1, \dots c_{k-1})
    &= \frac{P(c_1, \dots, c_k)}{P(c_1, \dots, c_{k-1})}\\
    &= \frac{\int_{\Delta^K}P(c_1, \dots, c_k|\vec{p})\dd \Pi_{\valpha}(\vec{p})}{\int_{\Delta^K}P(c_1, \dots, c_{k-1}|\vec{p})\dd \Pi_{\valpha}(\vec{p})}\\
    &= \frac{\int_{\Delta^K} \frac{\Gamma(\alpha)}{\Gamma(\alpha)^K} \prod_{k=1}^{K}p_k^{n_k(c_1, \dots, c_k)+\alpha/K-1} \dd \vec{p}}{\int_{\Delta^K} \frac{\Gamma(\alpha)}{\Gamma(\alpha)^K} \prod_{k=1}^{K}p_k^{n_k(c_1, \dots, c_{k-1})+\alpha/K-1} \dd \vec{p}}\\
    &= \frac{\Beta(\vec{n}(c_1, \dots, c_k)+\alpha/K)}{\Beta(\vec{n}(c_1, \dots, c_{k-1})+\alpha/K)}\\
    &= \frac{n_{c_k}(c_1, \dots, c_{k-1}) + \alpha/K}{k - 1 + \alpha}
  \end{split}
\end{equation}
$\vec{n} = (n_k)_{k=i}^K$ is the count statistic for the $K$ distinct classes.

\subsection*{Stick-breaking construction to generate G}
\cite{Sethuraman1994}
\begin{equation}
  \begin{split}
    \nu_k      &\sim \Beta(1, \alpha)\\
    \theta^*_k &\sim G_0\\
    \pi_k(\vec{\nu}) &= \nu_k \prod_{i=1}^{k-1}(1-\nu_i)\\
    G(\theta) &= \sum_{k=1}^{\infty} \pi_k(\vec{\nu})\delta_{\theta_k^*}(\theta)
  \end{split}
\end{equation}

\subsection*{P\'olya urn to sample from G}
\cite{Blackwell1973}
\begin{equation}
  \begin{split}
    \theta_{m_T} ~ | ~ \theta_1, \dots, \theta_{m_T-1}, \exists i < m_T ~ \theta_{m_T} = \theta_i &\sim
    \underbrace{\frac{1}{m_T - 1} \sum_{j=1}^{m_T-1}
      \delta_{\theta_j}}_{\text{empirical measure}}\\
    %
    \theta_{m_T} ~ | ~ \theta_1, \dots, \theta_{m_T-1}, \forall i < m_T ~ \theta_{m_T} \neq \theta_i &\sim
    G_0\\
    %
    \Rightarrow ~
    \theta_{m_T} ~ | ~ \theta_1, \dots, \theta_{m_T-1} &\sim
    \frac{1}{m_T-1+\alpha} \sum_{j=1}^{m_T-1} \delta_{\theta_j} +
    \frac{\alpha}{m_T-1+\alpha} G_0\\
    %
    \text{with} ~
    P(\exists i ~ \theta_{m_T} = \theta_i    ~ | ~ \theta_1, \dots, \theta_{m_T-1}) &= \frac{m_T - 1}{m_T - 1 + \alpha}\\
    P(\forall i ~ \theta_{m_T} \neq \theta_i ~ | ~ \theta_1, \dots, \theta_{m_T-1}) &= \frac{\alpha }{m_T - 1 + \alpha},\\
  \end{split}
\end{equation}
$\delta_x$ denotes the Dirac measure defined by
\begin{equation}
  \delta_x(A) =
  \begin{cases}
    1 & \text{if} ~ x \in A\\
    0 & \text{otherwise}.
  \end{cases}
\end{equation}
It is known as the Blackwell and MacQueen P\'olya urn scheme
(often described as the Chinese restaurant process)

\section*{Sampling from Dirichlet Process Mixtures}
Suppose we have an exchangeable sequence of observations $x_1, \dots,
x_{m_T}$. From exchangeability it follows that
\begin{equation}
  \begin{split}
    \theta_{i} ~ | ~ \vtheta_{-i} &\sim
    \frac{1}{m_T-1+\alpha} \sum_{j=1}^{m_T-1}
      \delta_{\theta_j} +
    \frac{\alpha}{m_T-1+\alpha} G_0\\
  \end{split}
\end{equation}

\begin{align*}
  \theta_i ~ | ~ x_i,\vtheta_{-i}
  &= \frac{(x_i ~ | ~ \theta_i,\vtheta_{-i})(\theta_i ~ | ~ \vtheta_{-i})}{(x_i ~ | ~ \vtheta_{-i})}\\
  % = \frac{(x_i ~ | ~ \theta_i,\vtheta_{-i})(\theta_i ~ | ~ \vtheta_{-i})}{(x_i ~ | ~ \vtheta_{-i})(\vtheta_{-i})}
  % \\
  % &= \frac{(x_i ~ | ~ \vtheta)}{(x_i ~ | ~ \vtheta_{-i})}(\theta_i ~ | ~ \vtheta_{-i})\\
  % \Rightarrow
  % (\theta_i  ~ | ~  \vtheta_{-i}, x_i)
  &= \frac{1}{(x_i ~ | ~ \vtheta_{-i})}
  [\\
    &\quad\frac{i-1}{i-1+\alpha}
    (x_i  ~ | ~  \vtheta, \exists j \neq i ~ \theta_j = \theta_i)
    (\theta_i  ~ | ~  \vtheta_{-i}, \exists j \neq i ~ \theta_j = \theta_i)+\\
    &\quad \frac{\alpha}{i-1+\alpha}
    (x_i  ~ | ~  \vtheta, \forall j \neq i ~ \theta_j \neq \theta_i)
    (\theta_i  ~ | ~  \vtheta_{-i}, \forall j \neq i ~ \theta_j \neq \theta_i)
  ]\\
  % &= \frac{(x_i ~ | ~ \vtheta)}{(x_i ~ | ~ \vtheta_{-i})}
  % \left[ \frac{1}{m_T - 1 + \alpha} \sum_{j \neq i}
  %   \delta_{\theta_j}(\theta_i) + \frac{\alpha}{m_T - 1 +
  %     \alpha} G_0(\theta_i)
  % \right]\\
  &= \sum_{j \neq i} b (x_i ~ | ~  \theta_j) \delta_{\theta_j}(\theta_i) +
  b \alpha (x_i ~ | ~ \theta_i) (\theta_i  ~ | ~  \vtheta_{-i}, \forall j \neq i ~ \theta_j \neq \theta_i),
  ~ \text{with}\\
  b &= \frac{1}{(x_i ~ | ~ \vtheta_{-i})(m_T - 1 + \alpha)}
\end{align*}
With
\begin{equation}
  \begin{split}
    P(\theta_i | x_i) = \frac{P(x_i,\theta_i)}{P(x_i)} =
    \frac{P(x_i|\theta_i)P(\theta_i)}{\int P(x_i|\theta)P(\theta) \dd\theta}
  \end{split}
\end{equation}
we can sample from
\begin{equation}
  \begin{split}
    P(\theta_i | \vtheta_{-i}, x_i)
    &= \sum_{j \neq i} b q_{i,j} \delta_{\theta_j}(\theta_i)
                   + b r_i P(\theta_i|x_i), ~ \text{with}\\
    q_{i,j} &= b P(x_i|\theta_j), ~ \text{and}\\
    r_i    &= b \alpha \int P(x_i|\theta)P(\theta) \dd\theta
  \end{split}
\end{equation}
(cf. \cite{Escobar1994, Escobar1995})

\begin{equation}
  \begin{split}
    P(c_i | y_i, \vec{c}_{-i}, \vec{\theta}^*)
    &= \frac{P(y_i | c_i, \vec{c}_{-i}, \vec{\theta}^*)P(c_i | \vec{c}_{-i}, \vec{\theta}^*)}{P(y_i | \vec{c}_{-i}, \vec{\theta}^*)}\\
    &= \frac{P(y_i | c_i, \vec{\theta}^*)P(c_i | \vec{c}_{-i})}{P(y_i | \vec{c}_{-i}, \vec{\theta}^*)}\\
    &= b P(y_i | c_i, \vec{\theta}^*) \frac{n_{c_i}(\vec{c}_{-i}) + \alpha/K}{i - 1 + \alpha}
  \end{split}
\end{equation}
