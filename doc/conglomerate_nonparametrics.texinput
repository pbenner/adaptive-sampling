% -*- mode: latex -*-
%
\section*{Nonparametric Bayesian Inference}
\cite{Antoniak1974}
\cquote{A strictly Bayesian approach might be to start out with an
  \textit{a priori} distribution of probability over the class
  $\mathcal{F}$ of all possible distributions $F$ $[\dots]$ this
  could possibly be converted into an \textit{a posteriori}
  distribution after $X_1, \dots, X_{m_T}$ have been observed. Whether
  anyone will espouse this view and recommend a procedure for carrying
  it out remains to be seen.}{\cite{Robbins1963}}

\section*{Dirichlet Process}
The Dirichlet process was first described with the notion of tail-free
measures (cf. \cite{Fabius1964}) by \cite{Freedman1963} and it was
later analyzed in detail by \cite{Ferguson1973, Blackwell1973}.
\cite{Antoniak1974} introduced Dirichlet process mixtures that
convolve the random measure with a continuous distribution since
random measures drawn from a Dirichlet process are discrete. A good
summary on the topic is provided by \cite{Ferguson1992}.
\begin{definition}[Tail-free measure]
  Todo (cf. \cite{Fabius1964, Freedman1963, Doksum1974})
\end{definition}

\begin{definition}[Dirichlet Process]
  Let $\Theta$ denote a measurable set, $\mathcal{F}$ a
  $\sigma$-algebra of subsets of $\Theta$, and let $G_0$ be a
  probability measure on $(\Theta, \mathcal{F})$. A random probability
  measure $G$ is drawn from a Dirichlet process $\DP(G_0, \alpha)$
  with parameter $\alpha \in \mathbb{R}$ if for any finite partition
  $A_1, \dots, A_K$ of $\Theta$
  \begin{equation}
    (G(A_1), \dots, G(A_K)) \sim \Dir(\alpha G_0(A_1), \dots, \alpha G_0(A_K)),
  \end{equation}
  and $P(\theta \in A_i | G) = G(A_i)$. (cf. \cite{Ferguson1973})
\end{definition}

We can extend Theorem~\ref{inf:jcc} to continuous spaces to get a
sufficient characterization of the Dirichlet process:
\begin{theorem} Let $\Theta$ denote a measurable set, $A \subset
  \Theta$ and $G$ any finite measure on $\Theta$ then from
  \begin{equation}
    P(X_{m_T+1} \in A | X_1, X_2, \dots, X_{m_T}) = \frac{G(A) +
      \sum_{i=1}^{m_T} \delta_{X_i}(A)}{G(\Theta) + m_T}
  \end{equation}
  it follows that $G$ is a Dirichlet process
  prior. (cf. \cite{Regazzini1978, Lo1991})
  \label{nonpara:th1}
\end{theorem}

\begin{lemma}[Properties of the Dirichlet process]\hfill
  \begin{enumerate}
  \item
    Given a set of observations $\theta_1, \dots, \theta_{m_T}$ then the
    posterior $G|\theta_1, \dots, \theta_{m_T}$ is as well a Dirichlet
    process, i.e.
    \begin{equation}
      \begin{split}
        G~ | ~\theta_1, \dots, \theta_{m_T} &\sim
        \DP\left(
        \frac{1}{\alpha + m_T} \sum_{i=1}^{m_T}\delta_{\theta_i} +
        \frac{\alpha}{\alpha + m_T} G_0,
        \alpha + m_T
        \right).
      \end{split}
    \end{equation}
  \item For any $A_k \subset \Theta$ we have
    \begin{equation}
      \begin{split}
        \E[G(A_k)]   &= G_0(A_k),\\
        \Var[G(A_k)] &= \frac{G_0(A_k)(1-G_0(A_k))}{\alpha + 1}.
      \end{split}
    \end{equation}
  \item If $G \sim DP(G_0, \alpha)$ then $G$ is almost surely discrete.
  \end{enumerate}
\end{lemma}

De Finetti representation
\begin{equation}
  P(\theta_{m_T} | \theta_1, \dots, \theta_{m_T-1}, G_o, \alpha) \propto
  \int \prod_{i=1}^{m_T-1} P(\theta_i|G)P(G|G_0,\alpha)\dd G
\end{equation}

\begin{theorem}
  Let
  \begin{equation}
    \begin{aligned}
      (\pi_k)_{k=1}^K &\sim \Dir(\alpha/K, \dots, \alpha/K)\\
      \theta^*_k ~ | ~ G_0 &\sim G_0
    \end{aligned}
    \quad\quad G^K = \sum_{k=1}^{K} \pi_k \delta_{\theta^*_k}
  \end{equation}
  then $G^K \codist G$ as $K \rightarrow \infty$, where $G \sim
  DP(G_0, \alpha)$.
  (cf. \cite{Ishwaran2002})
  \label{nonpara:th2}
\end{theorem}

\section*{Representations of the Dirichlet Process}
There are three main representations of the Dirichlet process:
\begin{enumerate}
\item The strick-breaking construction generates the random measure
  $G$,
\item the Blackwell and MacQueen P\'olya urn is used to generate
  $\theta_{m_T}$ given $\theta_1, \dots, \theta_{m_T-1}$,
\item and the Chinese restaurant process, which extends to P\'olya urn
  scheme for clustering applications.
\end{enumerate}

\subsection*{Stick-breaking construction}
\cite{Sethuraman1994}
\begin{equation}
  \begin{split}
    \nu_k      &\sim \Beta(1, \alpha)\\
    \theta^*_k &\sim G_0\\
    \pi_k(\vec{\nu}) &= \nu_k \prod_{i=1}^{k-1}(1-\nu_i)\\
    G(\theta) &= \sum_{k=1}^{\infty} \pi_k(\vec{\nu})\delta_{\theta_k^*}(\theta)
  \end{split}
\end{equation}

\subsection*{Blackwell and MacQueen P\'olya Urn}
\cite{Blackwell1973}
\begin{equation}
  \begin{split}
    \theta_{m_T} ~ | ~ \theta_1, \dots, \theta_{m_T-1}, \exists i < m_T ~ \theta_{m_T} = \theta_i &\sim
    \underbrace{\frac{1}{m_T - 1} \sum_{j=1}^{m_T-1}
      \delta_{\theta_j}}_{\text{empirical measure}}\\
    %
    \theta_{m_T} ~ | ~ \theta_1, \dots, \theta_{m_T-1}, \forall i < m_T ~ \theta_{m_T} \neq \theta_i &\sim
    G_0\\
    %
    \Rightarrow ~
    \theta_{m_T} ~ | ~ \theta_1, \dots, \theta_{m_T-1} &\sim
    \frac{1}{m_T-1+\alpha} \sum_{j=1}^{m_T-1} \delta_{\theta_j} +
    \frac{\alpha}{m_T-1+\alpha} G_0\\
    %
    \text{with} ~
    P(\exists i ~ \theta_{m_T} = \theta_i    ~ | ~ \theta_1, \dots, \theta_{m_T-1}) &= \frac{m_T - 1}{m_T - 1 + \alpha}\\
    P(\forall i ~ \theta_{m_T} \neq \theta_i ~ | ~ \theta_1, \dots, \theta_{m_T-1}) &= \frac{\alpha }{m_T - 1 + \alpha},\\
  \end{split}
  \label{nonpara:bmpu}
\end{equation}
$\delta_x$ denotes the Dirac measure defined by
\begin{equation}
  \delta_x(A) =
  \begin{cases}
    1 & \text{if} ~ x \in A\\
    0 & \text{otherwise}.
  \end{cases}
\end{equation}

\subsection*{Chinese restaurant process}
We know that $\theta_1, \dots, \theta_{m_T}$ drawn from a Blackwell
and MacQueen P\'olya urn take on $K < m_T$ values $\theta_1^*, \dots,
\theta_K^*$. We can therefore rewrite (\ref{nonpara:bmpu}) as
\begin{equation}
  \begin{aligned}
    \theta_{m_T} ~ | ~ \theta_1, \dots, \theta_{m_T-1} &\sim
    \sum_{k=1}^{K} \frac{n_{\theta^*_k}(\theta_1,\dots, \theta_{m_T-1})}{m_T-1+\alpha}\delta_{\theta^*_k} +
    \frac{\alpha}{m_T-1+\alpha} G_0,\\
  \end{aligned}
  \label{nonpara:chin}
\end{equation}
from which we conclude that
\begin{equation}
  \begin{aligned}
    P(            &\theta_{m_T} = \theta^*_k    ~ | ~ \theta_1, \dots, \theta_{m_T-1}) &=& ~\frac{n_{\theta^*_k}(\theta_1,\dots, \theta_{m_T-1})}{m_T - 1 + \alpha},\\
    P(\forall k ~ &\theta_{m_T} \neq \theta^*_k ~ | ~ \theta_1, \dots, \theta_{m_T-1}) &=& ~\frac{\alpha}{m_T - 1 + \alpha},\\
  \end{aligned}
\end{equation}
where $n_{\theta^*_k}(\theta_1,\dots, \theta_{m_T-1})$ counts the
number of occurences of the value $\theta^*_k$ in the sequence
$\theta_1,\dots, \theta_{m_T-1}$.

\section*{Dirichlet Process Mixtures}
\cite{Antoniak1974}
\begin{equation}
  \begin{aligned}
    x_i ~ &| ~ \theta_i  &\sim& ~ F(\theta_i)\\
    \theta_i ~ &| ~ G    &\sim& ~ G \\
    G ~ &| ~ G_0, \alpha &\sim& ~ \DP(G_0, \alpha)
  \end{aligned}
  \label{nonpara:dpm}
\end{equation}
From Theorem \ref{nonpara:th2} we know that we can represent
(\ref{nonpara:dpm}) as the limit of a finite mixture model
\begin{equation}
  \begin{aligned}
    x_i ~ &| ~ c_i, \vec{\theta}^* &\sim& ~ F(\theta^*_{c_i})\\
    c_i ~ &| ~ \vec{\pi} &\sim& ~ \Dis(\pi_1, \dots, \pi_K)\\
    \theta^*_k &&\sim& ~ G_0\\
    (\pi_k)_{k=1}^K &&\sim& ~ \Dir(\alpha/K, \dots, \alpha/K)
  \end{aligned}
\end{equation}
with $K \rightarrow \infty$, that is, we draw $K$ distinct values
$(\theta^*_k)_{k=1}^K$ from $G_0$ and $c_i$ indicates which of them we
use to generate $x_i$.

\begin{equation}
  \begin{split}
    P(c_{m_T} = c| c_1, \dots c_{m_T-1})
    &= \frac{P(c_1, \dots, c_{m_T})}{P(c_1, \dots, c_{m_T-1})}\\
    &= \frac{\int_{\Delta^K}P(c_1, \dots, c_{m_T}|\vec{p})\dd \Pi_{\valpha}(\vec{p})}{\int_{\Delta^K}P(c_1, \dots, c_{m_T-1}|\vec{p})\dd \Pi_{\valpha}(\vec{p})}\\
    &= \frac{\int_{\Delta^K} \frac{\Gamma(\alpha)}{\Gamma(\alpha)^K} \prod_{k=1}^{K}p_k^{n_k(c_1, \dots, c_{m_T})+\alpha/K-1} \dd \vec{p}}{\int_{\Delta^K} \frac{\Gamma(\alpha)}{\Gamma(\alpha)^K} \prod_{k=1}^{K}p_k^{n_k(c_1, \dots, c_{m_T-1})+\alpha/K-1} \dd \vec{p}}\\
    &= \frac{\Beta(\vec{n}(c_1, \dots, c_{m_T})+\alpha/K)}{\Beta(\vec{n}(c_1, \dots, c_{m_T-1})+\alpha/K)}\\
    &= \frac{n_{c}(c_1, \dots, c_{m_T-1}) + \alpha/K}{m_T - 1 + \alpha}\\
    &\rightarrow \frac{n_{c}(c_1, \dots, c_{m_T-1})}{m_T - 1 + \alpha}
  \end{split}
\end{equation}
as $K \rightarrow \infty$, where $\vec{n} = (n_k)_{k=i}^K$ is the
count statistic for the $K$ distinct classes.

\section*{Sampling from Dirichlet Process Mixtures}
Suppose we have an exchangeable sequence of observations $x_1, \dots,
x_{m_T}$. When we want draw samples of the $\theta_i$ we can use
either the P\'olya urn scheme (\ref{nonpara:bmpu}) or the
representation as a Chinese restaurant process
(\ref{nonpara:chin}). We will first derive the Gibbs sampler for the
P\'olya urn scheme. From exchangeability it follows that
\begin{equation}
  \begin{split}
    \theta_{i} ~ | ~ \vtheta_{-i} &\sim
    \frac{1}{m_T-1+\alpha} \sum_{j=1}^{m_T-1}
      \delta_{\theta_j} +
    \frac{\alpha}{m_T-1+\alpha} G_0\\
  \end{split}
\end{equation}

\begin{align*}
  P(\theta_i | x_i,\vtheta_{-i})
  &= \frac{P(x_i | \theta_i,\vtheta_{-i})P(\theta_i | \vtheta_{-i})}{P(x_i | \vtheta_{-i})}\\
  &= \frac{1}{P(x_i | \vtheta_{-i})}
  [\\
    &\quad\frac{i-1}{i-1+\alpha}
    P(x_i  |  \vtheta, \exists j \neq i ~ \theta_j = \theta_i)
    P(\theta_i  |  \vtheta_{-i}, \exists j \neq i ~ \theta_j = \theta_i)+\\
    &\quad \frac{\alpha}{i-1+\alpha}
    P(x_i  |  \vtheta, \forall j \neq i ~ \theta_j \neq \theta_i)
    P(\theta_i  |  \vtheta_{-i}, \forall j \neq i ~ \theta_j \neq \theta_i)
  ]\\
  &= \sum_{j \neq i} b P(x_i |  \theta_j) \delta_{\theta_j}(\theta_i) +
  b \alpha P(x_i | \theta_i) f_{G_0}(\theta_i),
  ~ \text{with}\\
  b &= \frac{1}{P(x_i | \vtheta_{-i})(m_T - 1 + \alpha)}
\end{align*}
With
\begin{equation}
  \begin{split}
    P(\theta_i | x_i) = \frac{P(x_i,\theta_i)}{P(x_i)} =
    \frac{P(x_i|\theta_i)P(\theta_i)}{\int P(x_i|\theta)P(\theta) \dd\theta}
  \end{split}
\end{equation}
we can sample from
\begin{equation}
  \begin{split}
    P(\theta_i | \vtheta_{-i}, x_i)
    &= \sum_{j \neq i} b q_{i,j} \delta_{\theta_j}(\theta_i)
                   + b r_i P(\theta_i|x_i), ~ \text{with}\\
    q_{i,j} &= b P(x_i|\theta_j), ~ \text{and}\\
    r_i    &= b \alpha \int P(x_i|\theta)P(\theta) \dd\theta
  \end{split}
\end{equation}
(cf. \cite{Escobar1994, Escobar1995})

% \begin{equation}
%   \begin{split}
%     P(c_i = c| y_i, \vec{c}_{-i}, \vec{\theta}^*)
%     &= \frac{P(y_i | c_i, \vec{c}_{-i}, \vec{\theta}^*)P(c_i | \vec{c}_{-i}, \vec{\theta}^*)}{P(y_i | \vec{c}_{-i}, \vec{\theta}^*)}\\
%     &= \frac{P(y_i | c_i, \vec{\theta}^*)P(c_i | \vec{c}_{-i})}{P(y_i | \vec{c}_{-i}, \vec{\theta}^*)}\\
%     &= b P(y_i | c_i, \vec{\theta}^*) \frac{n_{c}(\vec{c}_{-i}) + \alpha/K}{m_T - 1 + \alpha}
%   \end{split}
% \end{equation}

If we instead use the representation as a Chinese restaurant process,
we get
\begin{equation}
  \begin{split}
  P(\theta_i = \theta^*_k | x_i, \vtheta_{-i})
  &=
  \frac{P(x_i | \theta_i = \theta^*_k, \vtheta_{-i}) P(\theta_i =
    \theta^*_k | \vtheta_{-i})}{P(x_i | \vtheta_{-i})}\\
  &=
  \underbrace{\frac{1}{P(x_i|\vtheta_{-i})}}_{b}
  \frac{n_{\theta^*_k}(\vtheta_{-i})}{m_T - 1 + \alpha} P(x_i | \theta^*_k)\\
  %
  P(\forall k ~ \theta_i \neq \theta^*_k | x_i, \vtheta_{-i})
  &=
  \frac{P(x_i | \forall k ~ \theta_i \neq \theta^*_k, \vtheta_{-i})
    P(\forall k ~ \theta_i \neq \theta^*_k | \vtheta_{-i})}{P(x_i | \vtheta_{-i})}\\
  &=
  \underbrace{\frac{1}{P(x_i|\vtheta_{-i})}}_{b}
  \frac{\alpha}{m_T - 1 + \alpha} \int P(x_i | \theta) P(\theta) \dd\theta\\
  \end{split}
\end{equation}
(cf. \cite{Bush1996})
