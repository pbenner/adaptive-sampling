% -*- mode: latex -*-
%
\section*{Model}
%
\tikzstyle{rv}=        [shape=circle,    minimum size=1.2cm]
\tikzstyle{parameter}= [shape=rectangle, minimum size=1.2cm]
\begin{figure}[htbp]
  \begin{center}
    \begin{tikzpicture}[]
      % states
      \node[parameter] (A1) at (0,4) {$\alpha_{j-1}$}
      ;
      \node[parameter] (A2) at (2,4) {$\alpha_{j}$}
      ;
      \node[parameter] (A3) at (4,4) {$\alpha_{j+1}$}
      ;
      \node[parameter] (A4) at (6,4) {$\alpha_{j+2}$}
      ;
      % theta 1
      \node[rv] (T0) at (-2,2) {}
      ;
      \node[rv] (T1) at (-0.25,2) {$\Thetab^1_{j-1}$}
        edge [<-] (T0)
        edge [<-] (A1)
      ;
      \node[rv] (T2) at (1.75,2) {$\Thetab^1_{j}$}
        edge [<-] (T1)
        edge [<-] (A2)
      ;
      \node[rv] (T3) at (3.75,2) {$\Thetab^1_{j+1}$}
        edge [<-] (T2)
        edge [<-] (A3)
      ;
      \node[rv] (T4) at (5.75,2) {$\Thetab^1_{j+2}$}
        edge [<-] (T3)
        edge [<-] (A4)
      ;
      \node[rv] (T5) at (7.75,2) {}
        edge [<-] (T4)
      ;
      % theta 2
      \node[rv] (S0) at (-1.5,2.5) {}
      ;
      \node[rv] (S1) at (0.5,2.5) {$\Thetab^2_{j-1}$}
        edge [->] (S0)
        edge [<-] (A1)
      ;
      \node[rv] (S2) at (2.5,2.5) {$\Thetab^2_{j}$}
        edge [->] (S1)
        edge [<-] (A2)
      ;
      \node[rv] (S3) at (4.5,2.5) {$\Thetab^2_{j+1}$}
        edge [->] (S2)
        edge [<-] (A3)
      ;
      \node[rv] (S4) at (6.5,2.5) {$\Thetab^2_{j+2}$}
        edge [->] (S3)
        edge [<-] (A4)
      ;
      \node[rv] (S5) at (8.5,2.5) {}
        edge [->] (S4)
      ;
      % observations
      \node[rv] (X1) at (0,0) {$\Xb_{j-1}$}
        edge [<-] (T1)
        edge [<-] (S1)
      ;
      \node[rv] (X2) at (2,0) {$\Xb_{j}$}
        edge [<-] (T2)
        edge [<-] (S2)
      ;
      \node[rv] (X3) at (4,0) {$\Xb_{j+1}$}
        edge [<-] (T3)
        edge [<-] (S3)
      ;
      \node[rv] (X4) at (6,0) {$\Xb_{j+2}$}
        edge [<-] (T4)
        edge [<-] (S4)
      ;
    \end{tikzpicture}
  \end{center}
  \caption{Polya urn hidden Markov mixture process.}
  \label{fig:1}
\end{figure}

\begin{equation*}
  \begin{aligned}
    \Thetab^1_j
    &\given
    \Thetab^1_{j-1} = \thetab^1_{j-1}
    &\sim&\quad
    \rho
    \delta_{\theta^1_{j-1}}
    +
    (1-\rho)
    \mu_{\Dir(\alpha_j)}
    \\
    \Thetab^2_j
    &\given
    \Thetab^2_{j+1} = \thetab^2_{j+1}
    &\sim&\quad
    \rho
    \delta_{\theta^2_{j+1}}
    +
    (1-\rho)
    \mu_{\Dir(\alpha_j)}
    \\
    X_{j,m}
    &\given
    \Thetab^1_j = \thetab^1_j,
    \Thetab^2_j = \thetab^2_j
    &\sim&\quad
    \gamma
    \Discrete(\thetab^1_j)
    +
    (1-\gamma)
    \Discrete(\thetab^2_j)
  \end{aligned}
\end{equation*}
with $\Xb_j = (X_{j,m})_m$ and mixture parameter $\gamma \in [0, 1]$.

Define the transition kernel
\begin{equation*}
  \mu_T(d\theta_{j+1} \given \theta_j)
  =
  \rho
  \delta_{\theta_j}(d\theta_{j+1})
  +
  (1-\rho)
  \mu_{\Dir(\alpha_{j+1})}(d\theta_{j+1})
\end{equation*}
and multinomial emission probabilities $p_{X_{j}\given
  \Theta_{j}}(x_{j} \given \theta_{j})$.

The marginal posterior measure is given by
\begin{equation*}
  \mu_{\Theta_j \given \Xb_{1:n}}(d\theta_j \given x_{1:n})
  \propto
  \mu_{\Theta_{j} \given \Xb_{1:j}}(d\theta_{j} \given \xb_{1:j})
  p_{\Xb_{j+1:n} \given \Theta_j}(\xb_{j+1:n} \given \theta_j)
\end{equation*}

Forward measures
\begin{multline*}
  \mu_{\Theta_{j+1} \given \Xb_{1:j+1}}(d\theta_{j+1} \given \xb_{1:j+1})
  \\
  \begin{aligned}
    &=
    \frac{1}{p_{\Xb_{1:j+1}}(\xb_{1:j+1})}
    \int
    p_{X_{j+1}\given \Theta_{j+1}}(x_{j+1} \given \theta_{j+1})
    \mu_T(d\theta_{j+1} \given \theta_j)
    \mu_{\Theta_{j} \given \Xb_{1:j}}(d\theta_{j} \given \xb_{1:j})
    \\
    &=
    \frac{\rho}{p_{\Xb_{1:j+1}}(\xb_{1:j+1})}
    p_{X_{j+1}\given \Theta_{j+1}}(x_{j+1} \given \theta_{j+1})
    \mu_{\Theta_{j} \given \Xb_{1:j}}(d\theta_{j+1} \given \xb_{1:j})
    +
    (1-\rho)
    \mu_{\Dir(\alpha_{j+1})}(d\theta_{j+1})
  \end{aligned}
\end{multline*}
with $\mu_{\Theta_{1} \given \Xb_{1:1}}(d\theta_{1} \given \xb_{1:1})
= p_{X_{1}\given \Theta_{1}}(x_{1} \given \theta_{1})
\mu_{\Dir(\alpha_{1})}(d\theta_{1})$ and the backward probabilities
\begin{multline*}
  \begin{aligned}
    p_{\Xb_{j+1:n} \given \Theta_j}(\xb_{j+1:n} \given \theta_j)
    &=
    \int
    p_{X_{j+1}\given \Theta_{j+1}}(x_{j+1} \given \theta_{j+1})
    p_{\Xb_{j+2:n} \given \Theta_{j+1}}(\xb_{j+2:n} \given \theta_{j+1})
    \mu_T(d\theta_{j+1} \given \theta_j)
    \\
    &=
    \rho
    p_{X_{j+1}\given \Theta_{j+1}}(x_{j+1} \given \theta_{j})
    p_{\Xb_{j+2:n} \given \Theta_{j+1}}(\xb_{j+2:n} \given \theta_{j})
    \\
    &\quad+
    (1-\rho)
    \int
    p_{X_{j+1}\given \Theta_{j+1}}(x_{j+1} \given \theta_{j+1})
    p_{\Xb_{j+2:n} \given \Theta_{j+1}}(\xb_{j+2:n} \given \theta_{j+1})
    \mu_{\Dir(\alpha_{j+1})}(d\theta_{j+1})
  \end{aligned}
\end{multline*}

\section*{Utility}
%
Assume that at position $j$ we already have $m$ samples. A suitable
utility measure $u(j, \xb_{1:n})$ for sampling at position $j$ is
defined by the mutual information between $\Thetab_{1:n}$ and $X_{j,m+1}$
given our previous observations $\{\Xb_{1:n} = \xb_{1:n} \}$, i.e.
\begin{equation*}
  \begin{aligned}
    u(j, \xb_{1:n})
    &=
    \I(\Thetab_{1:n} ; X_{j,m+1}) \given \Xb_{1:n} = \xb_{1:n}
    \\
    &=
    \E_{X_{j,m+1}}\left[
      \Dkl\left(
      \mu_{\Thetab_{1:n} \given \Xb_{1:n}, X_{j,m+1}}(\cdot \given \xb_{1:n}, x)
      \ggiven
      \mu_{\Thetab_{1:n} \given \Xb_{1:n}}(\cdot \given \xb_{1:n})
      \right)
      \given
      \Xb_{1:n} = \xb_{1:n}
    \right]
    \ ,
  \end{aligned}
\end{equation*}
with
\begin{multline*}
  \Dkl\left(
  \mu_{\Thetab_{1:n} \given \Xb_{1:n}, X_{j,m+1}}(\cdot \given \xb_{1:n}, x)
  \ggiven
  \mu_{\Thetab_{1:n} \given \Xb_{1:n}}(\cdot \given \xb_{1:n})
  \right)
  \\
  \begin{aligned}
    &=
    \ln
    \frac{
      p_{\Xb_{1:n}}(\xb_{1:n})
    }{
      p_{\Xb_{1:n}, X_{j,m+1}}(\xb_{1:n}, x)
    }
    +
    \frac{
      1
    }{
      p_{\Xb_{1:j+1}, X_{j,m+1}}(\xb_{1:n}, x)
    }
    \int
    \theta_{j,x} \ln(\theta_{j,x})
    \prod_{i=1}^n
    p_{\Xb_{i}\given \Thetab_{i}}(\xb_{i} \given \thetab_{i})
    \mu_T(d\thetab_{i} \given \thetab_{i-1})
    \ .
  \end{aligned}
\end{multline*}

\section*{Computation}
%
For $0 \le i \le j \le k \le n$ we define
\begin{equation*}
  \begin{aligned}
    \lambda^F_k(h)
    &=
    \rho^{k-1} h(1, k)
    +
    \sum_{j=1}^{k-1} \lambda^F_j(h) (1-\rho)\rho^{k-j-1}h(j+1, k)
    \\
    \lambda^B_i(h)
    &=
    \rho^{n-i} h(i, n)
    +
    \sum_{j=i+1}^{n} \lambda^B_j(h) (1-\rho)\rho^{j-i-1}h(i, j-1)
    \\
    \lambda^{FB}_j(h_1, h_2)
    &=
    \rho^{n-j} \lambda^F_n(h_1)
    +
    \sum_{k=j+1}^{n} \lambda^F_{k-1}(h_1) (1-\rho)\rho^{k-j-1}\lambda^B_k(h_2)
  \end{aligned}
\end{equation*}
Posterior marginal expectation
\begin{equation*}
  \E[\Thetab_j \given \Xb_{1:n} = \xb_{1:n}]
  =
  \int
  \thetab_j
  \mu_{\Theta_j \given \Xb_{1:n}}(d\thetab_j \given x_{1:n})
  =
  \lambda^{FB}_j(h_e, h_p)
\end{equation*}
with
\begin{equation*}
  \begin{aligned}
    h_p(i,k)
    =
    \frac{
      \Beta(\alphab_i + \sum_{j=i}^k \nb_j)
    }{
      \Beta(\alphab_i)
    }
    \ 
    ,
    \quad
    h_e(i,k)
    =
    h_p(i,k)
    \frac{
      \alphab_i + \sum_{j=i}^k \nb_j
    }{
      \sum_{l'}
      \alpha_{i,l'} + \sum_{j=i}^k n_{j,l'}
    }
  \end{aligned}
\end{equation*}
Utility
\begin{equation*}
  u(j, l, \xb_{1:n})
  =
  \ln
  \frac{
    p_{\Xb_{1:n}}(\xb_{1:n})
  }{
    p_{\Xb_{1:n}}(\xb_{1:n}^{+(j,l)})
  }
  +
  \frac{
    \lambda^F_{n,n}(h_u^{(j,l)})
  }{
    p_{\Xb_{1:j+1}}(\xb_{1:n}^{+(j,l)})
  }
\end{equation*}
with
\begin{equation*}
  h_u^{(j,l)}(i, k)
  =
  \frac{
    \Beta(\alphab_i + \sum_{j'=i}^k \nb_{j'})
  }{
    \Beta(\alphab_i)
  }
  \left(
  \psi\left(\alpha_{i,l} + \sum_{j'=i}^k n_{j',l}\right)
  -
  \psi\left(\sum_{l'} \alpha_{i,l'} + \sum_{j'=i}^k n_{j',l'}\right)
  \right)^{\1(i \le j \le k)}
\end{equation*}
