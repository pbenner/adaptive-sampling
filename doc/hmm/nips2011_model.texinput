% -*- mode: latex -*-
%
\section*{Motivation}
\section*{Model}
%
\tikzstyle{rv}=        [shape=circle,    minimum size=1.08cm, fill=blue!10]
\tikzstyle{rvback}=    [shape=circle,    minimum size=1.08cm, fill=blue!5]
\tikzstyle{empty}=     [shape=circle,    minimum size=1.08cm]
\tikzstyle{parameter}= [shape=rectangle, minimum size=1.08cm]
\begin{figure}[htbp]
  \begin{center}
    \begin{tikzpicture}[]
      % states
      \node[parameter] (A1) at (0,4) {$\alpha_{j-1}$}
      ;
      \node[parameter] (A2) at (2,4) {$\alpha_{j}$}
      ;
      \node[parameter] (A3) at (4,4) {$\alpha_{j+1}$}
      ;
      \node[parameter] (A4) at (6,4) {$\alpha_{j+2}$}
      ;
      % theta 2
      \node[empty] (S0) at (-1.5,2.5) {}
      ;
      \node[rvback] (S1) at (0.5,2.5) {$\Thetab^2_{j-1}$}
        edge [->] (S0)
        edge [<-] (A1)
      ;
      \node[rvback] (S2) at (2.5,2.5) {$\Thetab^2_{j}$}
        edge [->] (S1)
        edge [<-] (A2)
      ;
      \node[rvback] (S3) at (4.5,2.5) {$\Thetab^2_{j+1}$}
        edge [->] (S2)
        edge [<-] (A3)
      ;
      \node[rvback] (S4) at (6.5,2.5) {$\Thetab^2_{j+2}$}
        edge [->] (S3)
        edge [<-] (A4)
      ;
      \node[empty] (S5) at (8.5,2.5) {}
        edge [->] (S4)
      ;
      % theta 1
      \node[empty] (T0) at (-2,2) {}
      ;
      \node[rv] (T1) at (-0.25,2) {$\Thetab^1_{j-1}$}
        edge [<-] (T0)
        edge [<-] (A1)
      ;
      \node[rv] (T2) at (1.75,2) {$\Thetab^1_{j}$}
        edge [<-] (T1)
        edge [<-] (A2)
      ;
      \node[rv] (T3) at (3.75,2) {$\Thetab^1_{j+1}$}
        edge [<-] (T2)
        edge [<-] (A3)
      ;
      \node[rv] (T4) at (5.75,2) {$\Thetab^1_{j+2}$}
        edge [<-] (T3)
        edge [<-] (A4)
      ;
      \node[empty] (T5) at (7.75,2) {}
        edge [<-] (T4)
      ;
      % observations
      \node[rv] (X1) at (0,0) {$\Zb_{j-1}$}
        edge [<-] (T1)
        edge [<-] (S1)
      ;
      \node[rv] (X2) at (2,0) {$\Zb_{j}$}
        edge [<-] (T2)
        edge [<-] (S2)
      ;
      \node[rv] (X3) at (4,0) {$\Zb_{j+1}$}
        edge [<-] (T3)
        edge [<-] (S3)
      ;
      \node[rv] (X4) at (6,0) {$\Zb_{j+2}$}
        edge [<-] (T4)
        edge [<-] (S4)
      ;
    \end{tikzpicture}
  \end{center}
  \caption{
    \Polya urn hidden Markov process. The generative process can be
    sketched as follows: For the chain $M_1$ we decide at each
    position $j$ whether or not the random variable $\Thetab^1_j$
    shares its event with its predecessor $\Thetab^2_{j-1}$. It shares
    the same event with probability $\rho$, whereas with probability
    $1-\rho$ a new value is drawn from a Dirichlet distribution with
    parameters $\alphab_j$. For $M_2$ we do the same in reverse
    direction. Emissions $\Zb_j = (Z_{j,1}, Z_{j,2}, \dots, Z_{j,m})$
    are then drawn i.i.d. from the mixture $\Thetab_j = \gamma
    \Thetab^1_j + (1-\gamma) \Thetab^2_j$.
  }
  \label{fig:1}
\end{figure}
The model consists of two hidden Markov chains $M^1 =
\{\Thetab^1_j\}_{j=1}^L$ and $M^2 = \{\Thetab^2_j\}_{j=1}^L$ on a
probability space $(\Omega, \Bc(\Omega), P)$. The transition kernels
$\mu^1_T, \mu^2_T: \Bc(\Omega) \times \Omega \rightarrow \mathbb{R}^+$
with
\begin{equation*}
  P(\Theta^1_{j} \in A \given \Theta^1_{j-1} = \thetab_{j-1})
  =
  \mu^1_T(A \given \thetab_{j-1})
  \ ,
  \quad
  P(\Theta^2_{j} \in A \given \Theta^2_{j+1} = \thetab_{j+1})
  =
  \mu^2_T(A \given \thetab_{j+1})
  \ ,
\end{equation*}
are defined as
\begin{equation*}
  \begin{aligned}
    \mu^1_T(d\thetab_j \given \thetab_{j-1})
    &=
    \rho
    \delta_{\thetab_{j-1}}(d\thetab_j)
    +
    (1-\rho)
    \mu_{\Thetab^1_j}(d\thetab_j)
    \ ,
    \\
    \mu^2_T(d\thetab_j \given \thetab_{j+1})
    &=
    \rho
    \delta_{\thetab_{j+1}}(d\thetab_j)
    +
    (1-\rho)
    \mu_{\Thetab^2_j}(d\thetab_j)
    \ ,
  \end{aligned}
\end{equation*}
where $\mu_{\Thetab^1_j}$ and $\mu_{\Thetab^2_j}$ are Dirichlet
distributions on $\Omega$ with parameters $\alphab_j$ and $\rho \in
[0,1]$ is the cohesion parameter. We also define the auxiliary random
variables $\Thetab_j = \gamma \Thetab^1_j + (1-\gamma)\Thetab^2_j$
with mixture parameter $\gamma \in [0, 1]$. At each position $j$ we have $m =
m(j)$ emissions $\Zb_j = (Z_{j,m})_m$ with distribution
\begin{equation*}
  Z_{j,m}
  \given
  \Thetab_j = \thetab_j
  \sim 
  \Discrete(\thetab_j)
  \ .
\end{equation*}
Each $Z_{j,m}$ takes values in some finite alphabet $\Ac$ such that
$\Omega = \Delta_{|\Ac|}$, the $|\Ac|$-dimensional probability
simplex. The full model is depicted in Figure~\ref{fig:1}. Our main
interest is to compute
\begin{equation*}
  \mu_{\Thetab_j \given \Zb_{1:L}}
  f
  =
  \int_{\Omega}
  f(\thetab)
  \mu_{\Thetab_j \given \Zb_{1:L}}(d\thetab \given \zb_{1:L})
  =
  \int_{\Omega^L}
  f(\thetab_j)
  \prod_{i=1}^L
  p_{\Zb_{i}\given \Thetab_{i}}(\zb_{i} \given \thetab_{i})
  \mu_T(d\thetab_{i} \given \thetab_{i-1})
  \ .
\end{equation*}
for some $f \in L^1$ and $j \in \{1, 2, \dots, n\}$.
With the transition kernel $\mu_T$ we obtain a posterior distribution
that can be written as a sum over all consecutive partitions of $\{1,
2, \dots, L\}$. An element of a consecutive partition consists of
consecutive integers. We denote the set of all such partitions as
$\Pc(1:L)$. For instance, the marginal likelihood
$p_{\Zb_{1:L}}(\zb_{1:L})$ can be written as
\begin{equation*}
  \sum_{P\in \Pc(1:L)}
  \rho^{L-|P|}
  (1-\rho)^{|P|-1}
  \left[
    \gamma
    \prod_{B\in P}
    \frac{
      \Beta\left(\alphab_{\min(B)} + \sum_{j \in B} \cb(\zb_j)\right)
    }{
      \Beta\left(\alphab_{\min(B)}\right)
    }
    +
    (1-\gamma)
    \prod_{B\in P}
    \frac{
      \Beta\left(\alphab_{\max(B)} + \sum_{j \in B} \cb(\zb_j)\right)
    }{
      \Beta\left(\alphab_{\max(B)}\right)
    }
  \right]
  \ .
\end{equation*}
An efficient way of computing the posterior distribution can be
derived by considering the forward-backward decomposition of hidden
Markov models. The measure
\begin{equation*}
  \mu_{\Thetab^1_j \given \Zb_{1:L}}(d\thetab_j \given \zb_{1:L})
  \propto
  \mu_{\Thetab^1_j \given \Zb_{1:j}}(d\thetab_{j} \given \zb_{1:j})
  p_{\Zb_{j+1:L} \given \Thetab^1_j}(\zb_{j+1:L} \given \thetab_j)
  \ ,
\end{equation*}
consists of the forward measure
\begin{multline*}
  \mu_{\Thetab^1_{j+1} \given \Zb_{1:j+1}}(d\thetab_{j+1} \given \zb_{1:j+1})
  \\
  \begin{aligned}
    &=
    \frac{1}{p_{\Zb_{1:j+1}}(\zb_{1:j+1})}
    \int_{\Omega}
    p_{\Zb_{j+1}\given \Thetab^1_{j+1}}(\zb_{j+1} \given \thetab_{j+1})
    \mu_T(d\thetab_{j+1} \given \thetab_j)
    \mu_{\Thetab^1_{j} \given \Zb_{1:j}}(d\thetab_{j} \given \zb_{1:j})
    \\
    &=
    \frac{\rho}{p_{\Zb_{1:j+1}}(\zb_{1:j+1})}
    p_{\Zb_{j+1}\given \Thetab^1_{j+1}}(\zb_{j+1} \given \thetab_{j+1})
    \mu_{\Thetab^1_{j} \given \Zb_{1:j}}(d\thetab_{j+1} \given \zb_{1:j})
    +
    (1-\rho)
    \mu_{\Thetab^1_{j+1}}(d\thetab_{j+1})
    \ ,
  \end{aligned}
\end{multline*}
with $\mu_{\Thetab^1_1 \given \Zb_{1:1}}(d\thetab_{1} \given \zb_{1:1})
= p_{\Zb_{1}\given \Thetab^1_1}(\zb_{1} \given \thetab_{1})
\mu_{\Thetab^1_1}(d\thetab_{1})$ and backward probabilities
\begin{multline*}
  \begin{aligned}
    p_{\Zb_{j+1:L} \given \Thetab^1_j}(\zb_{j+1:L} \given \thetab_j)
    &=
    \int_{\Omega}
    p_{\Zb_{j+1}\given \Thetab^1_{j+1}}(\zb_{j+1} \given \thetab_{j+1})
    p_{\Zb_{j+2:L} \given \Thetab^1_{j+1}}(\zb_{j+2:L} \given \thetab_{j+1})
    \mu^1_T(d\thetab_{j+1} \given \thetab_j)
    \\
    &=
    \rho
    p_{\Zb_{j+1}\given \Thetab^1_{j+1}}(\zb_{j+1} \given \thetab_{j})
    f_{\Thetab^1_{j+1}}(\thetab_{j})
    p_{\Zb_{j+2:L} \given \Thetab^1_{j+1}}(\zb_{j+2:L} \given \thetab_{j})
    \\
    &\quad+
    (1-\rho)
    \int_{\Omega}
    p_{\Zb_{j+1}\given \Thetab^1_{j+1}}(\zb_{j+1} \given \thetab_{j+1})
    p_{\Zb_{j+2:L} \given \Thetab^1_{j+1}}(\zb_{j+2:L} \given \thetab_{j+1})
    \mu_{\Thetab^1_{j+1}}(d\thetab_{j+1})
    \ .
  \end{aligned}
\end{multline*}
Both, the forward measure and the backward probabilities can be
computed in $\Oc(L^2)$.

\section*{Adaptive sequential sampling}
To formalize the sequential sampling process we introduce the random
variables $Y_n = Z_{j,m}$ with $m = \sum_{i=1}^n \1(X_i = j)$ at the
$n$th sampling step. That is, $X_n$ gives the stimulus for the $n$th
sample and $Y_n$ the response.  Furthermore, we summarize the first
$n$ experiments $(X_i, Y_i)_{i=1}^n$ as $\Eb_{1:n}$. Let's assume that
we want to take $N$ measurements in total and we have already taken
$n$ samples. We define a utility measure $u_p$ that quantifies how
much we value a sequence of $p = N-n$ measurements $\xb_{n+1:n+p} = \{
x_{n+1}, x_{n+2}, \dots, x_{n+p}\}$ with responses $\yb_{n+1:n+p} = \{
y_{n+1}, y_{n+2}, \dots, y_{n+p}\}$. Furthermore, we define the
\emph{expected utility}
\begin{multline*}
  U_p(\Xb_{n+1:n+p} = \xb_{n+1:n+p} \given \Eb_{1:n} = \eb_{1:n})
  \\
  =
  \E_{\Yb_{n+1:n+p}}[
    u_p(\Xb_{n+1:n+p} = \xb_{n+1:n+p}, \Yb_{n+1:n+p} \given \Eb_{1:n} = \eb_{1:n})
    \given
    \Xb_{n+1:n+p} = \xb_{n+1:n+p},
    \Eb_{1:n} = \eb_{1:n}
  ]
  \ .
\end{multline*}
Our goal is then to select a stimulus $x^*_{n+1}$ for the next
measurement $X_{n+1}$ that leads to a maximal expected utility in the
next $p$ experiments, i.e.
\begin{equation*}
  x^*_{n+1}
  =
  \argmax_{x_{n+1} \in 1:L}
  \max_{\xb_{2:p-1} \in (1:L)^{p-1}}
  U_p(\Xb_{n+1:n+p} = \xb_{n+1:n+n} \given \Eb_{1:n})
  \ .
\end{equation*}
With a $p$-step look-ahead we define the \emph{optimal expected
  utility} $U^*_p(X_{n+1} = x_{n+1}, Y_{n+1} = y_{n+1} \given
\Eb_{1:n} = \eb_{1:n})$ recursively as
\begin{equation*}
  \max_{x_{n+2}\in1:L}
  \E_{Y_{n+2}}[
    U^*_{p-1}(X_{n+2} = x_{n+2}, Y_{n+2}, \Eb_{n+1:n+1} = \eb_{n+1:n+1}
    \given
    \Eb_{1:n} = \eb_{1:n}) \given X_{n+2} = x_{n+2}, \Eb_{1:n+1} = \eb_{1:n+1}
  ]
  \ ,
\end{equation*}
where
\begin{equation*}
  U^*_0(\Eb_{n+1:n+p} = \eb_{n+1:n+p} \given \Eb_{1:n} = \eb_{1:n})
  =
  u_p(\Eb_{n+1:n+p} = \eb_{n+1:n+p} \given \Eb_{1:n} = \eb_{1:n}) \ .
\end{equation*}
We obtain the optimal stimulus $x^*_{n+1}$ from
\begin{multline*}
  x^*_{n+1}
  =
  \argmax_{x_{n+1} \in 1:L}
  \max_{\xb_{2:p-1} \in (1:L)^{p-1}}
  U_p(\Xb_{n+1:n+p} = \xb_{n+1:n+p} \given \Eb_{1:n} = \eb_{1:n})
  \\
  =
  \argmax_{x_{n+1}\in 1:L}
  \E_{Y_{n+1}}[U^*_p(X_{n+1} = x_{n+1}, Y_{n+1} \given \Eb_{1:n} = \eb_{1:n}) \given X_{n+1} = x_{n+1}, \Eb_{1:n} = \eb_{1:n}]
  \ .
\end{multline*}
We first consider the case of one-step look-ahead. A suitable utility
measure $u_1(X_{n+1} = x,  Y_{n+1} = y \given \Eb_{1:n} = \eb_{1:n})$ for a
stimulus $x$ with response $y$ is given by the Kullback-Leibler
divergence between the measures $\mu_{\Thetab_{1:L} \given \Eb_{1:n+1}}$ and 
$\mu_{\Thetab_{1:L} \given \Eb_{1:n}}$, so that the expected utility
\begin{multline*}
  U_1(X_{n+1} = x \given \Eb_{1:n} = \eb_{1:n})
  \\
  \begin{aligned}
    &=
    \E_{Y_{n+1}}\left[
      \Dkl\left(
      \mu_{\Thetab_{1:L} \given \Eb_{1:n}, X_{n+1}, Y_{n+1}}(\cdot \given \eb_{1:n}, x, y)
      \ggiven
      \mu_{\Thetab_{1:L} \given \Eb_{1:n}}(\cdot \given \eb_{1:n})
      \right)
      \given
      \Eb_{1:n} = \eb_{1:n}
    \right]
    \\
    &=
    \I(\Thetab_{1:L} ; Y_{n+1} \given X_{n+1} = x, \Eb_{1:n} = \eb_{1:n})
    \ ,
  \end{aligned}
\end{multline*}
is given by the mutual information between $\Thetab_{1:L}$ and
$Y_{n+1}$ given $X_{n+1} = x$ and our previous observations
$\{\Eb_{1:n} = \eb_{1:n} \}$. We obtain
\begin{multline*}
  \Dkl\left(
  \mu_{\Thetab_{1:L} \given \Eb_{1:n}, X_{n+1}, Y_{n+1}}(\cdot \given \eb_{1:n}, x, y)
  \ggiven
  \mu_{\Thetab_{1:L} \given \Eb_{1:n}}(\cdot \given \eb_{1:n})
  \right)
  \\
  \begin{aligned}
    &=
    \int_{\Omega^L}
    \ln
    \frac{
      \mu_{\Thetab_{1:L} \given \Eb_{1:n}, X_{n+1}, Y_{n+1}}(\cdot \given \eb_{1:n}, x, y)
    }{
      \mu_{\Thetab_{1:L} \given \Eb_{1:n}}(\cdot \given \eb_{1:n})
    }
    \mu_{\Thetab_{1:L} \given \Eb_{1:n}, X_{n+1}, Y_{n+1}}(d\thetab_{1:L} \given \eb_{1:n}, x, y)
    \ ,
  \end{aligned}
\end{multline*}
where
\begin{equation*}
    \frac{
      \mu_{\Thetab_{1:L} \given \Eb_{1:n}, X_{n+1}, Y_{n+1}}(\cdot \given \eb_{1:n}, x, y)
    }{
      \mu_{\Thetab_{1:L} \given \Eb_{1:n}}(\cdot \given \eb_{1:n})
    }
  =
  \frac{1}{p_{Y_{n+1} \given X_{n+1}, \Eb_{1:n}}(y \given x, \eb_{1:n})}
  \theta_{x,y}
\end{equation*}
is the Radon-Nykod{\'y}m derivative of $\mu_{\Thetab_{1:L} \given
  \Eb_{1:n+1}}$ with respect to $\mu_{\Thetab_{1:L} \given
  \Eb_{1:n}}$. Therefore, we obtain
\begin{multline*}
  \Dkl\left(
  \mu_{\Thetab_{1:L} \given \Eb_{1:n}, X_{n+1}, Y_{n+1}}(\cdot \given \eb_{1:n}, x, y)
  \ggiven
  \mu_{\Thetab_{1:L} \given \Eb_{1:n}}(\cdot \given \eb_{1:n})
  \right)
  \\
  \begin{aligned}
    &=
    \frac{
      1
    }{
      p_{Y_{n+1} \given X_{n+1}, \Eb_{1:n}}(y \given x, \eb_{1:n})
    }
    \int_{\Omega}
    \theta_{x,y} \ln(\theta_{x,y})
    \mu_{\Thetab_x \given \Eb_{1:n}}(d\thetab_x \given \eb_{1:n})
    -
    \ln
    p_{Y_{n+1} \given X_{n+1}, \Eb_{1:n}}(y \given x, \eb_{1:n})
    \ ,
  \end{aligned}
\end{multline*}
where we used
\begin{equation*}
  \int_{\Omega^L}
  \theta_{x,y} \ln(\theta_{x,y})
  \prod_{i=1}^L
  p_{\Zb_i \given \Thetab_i}(\zb_i \given \thetab_i)
  \mu_T(d\thetab_i \given \thetab_{i-1})
  =
  \int_{\Omega}
  \theta_{x,y} \ln(\theta_{x,y})
  \mu_{\Thetab_x \given \Eb_{1:n}}(d\thetab_x \given \eb_{1:n})
  \ .
\end{equation*}
We obtain the expected utility
\begin{equation*}
  U_1(X_{n+1} = x \given \Eb_{1:n} = \eb_{1:n})
  =
  \Entropy\left[Y_{n+1} \given X_{n+1} = x, \Eb_{1:n} = \eb_{1:n}\right]
  +
  \sum_{y\in\Ac}
  \int_{\Omega}
  \theta_{x,y} \ln(\theta_{x,y})
  \mu_{\Thetab_x \given \Eb_{1:n}}(d\thetab_x \given \eb_{1:n})
  \ .
\end{equation*}
Similarly, for a set of $p$ stimuli $\xb_{n+1:n+p} = \{x_{n+1}, x_
{n+2}, \dots, x_{n+p}\}$ the expected utility is
\begin{multline*}
  \begin{aligned}
    U_p(\Xb_{n+1:n+p} = \xb_{n+1:n+p} \given \Eb_{1:n} = \eb_{1:n})
    &=
%    \sum_{i=1}^p
%    \Entropy\left[Y_{n+i} \given \Eb_{1:n} = \eb_{1:n}, X_{n+1} = x_{n+1}, Y_{n+1}, \dots, X_{n+i} = x_{n+i}\right]
    \Entropy\left[\Yb_{n+1:n+p} \given \Xb_{n+1:n+p} = \xb_{n+1:n+p}, \Eb_{1:n} = \eb_{1:n}\right]
    \\
    &\quad
    +
    \sum_{i=1}^p
    \sum_{y \in\Ac}
    \int_{\Omega}
    \theta_{y} \ln(\theta_{y})
    \mu_{\Thetab_{x_{n+i}} \given \Eb_{1:n}}(d\thetab \given \eb_{1:n})
    \ .
  \end{aligned}
\end{multline*}
The expected utility can also be written in a recursive way as
\begin{multline*}
  U_p(\Xb_{n+1:n+p} = \xb_{n+1:n+p} \given \Eb_{1:n} = \eb_{1:n})
  \\
  \begin{aligned}
    &=
    U_1(X_{n+1} = x_{n+1} \given \Eb_{1:n} = \eb_{1:n})
    \\
    &\quad
    +
    \sum_{y \in\Ac}
    p_{Y_{n+1} \given X_{n+1}, \Eb_{1:n}} (y \given x_{n+1}, \eb_{1:n})
    U_{p-1}(\Xb_{n+2:n+p} = \xb_{n+2:n+p} \given \Eb_{1:n+1} = \eb_{1:n+1})
    \\
    &=
    \sum_{y \in\Ac}
    p_{Y_{n+1} \given X_{n+1}, \Eb_{1:n}} (y \given x_{n+1}, \eb_{1:n})
    \\
    &\quad\quad\quad
    \left[
    u_1(E_{n+1} = e_{n+1} \given \Eb_{1:n} = \eb_{1:n})
    +
    U_{p-1}(\Xb_{n+2:n+p} = \xb_{n+2:n+p} \given \Eb_{1:n+1} = \eb_{1:n+1})
    \right]
    \ ,
  \end{aligned}
\end{multline*}
which shows that the sampling process can be interpreted as a Markov
decision process (MDP), since in every sampling step we receive an
immediate reward given by the utility $u_1$. It also justifies myopic
sampling strategies.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.9\textwidth,page=1]{example/decision-tree.pdf}
  \caption{
    Sketch of the decision tree for an experiment with two stimuli
    $\{l, r\}$ and dichotomous outcome $\{s, f\}$. The tree shows all
    experimental outcomes for $N = 2$.  Circles represent states, which
    are determined by the count statistic, whereas rectangles represent
    decision nodes. Optimal paths are labeled by an arrow. An
    immediate utility $u_1$ is received whenever one of the circles is
    visited.
  }
  \label{fig:4}
\end{figure}

\section*{Markov decision process}
%
We continue by describing the adaptive sampling process as a Markov
decision process (MDP) with state space $S$ given by all possible
count statistics $\cb(\zb_{1:L})$ and action space $A = 1:L$. Suppose
we have performed $n$ experiments $\Eb_{1:n} = \eb_{1:n}$ and let
$s_n = \cb(\eb_{1:n}) \in S$. The state transition probabilities are
given by
\begin{equation*}
  p_{Tr}(s_{n+1} \given x_{n+1}, s_n)
  =
  p_{Y_{n+1}\given X_{n+1}, \Eb_{1:n}}(y_{n+1} \given x_{n+1}, \eb_{1:n})
  \ ,
\end{equation*}
and the immediate reward is
\begin{equation*}
  r(s_{n+1} \given s_n)
  =
  u_1(E_{n+1} = e_{n+1} \given \Eb_{1:n} = \eb_{1:n})
  \ .
\end{equation*}
Our goal is to find the policy $\pi(s)$ that finds for every state $s
\in S$ an optimal action $a$ so that
\begin{equation*}
  \sum_{n=1}^N
  u_1(E_{n} = e_{n} \given \Eb_{1:n-1} = \eb_{1:n-1})
\end{equation*}
is maximized. The MDP that we describe is special in the sense that
each state can be visited only once and after $N$ samples the
experiment stops. This simplifies for instance the computation of the
value function, given here as $U_p$, which can be computed
directly.

\section*{Computation}
%
By solving the recursive definitions of the forward measures and
backward probabilities we obtain the same computational structure as
in \cite{Yao1984} or \cite{Barry1992}. We discuss the computation for
the Markov chain $M_1$. For $0 \le i \le j \le k \le L$ we define
\begin{equation*}
  \begin{aligned}
    \lambda^F_k(h)
    &=
    \rho^{k-1} h(1, k)
    +
    \sum_{j=1}^{k-1} \lambda^F_j(h) (1-\rho)\rho^{k-j-1}h(j+1, k)
    \ ,
    \\
    \lambda^B_i(h)
    &=
    \rho^{n-i} h(i, n)
    +
    \sum_{j=i+1}^{n} \lambda^B_j(h) (1-\rho)\rho^{j-i-1}h(i, j-1)
    \ ,
  \end{aligned}
\end{equation*}
so that, for instance, $p_{\Zb_{1:L}}(\zb_{1:L}) = \lambda^F_L(h_p) =
\lambda^B_1(h_p)$ with
\begin{equation*}
  h_p(i,k)
  =
  \frac{
    \Beta\left(\alphab_i + \sum_{j=i}^k \cb(\zb_j)\right)
  }{
    \Beta(\alphab_i)
  }
  \ ,
\end{equation*}
where $\cb(\zb_j)$ is the count statistic of $\{\Zb_j = \zb_j\}$.
To compute $\mu_{\Thetab_j \given \Zb_{1:L}} f$ for some $f \in L^1$
we define
\begin{multline*}
  \lambda^{FB}_j(h)
  =
  \rho^{L-1}h(1, L)
  +
  \sum_{i=1}^{j-1}(1-\rho)\rho^{L-i-1}\lambda^F_i(h_p) h(i+1, L)
  \\
  +
  \sum_{k=j+1}^L (1-\rho)
  \left[
    \rho^{k-2} h(1, k-1)
    +
    \sum_{i=1}^{j-1} (1-\rho) \rho^{k-i-2}
    \lambda^F_i(h_p) h(i+1, k-1)
  \right]
  \lambda^B_k(h_p)
\end{multline*}
with
\begin{equation*}
  h(i,k)
  =
  \int_{\Omega}
  f(\thetab)
  \prod_{j=i}^k
  p_{\Zb_j \given \Thetab_j}(\zb_j \given \thetab)
  \mu_{\Thetab_j}(d\thetab)
\end{equation*}
so that
\begin{equation*}
  \mu_{\Thetab_j \given \Zb_{1:L}} f
  =
  \frac{
    \lambda^{FB}_j(h)
  }{
    \lambda^F_L(h_p)
  }
  \ .
\end{equation*}

For instance, the marginal posterior expectation $\mu_{\Thetab_j \given
  \Zb_{1:L}} \thetab$ can be obtained with
\begin{equation*}
  h(i,k)
  =
  \frac{h_e(i,k)}{h_p(i,k)}
  =
  \frac{
    \alphab_i+\sum_{j=i}^k\cb(\zb_j)
  }{
    \sum_{y\in\Ac}
    \alpha_{i,y}
    \sum_{j=i}^k c_y(\zb_j)
  }
  \ .
\end{equation*}
The utility for an event $y \in \Ac$ can be computed with $f(\thetab)
= \theta_y \ln \theta_y$ so that
\begin{equation*}
  \begin{aligned}
    h(i,k)
    =
    \frac{h^y_u(i,k)}{h_e(i,k)}
    &=
    \psi\left(1 + \alpha_{i,y} + \sum_{j=i}^k c_y(\zb_j)\right)
    -
    \psi\left(1 + \sum_{y\in \Ac} \alpha_{i,y} + \sum_{j=i}^k c_y(\zb_j)\right)
    \ .
  \end{aligned}
\end{equation*}

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.495\textwidth,page=2]{example/example1.pdf}
  \includegraphics[width=0.495\textwidth,page=3]{example/example1.pdf}\\
  \includegraphics[width=0.495\textwidth,page=4]{example/example1.pdf}
  \includegraphics[width=0.495\textwidth,page=5]{example/example1.pdf}\\
  \includegraphics[width=0.495\textwidth,page=6]{example/example1.pdf}
  \includegraphics[width=0.495\textwidth,page=7]{example/example1.pdf}
  \caption{
    Adaptive sampling in a hypothetical experiment with $\rho =
    0.8$. The figure shows the experiment after $1$, $2$, $10$, $50$,
    $100$, and $200$ samples. At first, samples are uniformly
    distributed. In (a) only one measurement was taken and the
    expected utility is largest at the left boundary. (d) shows the
    experiment after 50 samples where the algorithm starts to locate
    measurements at sloped regions. The general shape of the
    stimulus-response function is already well established after 100
    samples (e). Many measurements are also taken at $x=1$ and $x=35$
    since those $x$ have only one neighbor.
  }
  \label{fig:2}
\end{figure}

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.8\textwidth,page=1]{example/example2.pdf}\\
  \includegraphics[width=0.8\textwidth,page=4]{example/example2.pdf}
  \caption{
    $U^*_p(X_{n+1} = x \given \Eb_{1:n} = \eb_{1:n})$ for varying
    look-aheads $p$.
    (a) $U^*_p (X_1 = x)$ with no prior experiments.
    (b) $U^*_p (X_7 = x \given \Eb_{1:6} = \eb_{1:6})$ with six prior
    experiments.
    %$\eb_{1:6} = \{(1,1), (10, 1), (15, 2), (20, 2), (28, 1), (25, 1)\}.$
  }
  \label{fig:3}
\end{figure}
