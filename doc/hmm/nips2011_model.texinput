% -*- mode: latex -*-
%
\section*{Model}
%
\tikzstyle{rv}=        [shape=circle,    minimum size=1.2cm]
\tikzstyle{parameter}= [shape=rectangle, minimum size=1.2cm]
\begin{figure}[htbp]
  \begin{center}
    \begin{tikzpicture}[]
      % states
      \node[parameter] (A1) at (0,4) {$\alpha_{j-1}$}
      ;
      \node[parameter] (A2) at (2,4) {$\alpha_{j}$}
      ;
      \node[parameter] (A3) at (4,4) {$\alpha_{j+1}$}
      ;
      \node[parameter] (A4) at (6,4) {$\alpha_{j+2}$}
      ;
      % theta 1
      \node[rv] (T0) at (-2,2) {}
      ;
      \node[rv] (T1) at (-0.25,2) {$\Thetab^1_{j-1}$}
        edge [<-] (T0)
        edge [<-] (A1)
      ;
      \node[rv] (T2) at (1.75,2) {$\Thetab^1_{j}$}
        edge [<-] (T1)
        edge [<-] (A2)
      ;
      \node[rv] (T3) at (3.75,2) {$\Thetab^1_{j+1}$}
        edge [<-] (T2)
        edge [<-] (A3)
      ;
      \node[rv] (T4) at (5.75,2) {$\Thetab^1_{j+2}$}
        edge [<-] (T3)
        edge [<-] (A4)
      ;
      \node[rv] (T5) at (7.75,2) {}
        edge [<-] (T4)
      ;
      % theta 2
      \node[rv] (S0) at (-1.5,2.5) {}
      ;
      \node[rv] (S1) at (0.5,2.5) {$\Thetab^2_{j-1}$}
        edge [->] (S0)
        edge [<-] (A1)
      ;
      \node[rv] (S2) at (2.5,2.5) {$\Thetab^2_{j}$}
        edge [->] (S1)
        edge [<-] (A2)
      ;
      \node[rv] (S3) at (4.5,2.5) {$\Thetab^2_{j+1}$}
        edge [->] (S2)
        edge [<-] (A3)
      ;
      \node[rv] (S4) at (6.5,2.5) {$\Thetab^2_{j+2}$}
        edge [->] (S3)
        edge [<-] (A4)
      ;
      \node[rv] (S5) at (8.5,2.5) {}
        edge [->] (S4)
      ;
      % observations
      \node[rv] (X1) at (0,0) {$\Xb_{j-1}$}
        edge [<-] (T1)
        edge [<-] (S1)
      ;
      \node[rv] (X2) at (2,0) {$\Xb_{j}$}
        edge [<-] (T2)
        edge [<-] (S2)
      ;
      \node[rv] (X3) at (4,0) {$\Xb_{j+1}$}
        edge [<-] (T3)
        edge [<-] (S3)
      ;
      \node[rv] (X4) at (6,0) {$\Xb_{j+2}$}
        edge [<-] (T4)
        edge [<-] (S4)
      ;
    \end{tikzpicture}
  \end{center}
  \caption{Polya urn hidden Markov mixture process.}
  \label{fig:1}
\end{figure}
The model consists of two hidden Markov chains $M_1 =
\{\Thetab_j^1\}_{j=1}^n$ and $M_2 = \{\Thetab_j^2\}_{j=1}^n$ on a
probability space $(\Omega, \Bc(\Omega), P)$. The transition kernels
$\mu^1_T, \mu^2_T: \Bc(\Omega) \times \Omega \rightarrow \mathbb{R}^+$
with
\begin{equation*}
  \begin{aligned}
    P(\Theta^1_{j} \in A \given \Theta^1_{j-1} = \thetab_{j-1})
    =
    \mu^1_T(A \given \thetab_{j-1})
    \ ,
    \quad
    P(\Theta^2_{j} \in A \given \Theta^2_{j+1} = \thetab_{j+1})
    =
    \mu^2_T(A \given \thetab_{j+1})
    \ ,
  \end{aligned}
\end{equation*}
are defined as
\begin{equation*}
  \begin{aligned}
    \mu^1_T(d\thetab_j \given \thetab_{j-1})
    &=
    \rho
    \delta_{\thetab_{j-1}}(d\thetab_j)
    +
    (1-\rho)
    \mu_{\Dir(\alphab_j)}(d\thetab_j)
    \ ,
    \\
    \mu^2_T(d\thetab_j \given \thetab_{j+1})
    &=
    \rho
    \delta_{\thetab_{j+1}}(d\thetab_j)
    +
    (1-\rho)
    \mu_{\Dir(\alphab_j)}(d\thetab_j)
    \ ,
  \end{aligned}
\end{equation*}
where $\mu_{\Dir(\alphab_j)}$ is the Dirichlet distribution on $\Omega$
with parameters $\alphab_j$ and $\rho \in [0,1]$ a mixture parameter.
At each position $j$ we have $m = m(j)$ emissions $\Xb_j =
(X_{j,m})_m$ obtained from a mixture of the two Markov chains, i.e.
\begin{equation*}
  X_{j,m}
  \given
  \Thetab^1_j = \thetab^1_j,
  \Thetab^2_j = \thetab^2_j
  \sim 
  \gamma
  \Discrete(\thetab^1_j)
  +
  (1-\gamma)
  \Discrete(\thetab^2_j)
  \ ,
\end{equation*}
with $\gamma \in [0, 1]$. Each $X_{j,m}$ takes values in some finite
alphabet $\Ac$ such that $\Omega = \Delta_{|\Ac|}$, the
$|\Ac|$-dimensional probability simplex. The full model is depicted in
Figure~\ref{fig:1}.
%% \begin{equation*}
%%   \begin{aligned}
%%     \Thetab^1_j
%%     &\given
%%     \Thetab^1_{j-1} = \thetab^1_{j-1}
%%     &\sim&\quad
%%     \rho
%%     \delta_{\theta^1_{j-1}}
%%     +
%%     (1-\rho)
%%     \mu_{\Dir(\alpha_j)}
%%     \\
%%     \Thetab^2_j
%%     &\given
%%     \Thetab^2_{j+1} = \thetab^2_{j+1}
%%     &\sim&\quad
%%     \rho
%%     \delta_{\theta^2_{j+1}}
%%     +
%%     (1-\rho)
%%     \mu_{\Dir(\alpha_j)}
%%     \\
%%     X_{j,m}
%%     &\given
%%     \Thetab^1_j = \thetab^1_j,
%%     \Thetab^2_j = \thetab^2_j
%%     &\sim&\quad
%%     \gamma
%%     \Discrete(\thetab^1_j)
%%     +
%%     (1-\gamma)
%%     \Discrete(\thetab^2_j)
%%   \end{aligned}
%% \end{equation*}
%% with $\Xb_j = (X_{j,m})_m$ where each $X_{j,m}$ takes values in some
%% finite alphabet $\Ac$ and $\Thetab^1_j, \Thetab^2_j \in \Omega = \Delta_{|\Ac|}$.
%% Mixture parameter $\gamma \in [0, 1]$.

Since both Markov chains have the same transition kernel it suffices
to discuss inference with only one chain. Most posterior quantities of
interest, including the utility measure, can be computed with the
marginal posterior measure
\begin{equation*}
  \mu_{\Thetab_j \given \Xb_{1:n}}(d\thetab_j \given \xb_{1:n})
  \propto
  \mu_{\Thetab_{j} \given \Xb_{1:j}}(d\thetab_{j} \given \xb_{1:j})
  p_{\Xb_{j+1:n} \given \Thetab_j}(\xb_{j+1:n} \given \thetab_j)
  \ ,
\end{equation*}
which consists of the forward measure
\begin{multline*}
  \mu_{\Thetab_{j+1} \given \Xb_{1:j+1}}(d\thetab_{j+1} \given \xb_{1:j+1})
  \\
  \begin{aligned}
    &=
    \frac{1}{p_{\Xb_{1:j+1}}(\xb_{1:j+1})}
    \int_{\Omega}
    p_{\Xb_{j+1}\given \Thetab_{j+1}}(\xb_{j+1} \given \thetab_{j+1})
    \mu_T(d\thetab_{j+1} \given \thetab_j)
    \mu_{\Thetab_{j} \given \Xb_{1:j}}(d\thetab_{j} \given \xb_{1:j})
    \\
    &=
    \frac{\rho}{p_{\Xb_{1:j+1}}(\xb_{1:j+1})}
    p_{\Xb_{j+1}\given \Thetab_{j+1}}(\xb_{j+1} \given \thetab_{j+1})
    \mu_{\Thetab_{j} \given \Xb_{1:j}}(d\thetab_{j+1} \given \xb_{1:j})
    +
    (1-\rho)
    \mu_{\Dir(\alpha_{j+1})}(d\thetab_{j+1})
  \end{aligned}
\end{multline*}
with $\mu_{\Thetab_{1} \given \Xb_{1:1}}(d\thetab_{1} \given \xb_{1:1})
= p_{\Xb_{1}\given \Thetab_{1}}(\xb_{1} \given \thetab_{1})
\mu_{\Dir(\alpha_{1})}(d\thetab_{1})$ and backward probabilities
\begin{multline*}
  \begin{aligned}
    p_{\Xb_{j+1:n} \given \Thetab_j}(\xb_{j+1:n} \given \thetab_j)
    &=
    \int_{\Omega}
    p_{\Xb_{j+1}\given \Thetab_{j+1}}(\xb_{j+1} \given \thetab_{j+1})
    p_{\Xb_{j+2:n} \given \Thetab_{j+1}}(\xb_{j+2:n} \given \thetab_{j+1})
    \mu_T(d\thetab_{j+1} \given \thetab_j)
    \\
    &=
    \rho
    p_{\Xb_{j+1}\given \Thetab_{j+1}}(\xb_{j+1} \given \thetab_{j})
    p_{\Xb_{j+2:n} \given \Thetab_{j+1}}(\xb_{j+2:n} \given \thetab_{j})
    \\
    &\quad+
    (1-\rho)
    \int_{\Omega}
    p_{\Xb_{j+1}\given \Thetab_{j+1}}(\xb_{j+1} \given \thetab_{j+1})
    p_{\Xb_{j+2:n} \given \Thetab_{j+1}}(\xb_{j+2:n} \given \thetab_{j+1})
    \mu_{\Dir(\alpha_{j+1})}(d\thetab_{j+1})
    \ .
  \end{aligned}
\end{multline*}

\section*{Utility}
%
Assume that at position $j$ we already have $m$ samples. A suitable
utility measure $u(j, \xb_{1:n})$ for sampling at position $j$ is
defined by the mutual information between $\Thetab_{1:n}$ and $X_{j,m+1}$
given our previous observations $\{\Xb_{1:n} = \xb_{1:n} \}$, i.e.
\begin{equation*}
  \begin{aligned}
    u(j, \xb_{1:n})
    &=
    \I(\Thetab_{1:n} ; X_{j,m+1} \given \Xb_{1:n} = \xb_{1:n})
    \\
    &=
    \E_{X_{j,m+1}}\left[
      \Dkl\left(
      \mu_{\Thetab_{1:n} \given \Xb_{1:n}, X_{j,m+1}}(\cdot \given \xb_{1:n}, x)
      \ggiven
      \mu_{\Thetab_{1:n} \given \Xb_{1:n}}(\cdot \given \xb_{1:n})
      \right)
      \given
      \Xb_{1:n} = \xb_{1:n}
    \right]
    \ ,
  \end{aligned}
\end{equation*}
with
\begin{multline*}
  \Dkl\left(
  \mu_{\Thetab_{1:n} \given \Xb_{1:n}, X_{j,m+1}}(\cdot \given \xb_{1:n}, x)
  \ggiven
  \mu_{\Thetab_{1:n} \given \Xb_{1:n}}(\cdot \given \xb_{1:n})
  \right)
  \\
  \begin{aligned}
    %% &=
    %% \ln
    %% \frac{
    %%   p_{\Xb_{1:n}}(\xb_{1:n})
    %% }{
    %%   p_{\Xb_{1:n}, X_{j,m+1}}(\xb_{1:n}, x)
    %% }
    %% +
    %% \frac{
    %%   1
    %% }{
    %%   p_{\Xb_{1:j+1}, X_{j,m+1}}(\xb_{1:n}, x)
    %% }
    %% \int_{\Omega^n}
    %% \theta_{j,x} \ln(\theta_{j,x})
    %% \prod_{i=1}^n
    %% p_{\Xb_{i}\given \Thetab_{i}}(\xb_{i} \given \thetab_{i})
    %% \mu_T(d\thetab_{i} \given \thetab_{i-1})
    %% \\
    &=
    \ln
    \frac{
      p_{\Xb_{1:n}}(\xb_{1:n})
    }{
      p_{\Xb_{1:n}, X_{j,m+1}}(\xb_{1:n}, x)
    }
    +
    \frac{
      1
    }{
      p_{\Xb_{1:j+1}, X_{j,m+1}}(\xb_{1:n}, x)
    }
    \int_{\Omega}
    \theta_{j,x} \ln(\theta_{j,x})
    \mu_{\Thetab_j \given \Xb_{1:n}}(d\thetab_j \given \xb_{1:n})
    \ ,
  \end{aligned}
\end{multline*}
where we used
\begin{equation*}
  \int_{\Omega^n}
  \theta_{j,x} \ln(\theta_{j,x})
  \prod_{i=1}^n
  p_{\Xb_{i}\given \Thetab_{i}}(\xb_{i} \given \thetab_{i})
  \mu_T(d\thetab_{i} \given \thetab_{i-1})
  =
  \int_{\Omega}
  \theta_{j,x} \ln(\theta_{j,x})
  \mu_{\Thetab_j \given \Xb_{1:n}}(d\thetab_j \given \xb_{1:n})
  \ .
\end{equation*}

\section*{Computation}
%
By solving the recursive definitions of the forward measures and
backward probabilities we obtain the same computational structure as
in \cite{Yao1984} or \cite{Barry1992}.
For $0 \le i \le j \le k \le n$ we define
\begin{equation*}
  \begin{aligned}
    \lambda^F_k(h)
    &=
    \rho^{k-1} h(1, k)
    +
    \sum_{j=1}^{k-1} \lambda^F_j(h) (1-\rho)\rho^{k-j-1}h(j+1, k)
    \ ,
    \\
    \lambda^B_i(h)
    &=
    \rho^{n-i} h(i, n)
    +
    \sum_{j=i+1}^{n} \lambda^B_j(h) (1-\rho)\rho^{j-i-1}h(i, j-1)
    \ ,
  \end{aligned}
\end{equation*}
so that, for instance,
\begin{equation*}
  p_{\Xb_{1:n}}(\xb_{1:n}) = \lambda^F_n(h_p) = \lambda^B_1(h_p)
  \ ,
  \quad
  \text{with}
  \quad
  h_p(i,k)
  =
  \frac{
    \Beta\left(\alphab_i + \sum_{j=i}^k \cb(\xb_j)\right)
  }{
      \Beta(\alphab_i)
  }
  \ 
  ,
\end{equation*}
where $\cb(\xb_j)$ is the count statistic of $\{\Xb_j = \xb_j\}$. Our
main interest however is to compute
\begin{equation*}
  \int_{\Omega}
  f(\thetab_j)
  \mu_{\Thetab_j \given \Xb_{1:n}}(d\thetab_j \given \xb_{1:n})
\end{equation*}
for some $f \in L^1$. We define
\begin{multline*}
  \lambda^{FB}_j(h)
  =
  \rho^{n-1}h(1, n)
  +
  \sum_{i=1}^{j-1}(1-\rho)\rho^{n-i-1}\lambda^F_i(h_p) h(i+1, n)
  \\
  +
  \sum_{k=j+1}^n (1-\rho)
  \left[
    \rho^{k-2} h(1, k-1)
    +
    \sum_{i=1}^{j-1} (1-\rho) \rho^{k-i-2}
    \lambda^F_i(h_p) h(i+1, k-1)
  \right]
  \lambda^B_k(h_p)
\end{multline*}
with
\begin{equation*}
  h(i,k)
  =
  \frac{1}{\Beta(\alphab_i)}
  \int_{\Omega}
  f(\thetab)
  \left[
    \prod_{j=i}^k
    \prod_{x\in\Ac}
    \theta_x^{c_x(\xb_j)}
  \right]
  \prod_{x\in\Ac}
  \theta_x^{\alpha_x-1}
  d\thetab
\end{equation*}
so that
\begin{equation*}
  \int_{\Omega}
  f(\thetab_j)
  \mu_{\Thetab_j \given \Xb_{1:n}}(d\thetab_j \given \xb_{1:n})
  =
  \lambda^{FB}_j(h)
  \ .
\end{equation*}

For instance, with $f(\thetab) = \thetab$ we obtain the marginal
posterior expectation with
\begin{equation*}
  h(i,k) = h_e(i,k)
  =
  h_p(i,k)
  \frac{
    \alphab_i + \sum_{j=i}^k \nb_j
  }{
    \sum_{x}
    \alpha_{i,x} + \sum_{j=i}^k c_x(\xb_j)
  }
  \ .
\end{equation*}
The utility for an event $x \in \Ac$ can be computed with $f(\thetab)
= \theta_x \ln \theta_x$ so that
\begin{equation*}
  h(i,k) = h^x_u(i,k)
  =
  h_p(i,k)
  \left(
  \psi\left(\alpha_{i,x} + \sum_{j=i}^k c_x(\xb_j)\right)
  -
  \psi\left(\sum_{y\in \Ac} \alpha_{i,y} + \sum_{j=i}^k c_y(\xb_j)\right)
  \right)
  \ .
\end{equation*}
