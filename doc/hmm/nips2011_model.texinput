% -*- mode: latex -*-
%
\section*{Motivation}
The learning problem we are considering is similar to an $L$-armed
bandit problem, where you are repeatedly faced with a choice among $L$
different actions. After each action an immediate reward is received
that depends on the action you have chosen. The objective is then to
maximize the sum of rewards over a period of $N$ trials. In the setup
we are considering, our goal is not to maximize the sum of immediate
rewards, but to learn as much as possible about a stimulus-response
relation during an experiment. For instance \cite{Bellman1956}
considered this problem for $L=2$ and independent arms.

% Efficient adaptive sampling methods rely on well-justified smoothing
% methods that allow sharing of statistical strength across neighboring
% stimuli. The sampling method then focuses on regions where no sharing
% strength is possible and that are more informative.

\section*{Model}
%
\tikzstyle{rv}=        [shape=circle,    minimum size=1.08cm, fill=blue!10]
\tikzstyle{rvback}=    [shape=circle,    minimum size=1.08cm, fill=blue!5]
\tikzstyle{empty}=     [shape=circle,    minimum size=1.08cm]
\tikzstyle{parameter}= [shape=rectangle, minimum size=1.08cm]
\begin{figure}[htbp]
  \begin{center}
    \begin{tikzpicture}[]
      % states
      \node[parameter] (A1) at (0,4) {$\alpha_{j-1}$}
      ;
      \node[parameter] (A2) at (2,4) {$\alpha_{j}$}
      ;
      \node[parameter] (A3) at (4,4) {$\alpha_{j+1}$}
      ;
      \node[parameter] (A4) at (6,4) {$\alpha_{j+2}$}
      ;
      % theta 2
      \node[empty] (S0) at (-1.5,2.5) {}
      ;
      \node[rvback] (S1) at (0.5,2.5) {$\Thetab^2_{j-1}$}
        edge [->] (S0)
        edge [<-] (A1)
      ;
      \node[rvback] (S2) at (2.5,2.5) {$\Thetab^2_{j}$}
        edge [->] (S1)
        edge [<-] (A2)
      ;
      \node[rvback] (S3) at (4.5,2.5) {$\Thetab^2_{j+1}$}
        edge [->] (S2)
        edge [<-] (A3)
      ;
      \node[rvback] (S4) at (6.5,2.5) {$\Thetab^2_{j+2}$}
        edge [->] (S3)
        edge [<-] (A4)
      ;
      \node[empty] (S5) at (8.5,2.5) {}
        edge [->] (S4)
      ;
      % theta 1
      \node[empty] (T0) at (-2,2) {}
      ;
      \node[rv] (T1) at (-0.25,2) {$\Thetab^1_{j-1}$}
        edge [<-] (T0)
        edge [<-] (A1)
      ;
      \node[rv] (T2) at (1.75,2) {$\Thetab^1_{j}$}
        edge [<-] (T1)
        edge [<-] (A2)
      ;
      \node[rv] (T3) at (3.75,2) {$\Thetab^1_{j+1}$}
        edge [<-] (T2)
        edge [<-] (A3)
      ;
      \node[rv] (T4) at (5.75,2) {$\Thetab^1_{j+2}$}
        edge [<-] (T3)
        edge [<-] (A4)
      ;
      \node[empty] (T5) at (7.75,2) {}
        edge [<-] (T4)
      ;
      % observations
      \node[rv] (X1) at (0,0) {$\Zb_{j-1}$}
        edge [<-] (T1)
        edge [<-] (S1)
      ;
      \node[rv] (X2) at (2,0) {$\Zb_{j}$}
        edge [<-] (T2)
        edge [<-] (S2)
      ;
      \node[rv] (X3) at (4,0) {$\Zb_{j+1}$}
        edge [<-] (T3)
        edge [<-] (S3)
      ;
      \node[rv] (X4) at (6,0) {$\Zb_{j+2}$}
        edge [<-] (T4)
        edge [<-] (S4)
      ;
    \end{tikzpicture}
  \end{center}
  \caption{
    \Polya urn hidden Markov process. The generative process can be
    sketched as follows: For the chain $M_1$ we decide at each
    position $j$ whether or not the random variable $\Thetab^1_j$
    shares its event with its predecessor $\Thetab^2_{j-1}$. It shares
    the same event with probability $\rho$, whereas with probability
    $1-\rho$ a new value is drawn from a Dirichlet distribution with
    parameters $\alphab_j$. For $M_2$ we do the same in reverse
    direction. Emissions $\Zb_j = (Z_{j,1}, Z_{j,2}, \dots, Z_{j,m})$
    are then drawn i.i.d. from the mixture $\Thetab_j = \gamma
    \Thetab^1_j + (1-\gamma) \Thetab^2_j$.
  }
  \label{fig:1}
\end{figure}
The model consists of two hidden Markov chains $M^1 =
\{\Thetab^1_j\}_{j\in\Xc}$ and $M^2 = \{\Thetab^2_j\}_{j\in\Xc}$ with
$\Xc = \{ 1, 2, \dots, L \}$ on a probability space $(\Omega,
\Bc(\Omega), P)$. The transition kernels $\mu^1_T, \mu^2_T:
\Bc(\Omega) \times \Omega \rightarrow \mathbb{R}^+$ with
\begin{equation*}
  P(\Theta^1_{j} \in A \given \Theta^1_{j-1} = \thetab_{j-1})
  =
  \mu^1_T(A \given \thetab_{j-1})
  \ ,
  \quad
  P(\Theta^2_{j} \in A \given \Theta^2_{j+1} = \thetab_{j+1})
  =
  \mu^2_T(A \given \thetab_{j+1})
  \ ,
\end{equation*}
are defined as
\begin{equation*}
  \begin{aligned}
    \mu^1_T(d\thetab_j \given \thetab_{j-1})
    &=
    \rho
    \delta_{\thetab_{j-1}}(d\thetab_j)
    +
    (1-\rho)
    \mu_{\Thetab^1_j}(d\thetab_j)
    \ ,
    \\
    \mu^2_T(d\thetab_j \given \thetab_{j+1})
    &=
    \rho
    \delta_{\thetab_{j+1}}(d\thetab_j)
    +
    (1-\rho)
    \mu_{\Thetab^2_j}(d\thetab_j)
    \ ,
  \end{aligned}
\end{equation*}
where $\mu_{\Thetab^1_j}$ and $\mu_{\Thetab^2_j}$ are Dirichlet
distributions on $\Omega$ with parameters $\alphab_j$ and $\rho \in
[0,1]$ is the cohesion parameter. We also define the auxiliary random
variables $\Thetab_j = \gamma \Thetab^1_j + (1-\gamma)\Thetab^2_j$
with mixture parameter $\gamma \in [0, 1]$. At each position $j \in
\Xc$ we have $m = m(j)$ emissions $\Zb_j = (Z_{j,m})_m$ with
distribution
\begin{equation*}
  Z_{j,m}
  \given
  \Thetab_j = \thetab_j
  \sim 
  \Discrete(\thetab_j)
  \ .
\end{equation*}
Each $Z_{j,m}$ takes values in some finite alphabet $\Ac$ such that
$\Omega = \Delta_{|\Ac|}$, the $|\Ac|$-dimensional probability
simplex. The full model is depicted in Figure~\ref{fig:1}.  With the
transition kernel $\mu_T$ we obtain a posterior distribution that can
be written as a sum over all consecutive partitions of $\Xc$. An
element of a consecutive partition consists of consecutive
integers. We denote the set of all such partitions as $\Pc(\Xc)$. For
instance, the marginal likelihood $p_{\Zb_{1:L}}(\zb_{1:L})$ can be
written as
\begin{equation*}
  \sum_{P\in \Pc(\Xc)}
  \rho^{L-|P|}
  (1-\rho)^{|P|-1}
  \left[
    \gamma
    \prod_{B\in P}
    \frac{
      \Beta\left(\alphab_{\min(B)} + \sum_{j \in B} \cb(\zb_j)\right)
    }{
      \Beta\left(\alphab_{\min(B)}\right)
    }
    +
    (1-\gamma)
    \prod_{B\in P}
    \frac{
      \Beta\left(\alphab_{\max(B)} + \sum_{j \in B} \cb(\zb_j)\right)
    }{
      \Beta\left(\alphab_{\max(B)}\right)
    }
  \right]
  \ .
\end{equation*}
Our main interest is to compute
\begin{equation*}
  \mu_{\Thetab_j \given \Zb_{1:L}}
  f
  =
  \int_{\Omega}
  f(\thetab)
  \mu_{\Thetab_j \given \Zb_{1:L}}(d\thetab \given \zb_{1:L})
  =
  \int_{\Omega^L}
  f(\thetab_j)
  \prod_{i=1}^L
  p_{\Zb_{i}\given \Thetab_{i}}(\zb_{i} \given \thetab_{i})
  \mu_T(d\thetab_{i} \given \thetab_{i-1})
  \ .
\end{equation*}
for some $f \in L^1$ and $j \in \Xc$. An efficient way of computing
posterior quantities can be derived by considering the
forward-backward decomposition of the posterior distribution that is
possible in hidden Markov models. The measure
\begin{equation*}
  \mu_{\Thetab^1_j \given \Zb_{1:L}}(d\thetab_j \given \zb_{1:L})
  \propto
  \mu_{\Thetab^1_j \given \Zb_{1:j}}(d\thetab_{j} \given \zb_{1:j})
  p_{\Zb_{j+1:L} \given \Thetab^1_j}(\zb_{j+1:L} \given \thetab_j)
  \ ,
\end{equation*}
consists of the forward measure
\begin{multline*}
  \mu_{\Thetab^1_{j+1} \given \Zb_{1:j+1}}(d\thetab_{j+1} \given \zb_{1:j+1})
  \\
  \begin{aligned}
    &=
    \frac{1}{p_{\Zb_{1:j+1}}(\zb_{1:j+1})}
    \int_{\Omega}
    p_{\Zb_{j+1}\given \Thetab^1_{j+1}}(\zb_{j+1} \given \thetab_{j+1})
    \mu_T(d\thetab_{j+1} \given \thetab_j)
    \mu_{\Thetab^1_{j} \given \Zb_{1:j}}(d\thetab_{j} \given \zb_{1:j})
    \\
    &=
    \frac{\rho}{p_{\Zb_{1:j+1}}(\zb_{1:j+1})}
    p_{\Zb_{j+1}\given \Thetab^1_{j+1}}(\zb_{j+1} \given \thetab_{j+1})
    \mu_{\Thetab^1_{j} \given \Zb_{1:j}}(d\thetab_{j+1} \given \zb_{1:j})
    +
    (1-\rho)
    \mu_{\Thetab^1_{j+1}}(d\thetab_{j+1})
    \ ,
  \end{aligned}
\end{multline*}
with $\mu_{\Thetab^1_1 \given \Zb_{1:1}}(d\thetab_{1} \given \zb_{1:1})
= p_{\Zb_{1}\given \Thetab^1_1}(\zb_{1} \given \thetab_{1})
\mu_{\Thetab^1_1}(d\thetab_{1})$ and backward probabilities
\begin{multline*}
  \begin{aligned}
    p_{\Zb_{j+1:L} \given \Thetab^1_j}(\zb_{j+1:L} \given \thetab_j)
    &=
    \int_{\Omega}
    p_{\Zb_{j+1}\given \Thetab^1_{j+1}}(\zb_{j+1} \given \thetab_{j+1})
    p_{\Zb_{j+2:L} \given \Thetab^1_{j+1}}(\zb_{j+2:L} \given \thetab_{j+1})
    \mu^1_T(d\thetab_{j+1} \given \thetab_j)
    \\
    &=
    \rho
    p_{\Zb_{j+1}\given \Thetab^1_{j+1}}(\zb_{j+1} \given \thetab_{j})
    f_{\Thetab^1_{j+1}}(\thetab_{j})
    p_{\Zb_{j+2:L} \given \Thetab^1_{j+1}}(\zb_{j+2:L} \given \thetab_{j})
    \\
    &\quad+
    (1-\rho)
    \int_{\Omega}
    p_{\Zb_{j+1}\given \Thetab^1_{j+1}}(\zb_{j+1} \given \thetab_{j+1})
    p_{\Zb_{j+2:L} \given \Thetab^1_{j+1}}(\zb_{j+2:L} \given \thetab_{j+1})
    \mu_{\Thetab^1_{j+1}}(d\thetab_{j+1})
    \ .
  \end{aligned}
\end{multline*}
Both, the forward measure and the backward probabilities can be
computed in $\Oc(L^2)$.

\section*{Active learning in sequential experiments}
\begin{figure}[t]
  \centering
  \includegraphics[width=0.9\textwidth,page=1]{example/decision-tree.pdf}
  \caption{
    Sketch of the decision tree for an experiment with two stimuli
    $\Xc = \{l, r\}$ and dichotomous outcome $\Ac = \{s, f\}$. The tree
    shows all experimental outcomes for $N = 2$.  Circles represent
    decision nodes at which we have to decide for a stimulus $x \in
    \Xc$. Chance nodes are shown as rectangles at which an experimental
    outcome is obtained. A possible sampling policy $\pi$ is depicted as
    arrows that origin from the decision nodes. The policy depends on the
    count statistic of the previous experiments. An immediate utility
    $u_1$ is received whenever one of the decision nodes is reached.
  }
  \label{fig:4}
\end{figure}
To formalize the sequential sampling process we introduce the random
variables $Y_n = Z_{j,m}$ with $m = \sum_{i=1}^n \1(X_i = j)$ at the
$n$th sampling step. That is, $X_n$ gives the stimulus for the $n$th
sample and $Y_n$ the response. Furthermore, we summarize the first $n$
experiments $(X_i, Y_i)_{i=1}^n$ as $\Eb_{1:n}$. The adaptive
sequential sampling process can be interpreted as a \emph{Markov
decision process} with state space $\Sc = \bigcup_{n=1}^N \Sc_n$,
where each $\Sc_n$ consists of the count statistics $\cb(\eb_{1:n})$
of all experimental outcomes $\eb_{1:n}$. A \emph{sampling policy}
$\pi : \Sc \rightarrow \Xc$ maps each state $\cb(\eb_{1:n})$ to a
stimulus $x_{n+1}$ (cf. Figure~\ref{fig:4}). Let's assume that we want
to take $N$ measurements in total and we have already taken $n$
samples. We define a utility measure $u_p$ that quantifies how much we
value a sequence of $p = N-n$ measurements $\xb_{n+1:n+p} = \{
x_{n+1}, x_{n+2}, \dots, x_{n+p}\}$ with responses $\yb_{n+1:n+p} = \{
y_{n+1}, y_{n+2}, \dots, y_{n+p}\}$.  Furthermore, let the next $p$
stimuli $\Xb_{n+1:n+p}$ be determined by the policy $\pi$, for which
we use the short notation $\Xb_{n+1:n+p} = \pi$. We define the
\emph{expected utility}
\begin{multline*}
  U_p(\Xb_{n+1:n+p} =  \pi \given \Eb_{1:n} = \eb_{1:n})
  \\
  =
  \E_{\Yb_{n+1:n+p}}[
    u_p(\Xb_{n+1:n+p} = \pi, \Yb_{n+1:n+p} \given \Eb_{1:n} = \eb_{1:n})
    \given
    \Xb_{n+1:n+p} = \pi,
    \Eb_{1:n} = \eb_{1:n}
  ]
  \ .
\end{multline*}
(cf. \cite{DeGroot1962}). In principle, for an experiment with $N$
measurements our goal would be to compute an optimal policy
\begin{equation*}
  \pi^*
  =
  \argmax_{\pi}
  U_N(\Xb_{1:N} = \pi)
  \ ,
\end{equation*}
that maximizes the expected utility for the entire experiment. Since
the state space $\Sc$ grows exponentially with $N$ it becomes quickly
impossible to determine an optimal policy. We therefore need to rely
on a \emph{myopic approximation} of the expected utility. Suppose that
we have already taken $n$ samples, then
\begin{equation*}
  \max_{\pi}
  U_{N-n-1}(\Xb_{n+1:N} = \pi \given \Eb_{1:n} = \eb_{1:n})
  =
  \max_{\pi}
  U_p(\Xb_{n+1:n+p} = \pi \given \Eb_{1:n} = \eb_{1:n})
  +
  \epsilon
\end{equation*}
for large enough $p < N-n-1$. Our goal is then to select a stimulus
$x^*_{n+1} = \pi^*(\eb_{1:n})$ for the next measurement $X_{n+1}$ that
leads to a maximal expected utility in the next $p$ experiments, i.e.
\begin{equation*}
  \pi^*
  =
  \argmax_{\pi}
  U_p(\Xb_{n+1:n+p} = \pi \given \Eb_{1:n} = \eb_{1:n})
  \ .
\end{equation*}
The optimal policy $\pi^*$ can be computed recursively with backward
induction. Suppose we have taken $n$ measurements. The \emph{optimal
expected utility} for an experiment $E_{n+1} = e_{n+1}$ given that we
follow an optimal policy $\pi^*$ in the next $p-1$ experiments is
denoted $U^*_p(E_{n+1} = e_{n+1} \given \Eb_{1:n} = \eb_{1:n})$ and
can be computed as
\begin{equation*}
  \max_{x_{n+2} \in \Xc}
  \E_{Y_{n+2}}[
    U^*_{p-1}(
      \Eb_{n+1:n+2} = \eb_{n+1:n+2}
      \given
      \Eb_{1:n} = \eb_{1:n}
    )
    \given
    X_{n+2} = x_{n+2}, \Eb_{1:n+1} = \eb_{1:n+1}
  ]
  \ .
\end{equation*}
This leads to the recursive definition
\begin{multline*}
  U^*_{p-k+1}(\Eb_{n+1:n+k} = \eb_{n+1:n+k} \given \Eb_{1:n} = \eb_{1:n})
  \\
  =
  \max_{x_{n+k+1} \in \Xc}
  \E_{Y_{n+k+1}}[
    U^*_{p-k}(
      \Eb_{n+1:n+k+1} = \eb_{n+1:n+k+1}
      \given
      \Eb_{1:n} = \eb_{1:n}
    )
    \given
    X_{n+k+1} = x_{n+k+1}, \Eb_{1:n+k} = \eb_{1:n+k}
  ]
  \ ,
\end{multline*}
for $k \in \{1, 2, \dots, p+1 \}$, where
\begin{equation*}
  U^*_0(\Eb_{n+1:n+p} = \eb_{n+1:n+p} \given \Eb_{1:n} = \eb_{1:n})
  =
  u_p(\Eb_{n+1:n+p} = \eb_{n+1:n+p} \given \Eb_{1:n} = \eb_{1:n}) \ .
\end{equation*}
We obtain the optimal stimulus $x^*_{n+1}$ from
\begin{equation*}
  x^*_{n+1}
  =
  \argmax_{x_{n+1}\in \Xc}
  \E_{Y_{n+1}}[U^*_p(E_{n+1} = e_{n+1} \given \Eb_{1:n} = \eb_{1:n}) \given X_{n+1} = x_{n+1}, \Eb_{1:n} = \eb_{1:n}]
  \ .
\end{equation*}

\section*{Active learning with hidden Markov models}
%
We first consider the case of one-step look-ahead. A suitable utility
measure $u_1(X_{n+1} = x,  Y_{n+1} = y \given \Eb_{1:n} = \eb_{1:n})$ for a
stimulus $x$ with response $y$ is given by the Kullback-Leibler
divergence between the measures $\mu_{\Thetab_{1:L} \given \Eb_{1:n+1}}$ and 
$\mu_{\Thetab_{1:L} \given \Eb_{1:n}}$, so that the expected utility
\begin{multline*}
  U_1(X_{n+1} = x \given \Eb_{1:n} = \eb_{1:n})
  \\
  \begin{aligned}
    &=
    \E_{Y_{n+1}}\left[
      \Dkl\left(
      \mu_{\Thetab_{1:L} \given \Eb_{1:n}, X_{n+1}, Y_{n+1}}(\cdot \given \eb_{1:n}, x, y)
      \ggiven
      \mu_{\Thetab_{1:L} \given \Eb_{1:n}}(\cdot \given \eb_{1:n})
      \right)
      \given
      \Eb_{1:n} = \eb_{1:n}
    \right]
    \\
    &=
    \I(\Thetab_{1:L} ; Y_{n+1} \given X_{n+1} = x, \Eb_{1:n} = \eb_{1:n})
    \ ,
  \end{aligned}
\end{multline*}
is given by the mutual information between $\Thetab_{1:L}$ and
$Y_{n+1}$ given $X_{n+1} = x$ and our previous observations
$\{\Eb_{1:n} = \eb_{1:n} \}$. We obtain
\begin{multline*}
  \Dkl\left(
  \mu_{\Thetab_{1:L} \given \Eb_{1:n}, X_{n+1}, Y_{n+1}}(\cdot \given \eb_{1:n}, x, y)
  \ggiven
  \mu_{\Thetab_{1:L} \given \Eb_{1:n}}(\cdot \given \eb_{1:n})
  \right)
  \\
  \begin{aligned}
    &=
    \int_{\Omega^L}
    \ln
    \frac{
      \mu_{\Thetab_{1:L} \given \Eb_{1:n}, X_{n+1}, Y_{n+1}}(\cdot \given \eb_{1:n}, x, y)
    }{
      \mu_{\Thetab_{1:L} \given \Eb_{1:n}}(\cdot \given \eb_{1:n})
    }
    \mu_{\Thetab_{1:L} \given \Eb_{1:n}, X_{n+1}, Y_{n+1}}(d\thetab_{1:L} \given \eb_{1:n}, x, y)
    \ ,
  \end{aligned}
\end{multline*}
where
\begin{equation*}
    \frac{
      \mu_{\Thetab_{1:L} \given \Eb_{1:n}, X_{n+1}, Y_{n+1}}(\cdot \given \eb_{1:n}, x, y)
    }{
      \mu_{\Thetab_{1:L} \given \Eb_{1:n}}(\cdot \given \eb_{1:n})
    }
  =
  \frac{1}{p_{Y_{n+1} \given X_{n+1}, \Eb_{1:n}}(y \given x, \eb_{1:n})}
  \theta_{x,y}
\end{equation*}
is the Radon-Nykod{\'y}m derivative of $\mu_{\Thetab_{1:L} \given
  \Eb_{1:n+1}}$ with respect to $\mu_{\Thetab_{1:L} \given
  \Eb_{1:n}}$. Therefore, we obtain
\begin{multline*}
  \Dkl\left(
  \mu_{\Thetab_{1:L} \given \Eb_{1:n}, X_{n+1}, Y_{n+1}}(\cdot \given \eb_{1:n}, x, y)
  \ggiven
  \mu_{\Thetab_{1:L} \given \Eb_{1:n}}(\cdot \given \eb_{1:n})
  \right)
  \\
  \begin{aligned}
    &=
    \frac{
      1
    }{
      p_{Y_{n+1} \given X_{n+1}, \Eb_{1:n}}(y \given x, \eb_{1:n})
    }
    \int_{\Omega}
    \theta_{x,y} \ln(\theta_{x,y})
    \mu_{\Thetab_x \given \Eb_{1:n}}(d\thetab_x \given \eb_{1:n})
    -
    \ln
    p_{Y_{n+1} \given X_{n+1}, \Eb_{1:n}}(y \given x, \eb_{1:n})
    \ ,
  \end{aligned}
\end{multline*}
where we used
\begin{equation*}
  \int_{\Omega^L}
  \theta_{x,y} \ln(\theta_{x,y})
  \prod_{i=1}^L
  p_{\Zb_i \given \Thetab_i}(\zb_i \given \thetab_i)
  \mu_T(d\thetab_i \given \thetab_{i-1})
  =
  \int_{\Omega}
  \theta_{x,y} \ln(\theta_{x,y})
  \mu_{\Thetab_x \given \Eb_{1:n}}(d\thetab_x \given \eb_{1:n})
  \ .
\end{equation*}
We obtain the expected utility
\begin{equation*}
  U_1(X_{n+1} = x \given \Eb_{1:n} = \eb_{1:n})
  =
  \Entropy\left[Y_{n+1} \given X_{n+1} = x, \Eb_{1:n} = \eb_{1:n}\right]
  +
  \sum_{y\in\Ac}
  \int_{\Omega}
  \theta_{x,y} \ln(\theta_{x,y})
  \mu_{\Thetab_x \given \Eb_{1:n}}(d\thetab_x \given \eb_{1:n})
  \ .
\end{equation*}
Similarly, for the next $p$ experiments we obtain the expected utility
\begin{multline*}
  U_p(\Xb_{n+1:n+p} = \pi \given \Eb_{1:n} = \eb_{1:n})
  \\
  \begin{aligned}
    &=
    U_1(X_{n+1} = x_{n+1} \given \Eb_{1:n} = \eb_{1:n})
    \\
    &\quad
    +
    \sum_{y \in\Ac}
    p_{Y_{n+1} \given X_{n+1}, \Eb_{1:n}} (y \given x_{n+1}, \eb_{1:n})
    U_{p-1}(\Xb_{n+2:n+p} = \pi \given \Eb_{1:n+1} = \eb_{1:n+1})
    \\
    &=
    \sum_{y \in\Ac}
    p_{Y_{n+1} \given X_{n+1}, \Eb_{1:n}} (y \given x_{n+1}, \eb_{1:n})
    \\
    &\quad\quad\quad
    \left[
    u_1(E_{n+1} = e_{n+1} \given \Eb_{1:n} = \eb_{1:n})
    +
    U_{p-1}(\Xb_{n+2:n+p} = \pi \given \Eb_{1:n+1} = \eb_{1:n+1})
    \right]
    \ ,
  \end{aligned}
\end{multline*}
where $x_{n+1} = \pi(\eb_{1:n})$. This shows that the expected utility
for a policy $\pi$ considered by \cite{DeGroot1962} can be written as
a reinforcement learning problem as defined by \cite{Bellman1957a,
Bellman1957b}), where $U_p$ is called the \emph{value function}
(cf. \cite{Sutton1998}). Note also that the discount factor for future
utilities is here implicit in the utility function. That is, if we
have an optimal policy $\pi^*$ then the expected utility of future
experiments should always be less or equal to the expected immediate
utility. This also justifies the use of myopic sampling strategies.

\section*{Submodular utility functions}
%
For a sequence of experiments $\{ \Eb_{1:n} = \eb_{1:n} \}$ a
\emph{submodular set function} satisfies
\begin{multline*}
  u_{m+1}(E_{n+1} = e_{n+1}, \Eb_{1:m} = \eb_{1:m})
  - 
  u_{m}(\Eb_{1:m} = \eb_{1:m})
  \\
  \ge
  u_{n+1}(E_{n+1} = e_{n+1}, \Eb_{1:n} = \eb_{1:n})
  - 
  u_{n}(\Eb_{1:n} = \eb_{1:n})
  \ ,
\end{multline*}
where $m \le n$, which is for the Kullback-Leibler divergence
equivalent to
\begin{equation*}
  u_{m+1}(E_{n+1} = e_{n+1} \given \Eb_{1:m} = \eb_{1:m})
  \ge
  u_{n+1}(E_{n+1} = e_{n+1} \given \Eb_{1:n} = \eb_{1:n})
  \ .
\end{equation*}
In other words, we can learn more from an experiment $\{ E_{n+1} =
e_{n+1} \}$ if we have less prior kowledge. See \cite{Nemhauser1978}
for the maximization of approximated submodular set functions.

\section*{Computation}
%
By solving the recursive definitions of the forward measures and
backward probabilities we obtain the same computational structure as
in \cite{Yao1984} or \cite{Barry1992}. We discuss the computation for
the Markov chain $M_1$. For integers $1 \le i \le j \le k \le L$ we
define
\begin{equation*}
  \begin{aligned}
    \lambda^F_k(h)
    &=
    \rho^{k-1} h(1, k)
    +
    \sum_{j=1}^{k-1} \lambda^F_j(h) (1-\rho)\rho^{k-j-1}h(j+1, k)
    \ ,
    \\
    \lambda^B_i(h)
    &=
    \rho^{L-i} h(i, L)
    +
    \sum_{j=i+1}^{L} \lambda^B_j(h) (1-\rho)\rho^{j-i-1}h(i, j-1)
    \ ,
  \end{aligned}
\end{equation*}
so that, for instance, $p_{\Zb_{1:L}}(\zb_{1:L}) = \lambda^F_L(h_p) =
\lambda^B_1(h_p)$ with
\begin{equation*}
  h_p(i,k)
  =
  \frac{
    \Beta\left(\alphab_i + \sum_{j=i}^k \cb(\zb_j)\right)
  }{
    \Beta(\alphab_i)
  }
  \ ,
\end{equation*}
where $\cb(\zb_j)$ is the count statistic of $\{\Zb_j = \zb_j\}$.
To compute $\mu_{\Thetab_j \given \Zb_{1:L}} f$ for some $f \in L^1$
we define
\begin{multline*}
  \lambda^{FB}_j(h)
  =
  \rho^{L-1}h(1, L)
  +
  \sum_{i=1}^{j-1}(1-\rho)\rho^{L-i-1}\lambda^F_i(h_p) h(i+1, L)
  \\
  +
  \sum_{k=j+1}^L (1-\rho)
  \left[
    \rho^{k-2} h(1, k-1)
    +
    \sum_{i=1}^{j-1} (1-\rho) \rho^{k-i-2}
    \lambda^F_i(h_p) h(i+1, k-1)
  \right]
  \lambda^B_k(h_p)
\end{multline*}
with
\begin{equation*}
  h(i,k)
  =
  \int_{\Omega}
  f(\thetab)
  \prod_{j=i}^k
  p_{\Zb_j \given \Thetab_j}(\zb_j \given \thetab)
  \mu_{\Thetab_j}(d\thetab)
\end{equation*}
so that
\begin{equation*}
  \mu_{\Thetab_j \given \Zb_{1:L}} f
  =
  \frac{
    \lambda^{FB}_j(h)
  }{
    \lambda^F_L(h_p)
  }
  \ .
\end{equation*}

For instance, the marginal posterior expectation $\mu_{\Thetab_j \given
  \Zb_{1:L}} \thetab$ can be obtained with
\begin{equation*}
  h(i,k)
  =
  \frac{h_e(i,k)}{h_p(i,k)}
  =
  \frac{
    \alphab_i+\sum_{j=i}^k\cb(\zb_j)
  }{
    \sum_{y\in\Ac}
    \alpha_{i,y}
    \sum_{j=i}^k c_y(\zb_j)
  }
  \ .
\end{equation*}
The utility for an event $y \in \Ac$ can be computed with $f(\thetab)
= \theta_y \ln \theta_y$ so that
\begin{equation*}
  \begin{aligned}
    h(i,k)
    =
    \frac{h^y_u(i,k)}{h_e(i,k)}
    &=
    \psi\left(1 + \alpha_{i,y} + \sum_{j=i}^k c_y(\zb_j)\right)
    -
    \psi\left(1 + \sum_{y\in \Ac} \alpha_{i,y} + \sum_{j=i}^k c_y(\zb_j)\right)
    \ .
  \end{aligned}
\end{equation*}

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.495\textwidth,page=2]{example/example1.pdf}
  \includegraphics[width=0.495\textwidth,page=3]{example/example1.pdf}\\
  \includegraphics[width=0.495\textwidth,page=4]{example/example1.pdf}
  \includegraphics[width=0.495\textwidth,page=5]{example/example1.pdf}\\
  \includegraphics[width=0.495\textwidth,page=6]{example/example1.pdf}
  \includegraphics[width=0.495\textwidth,page=7]{example/example1.pdf}
  \caption{
    Adaptive sampling in a hypothetical experiment with $\rho =
    0.8$. The figure shows the experiment after $1$, $2$, $10$, $50$,
    $100$, and $200$ samples. At first, samples are uniformly
    distributed. In (a) only one measurement was taken and the
    expected utility is largest at the left boundary. (d) shows the
    experiment after 50 samples where the algorithm starts to locate
    measurements at sloped regions. The general shape of the
    stimulus-response function is already well established after 100
    samples (e). Many measurements are also taken at $x=1$ and $x=35$
    since those $x$ have only one neighbor.
  }
  \label{fig:2}
\end{figure}

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.8\textwidth,page=1]{example/example2.pdf}\\
  \includegraphics[width=0.8\textwidth,page=4]{example/example2.pdf}
  \caption{
    $U^*_p(X_{n+1} = x \given \Eb_{1:n} = \eb_{1:n})$ for varying
    look-aheads $p$.
    (a) $U^*_p (X_1 = x)$ with no prior experiments.
    (b) $U^*_p (X_7 = x \given \Eb_{1:6} = \eb_{1:6})$ with six prior
    experiments.
    %$\eb_{1:6} = \{(1,1), (10, 1), (15, 2), (20, 2), (28, 1), (25, 1)\}.$
  }
  \label{fig:3}
\end{figure}

\section*{Appendix}
The derivation of the forward measure depends on the following
property of the dirac measure: Let $f,g \in L^1$ and let $\mu$ be some
measure. From
\begin{equation*}
  \int_{x\in\Omega}
  \int_{y\in\Omega}
  f(y)
  \delta_y(dx)
  \mu(dy)
  =
  \int_{y\in\Omega}
  f(y)
  \mu(dy)
  \int_{x\in\Omega}
  \delta_y(dx)
  =
  \int_{y\in\Omega}
  f(y)
  \mu(dy)
\end{equation*}
it follows that $\int_{y\in\Omega} f(y) \delta_y(dx) \mu(dy)$ can be
written as $f(x)\mu(dx)$.
