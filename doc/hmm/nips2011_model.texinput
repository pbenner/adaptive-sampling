% -*- mode: latex -*-
%
\section*{Model}
%
\tikzstyle{rv}=        [shape=circle,    minimum size=1.2cm]
\tikzstyle{parameter}= [shape=rectangle, minimum size=1.2cm]
\begin{figure}[htbp]
  \begin{center}
    \begin{tikzpicture}[]
      % states
      \node[parameter] (A1) at (0,4) {$\alpha_{j-1}$}
      ;
      \node[parameter] (A2) at (2,4) {$\alpha_{j}$}
      ;
      \node[parameter] (A3) at (4,4) {$\alpha_{j+1}$}
      ;
      \node[parameter] (A4) at (6,4) {$\alpha_{j+2}$}
      ;
      % theta 1
      \node[rv] (T0) at (-2,2) {}
      ;
      \node[rv] (T1) at (0.0,2) {$\Thetab_{j-1}$}
        edge [<-] (T0)
        edge [<-] (A1)
      ;
      \node[rv] (T2) at (2.0,2) {$\Thetab_{j}$}
        edge [<-] (T1)
        edge [<-] (A2)
      ;
      \node[rv] (T3) at (4.0,2) {$\Thetab_{j+1}$}
        edge [<-] (T2)
        edge [<-] (A3)
      ;
      \node[rv] (T4) at (6.0,2) {$\Thetab_{j+2}$}
        edge [<-] (T3)
        edge [<-] (A4)
      ;
      \node[rv] (T5) at (8.0,2) {}
        edge [<-] (T4)
      ;
      % observations
      \node[rv] (X1) at (0,0) {$\Xb_{j-1}$}
        edge [<-] (T1)
      ;
      \node[rv] (X2) at (2,0) {$\Xb_{j}$}
        edge [<-] (T2)
      ;
      \node[rv] (X3) at (4,0) {$\Xb_{j+1}$}
        edge [<-] (T3)
      ;
      \node[rv] (X4) at (6,0) {$\Xb_{j+2}$}
        edge [<-] (T4)
      ;
    \end{tikzpicture}
  \end{center}
  \caption{Polya urn hidden Markov process.}
  \label{fig:1}
\end{figure}
The model consists of hidden Markov chain $M = \{\Thetab_j\}_{j=1}^n$
on a probability space $(\Omega, \Bc(\Omega), P)$. The transition
kernel $\mu_T: \Bc(\Omega) \times \Omega \rightarrow \mathbb{R}^+$
with
\begin{equation*}
  P(\Theta_{j} \in A \given \Theta_{j-1} = \thetab_{j-1})
  =
  \mu_T(A \given \thetab_{j-1})
  \ ,
\end{equation*}
is defined as
\begin{equation*}
  \mu_T(d\thetab_j \given \thetab_{j-1})
  =
  f_{\Dir(\alphab_j)}(\thetab_{j-1})
  \left[
    \rho
    \delta_{\thetab_{j-1}}(d\thetab_j)
    +
    (1-\rho)
    d\thetab_j
  \right]
  \ ,
\end{equation*}
where $f_{\Dir(\alphab_j)}(\thetab)d\thetab =
\mu_{\Dir(\alphab_j)}(d\thetab)$ is the Dirichlet distribution on
$\Omega$ with parameters $\alphab_j$ and $\rho \in [0,1]$ the cohesion
parameter. At each position $j$ we have $m = m(j)$ emissions $\Xb_j =
(X_{j,m})_m$ obtained from a mixture of the two Markov chains, i.e.
\begin{equation*}
  X_{j,m}
  \given
  \Thetab_j = \thetab_j
  \sim 
  \Discrete(\thetab_j)
  \ .
\end{equation*}
Each $X_{j,m}$ takes values in some finite alphabet $\Ac$ such that
$\Omega = \Delta_{|\Ac|}$, the $|\Ac|$-dimensional probability
simplex. The full model is depicted in Figure~\ref{fig:1}. Our main
interest is to compute
\begin{equation*}
  \mu_{\Thetab_j \given \Xb_{1:n}}
  f
  =
  \int_{\Omega}
  f(\thetab)
  \mu_{\Thetab_j \given \Xb_{1:n}}(d\thetab \given \xb_{1:n})
  =
  \int_{\Omega^n}
  f(\thetab_j)
  \prod_{i=1}^n
  p_{\Xb_{i}\given \Thetab_{i}}(\xb_{i} \given \thetab_{i})
  \mu_T(d\thetab_{i} \given \thetab_{i-1})
  \ .
\end{equation*}
for some $f \in L^1$ and $j \in \{1, 2, \dots, n\}$.
With the transition kernel $\mu_T$ we obtain a posterior distribution
that can be written as a sum over all consecutive partitions of $\{1,
2, \dots, n\}$. An element of a consecutive partition consists of
consecutive integers. We denote the set of all such partitions as
$\Pc(1:n)$. For instance, the marginal likelihood
$p_{\Xb_{1:n}}(\xb_{1:n})$ can be written as
\begin{equation*}
  \sum_{P\in \Pc(1:n)}
  \rho^{n-|P|}
  (1-\rho)^{|P|-1}
  \prod_{B\in P}
  \frac{
    \Beta\left(1-|B|+\sum_{j \in B} \alphab_j + \cb(\xb_j)\right)
  }{
    \prod_{j \in B} \Beta\left(\alphab_j\right)
  }
  \ .
\end{equation*}
An efficient way of computing the posterior distribution can be
derived by considering the forward-backward algorithm of hidden Markov
models. The measure
\begin{equation*}
  \mu_{\Thetab_j \given \Xb_{1:n}}(d\thetab_j \given \xb_{1:n})
  \propto
  \mu_{\Thetab_{j} \given \Xb_{1:j}}(d\thetab_{j} \given \xb_{1:j})
  p_{\Xb_{j+1:n} \given \Thetab_j}(\xb_{j+1:n} \given \thetab_j)
  \ ,
\end{equation*}
consists of the forward measure
\begin{multline*}
  \mu_{\Thetab_{j+1} \given \Xb_{1:j+1}}(d\thetab_{j+1} \given \xb_{1:j+1})
  \\
  \begin{aligned}
    &=
    \frac{1}{p_{\Xb_{1:j+1}}(\xb_{1:j+1})}
    \int_{\Omega}
    p_{\Xb_{j+1}\given \Thetab_{j+1}}(\xb_{j+1} \given \thetab_{j+1})
    \mu_T(d\thetab_{j+1} \given \thetab_j)
    \mu_{\Thetab_{j} \given \Xb_{1:j}}(d\thetab_{j} \given \xb_{1:j})
    \\
    &=
    \frac{\rho}{p_{\Xb_{1:j+1}}(\xb_{1:j+1})}
    p_{\Xb_{j+1}\given \Thetab_{j+1}}(\xb_{j+1} \given \thetab_{j+1})
    f_{\Dir(\alphab_{j+1})}(\thetab_{j+1})
    \mu_{\Thetab_{j} \given \Xb_{1:j}}(d\thetab_{j+1} \given \xb_{1:j})
    \\
    &\quad+
    (1-\rho)
    \mu_{\Dir(\alphab_{j+1})}(d\thetab_{j+1})
    \ ,
  \end{aligned}
\end{multline*}
with $\mu_{\Thetab_{1} \given \Xb_{1:1}}(d\thetab_{1} \given \xb_{1:1})
= p_{\Xb_{1}\given \Thetab_{1}}(\xb_{1} \given \thetab_{1})
\mu_{\Dir(\alphab_{1})}(d\thetab_{1})$ and backward probabilities
\begin{multline*}
  \begin{aligned}
    p_{\Xb_{j+1:n} \given \Thetab_j}(\xb_{j+1:n} \given \thetab_j)
    &=
    \int_{\Omega}
    p_{\Xb_{j+1}\given \Thetab_{j+1}}(\xb_{j+1} \given \thetab_{j+1})
    p_{\Xb_{j+2:n} \given \Thetab_{j+1}}(\xb_{j+2:n} \given \thetab_{j+1})
    \mu_T(d\thetab_{j+1} \given \thetab_j)
    \\
    &=
    \rho
    p_{\Xb_{j+1}\given \Thetab_{j+1}}(\xb_{j+1} \given \thetab_{j})
    f_{\Dir(\alphab_{j+1})}(\thetab_{j})
    p_{\Xb_{j+2:n} \given \Thetab_{j+1}}(\xb_{j+2:n} \given \thetab_{j})
    \\
    &\quad+
    (1-\rho)
    \int_{\Omega}
    p_{\Xb_{j+1}\given \Thetab_{j+1}}(\xb_{j+1} \given \thetab_{j+1})
    p_{\Xb_{j+2:n} \given \Thetab_{j+1}}(\xb_{j+2:n} \given \thetab_{j+1})
    \mu_{\Dir(\alphab_{j+1})}(d\thetab_{j+1})
    \ .
  \end{aligned}
\end{multline*}
Both, the forward measure and the backward probabilities can be
computed in $\Oc(n^2)$.

\section*{Utility}
%
Assume that at position $j$ we already have $m$ samples. A suitable
utility measure $u(j, \xb_{1:n})$ for sampling at position $j$ is
defined by the mutual information between $\Thetab_{1:n}$ and $X_{j,m+1}$
given our previous observations $\{\Xb_{1:n} = \xb_{1:n} \}$, i.e.
\begin{equation*}
  \begin{aligned}
    u(j, \xb_{1:n})
    &=
    \I(\Thetab_{1:n} ; X_{j,m+1} \given \Xb_{1:n} = \xb_{1:n})
    \\
    &=
    \E_{X_{j,m+1}}\left[
      \Dkl\left(
      \mu_{\Thetab_{1:n} \given \Xb_{1:n}, X_{j,m+1}}(\cdot \given \xb_{1:n}, x)
      \ggiven
      \mu_{\Thetab_{1:n} \given \Xb_{1:n}}(\cdot \given \xb_{1:n})
      \right)
      \given
      \Xb_{1:n} = \xb_{1:n}
    \right]
    \ ,
  \end{aligned}
\end{equation*}
with
\begin{multline*}
  \Dkl\left(
  \mu_{\Thetab_{1:n} \given \Xb_{1:n}, X_{j,m+1}}(\cdot \given \xb_{1:n}, x)
  \ggiven
  \mu_{\Thetab_{1:n} \given \Xb_{1:n}}(\cdot \given \xb_{1:n})
  \right)
  \\
  \begin{aligned}
    &=
    \frac{
      1
    }{
      p_{\Xb_{1:j+1}, X_{j,m+1}}(\xb_{1:n}, x)
    }
    \int_{\Omega}
    \theta_{j,x} \ln(\theta_{j,x})
    \mu_{\Thetab_j \given \Xb_{1:n}}(d\thetab_j \given \xb_{1:n})
    -
    \ln
    p_{X_{j,m+1} \given \Xb_{1:n}}(x \given \xb_{1:n})
    \ ,
  \end{aligned}
\end{multline*}
where we used
\begin{equation*}
  \int_{\Omega^n}
  \theta_{j,x} \ln(\theta_{j,x})
  \prod_{i=1}^n
  p_{\Xb_{i}\given \Thetab_{i}}(\xb_{i} \given \thetab_{i})
  \mu_T(d\thetab_{i} \given \thetab_{i-1})
  =
  \int_{\Omega}
  \theta_{j,x} \ln(\theta_{j,x})
  \mu_{\Thetab_j \given \Xb_{1:n}}(d\thetab_j \given \xb_{1:n})
  \ .
\end{equation*}

\section*{Computation}
%
By solving the recursive definitions of the forward measures and
backward probabilities we obtain the same computational structure as
in \cite{Yao1984} or \cite{Barry1992}.
For $0 \le i \le j \le k \le n$ we define
\begin{equation*}
  \begin{aligned}
    \lambda^F_k(h)
    &=
    \rho^{k-1} h(1, k)
    +
    \sum_{j=1}^{k-1} \lambda^F_j(h) (1-\rho)\rho^{k-j-1}h(j+1, k)
    \ ,
    \\
    \lambda^B_i(h)
    &=
    \rho^{n-i} h(i, n)
    +
    \sum_{j=i+1}^{n} \lambda^B_j(h) (1-\rho)\rho^{j-i-1}h(i, j-1)
    \ ,
  \end{aligned}
\end{equation*}
so that, for instance, $p_{\Xb_{1:n}}(\xb_{1:n}) = \lambda^F_n(h_p) =
\lambda^B_1(h_p)$ with
\begin{equation*}
  h_p(i,k)
  =
  \frac{
    \Beta\left(i-k+\sum_{j=i}^k \alphab_j + \cb(\xb_j)\right)
  }{
    \prod_{j=i}^k \Beta(\alphab_j)
  }
  \ ,
\end{equation*}
where $\cb(\xb_j)$ is the count statistic of $\{\Xb_j = \xb_j\}$.
To compute $\mu_{\Thetab_j \given \Xb_{1:n}} f$ for some $f \in L^1$
we define
\begin{multline*}
  \lambda^{FB}_j(h)
  =
  \rho^{n-1}h(1, n)
  +
  \sum_{i=1}^{j-1}(1-\rho)\rho^{n-i-1}\lambda^F_i(h_p) h(i+1, n)
  \\
  +
  \sum_{k=j+1}^n (1-\rho)
  \left[
    \rho^{k-2} h(1, k-1)
    +
    \sum_{i=1}^{j-1} (1-\rho) \rho^{k-i-2}
    \lambda^F_i(h_p) h(i+1, k-1)
  \right]
  \lambda^B_k(h_p)
\end{multline*}
with
\begin{equation*}
  h(i,k)
  =
  \int_{\Omega}
  f(\thetab)
  \prod_{j=i}^k
  p_{\Xb_j \given \Thetab_j}(\xb_j \given \thetab)
  \mu_{\Dir(\alphab_j)}(d\thetab)
\end{equation*}
so that
\begin{equation*}
  \mu_{\Thetab_j \given \Xb_{1:n}} f
  =
  \frac{
    \lambda^{FB}_j(h)
  }{
    \lambda^F_n(h_p)
  }
  \ .
\end{equation*}

For instance, the marginal posterior expectation $\mu_{\Thetab_j \given
  \Xb_{1:n}} \thetab$ can be obtained with
\begin{equation*}
  h(i,k)
  =
  \frac{h_e(i,k)}{h_p(i,k)}
  =
  \frac{
    i-k+\sum_{j=i}^k\alphab_j + \cb(\xb_j)
  }{
    |\Ac|(i-k)+\sum_{y\in\Ac}
    \sum_{j=i}^k \alpha_{j,y} + c_y(\xb_j)
  }
  \ .
\end{equation*}
The utility for an event $x \in \Ac$ can be computed with $f(\thetab)
= \theta_x \ln \theta_x$ so that
\begin{equation*}
  \begin{aligned}
    h(i,k)
    =
    \frac{h^x_u(i,k)}{h_e(i,k)}
    &=
    \psi\left(i-k+1 + \sum_{j=i}^k \alpha_{j,x} + c_x(\xb_j)\right)
    -
    \psi\left(|\Ac|(i-k)+1 + \sum_{y\in \Ac} \sum_{j=i}^k \alpha_{j,y} + c_y(\xb_j)\right)
    \ .
  \end{aligned}
\end{equation*}

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.495\textwidth,page=2]{example/example1.pdf}
  \includegraphics[width=0.495\textwidth,page=3]{example/example1.pdf}\\
  \includegraphics[width=0.495\textwidth,page=4]{example/example1.pdf}
  \includegraphics[width=0.495\textwidth,page=5]{example/example1.pdf}\\
  \includegraphics[width=0.495\textwidth,page=6]{example/example1.pdf}
  \includegraphics[width=0.495\textwidth,page=7]{example/example1.pdf}
  \caption{
    Adaptive sampling in a hypothetical experiment with $\gamma = 0.5$
    and $\rho = 0.8$. The figure shows the experiment after $1$, $2$,
    $10$, $50$, $100$, and $200$ samples. At first, samples are
    uniformly distributed. In (a) only one measurement was taken and
    the expected utility is largest at the left boundary. (d) shows the
    experiment after 50 samples where the algorithm starts to locate
    measurements at sloped regions. The general shape of the
    stimulus-response function is already well established after 100
    samples (e). Many measurements are also taken at $x=1$ and $x=35$
    since those $x$ have only one neighbor.}
  \label{fig:05}
\end{figure}
