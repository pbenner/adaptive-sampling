% -*- mode: latex -*-
%
\section*{Model}
%
\tikzstyle{rv}=        [shape=circle,    fill=black!0,  minimum size=1.2cm]
\tikzstyle{parameter}= [shape=rectangle, fill=black!0,  minimum size=1.2cm]
\begin{figure}[htbp]
  \begin{center}
    \begin{tikzpicture}[]
      % states
      \node[parameter] (A1) at (0,4) {$\alpha_{i-1}$}
      ;
      \node[parameter] (A2) at (2,4) {$\alpha_{i}$}
      ;
      \node[parameter] (A3) at (4,4) {$\alpha_{i+1}$}
      ;
      \node[parameter] (A4) at (6,4) {$\alpha_{i+2}$}
      ;
      % theta
      \node[] (T0) at (-2,2) {}
      ;
      \node[rv] (T1) at (0,2) {$\Thetab_{i-1}$}
        edge [<-] (T0)
        edge [<-] (A1)
      ;
      \node[rv] (T2) at (2,2) {$\Thetab_{i}$}
        edge [<-] (T1)
        edge [<-] (A2)
      ;
      \node[rv] (T3) at (4,2) {$\Thetab_{i+1}$}
        edge [<-] (T2)
        edge [<-] (A3)
      ;
      \node[rv] (T4) at (6,2) {$\Thetab_{i+2}$}
        edge [<-] (T3)
        edge [<-] (A4)
      ;
      \node[] (T5) at (8,2) {}
        edge [<-] (T4)
      ;
      % observations
      \node[rv] (X1) at (0,0) {$\Xb_{i-1}$}
        edge [<-] (T1)
      ;
      \node[rv] (X2) at (2,0) {$\Xb_{i}$}
        edge [<-] (T2)
      ;
      \node[rv] (X3) at (4,0) {$\Xb_{i+1}$}
        edge [<-] (T3)
      ;
      \node[rv] (X4) at (6,0) {$\Xb_{i+2}$}
        edge [<-] (T4)
      ;
    \end{tikzpicture}
  \end{center}
  \caption{Polya urn hidden Markov process.}
  \label{fig:2}
\end{figure}

Define the transition kernel
\begin{equation*}
  \mu_T(d\theta_{j+1} \given \theta_j)
  =
  \rho
  \delta_{\theta_j}(d\theta_{j+1})
  +
  (1-\rho)
  \mu_{\Dir(\alpha_{j+1})}(d\theta_{j+1})
\end{equation*}
and multinomial emission probabilities $p_{X_{j}\given
  \Theta_{j}}(x_{j} \given \theta_{j})$.

The marginal posterior measure is given by
\begin{equation*}
  \mu_{\Theta_j \given \Xb_{1:n}}(d\theta_j \given x_{1:n})
  \propto
  \mu_{\Theta_{j} \given \Xb_{1:j}}(d\theta_{j} \given \xb_{1:j})
  p_{\Xb_{j+1:n} \given \Theta_j}(\xb_{j+1:n} \given \theta_j)
\end{equation*}

Forward measures
\begin{multline*}
  \mu_{\Theta_{j+1} \given \Xb_{1:j+1}}(d\theta_{j+1} \given \xb_{1:j+1})
  \\
  \begin{aligned}
    &=
    \frac{1}{p_{\Xb_{1:j+1}}(\xb_{1:j+1})}
    \int
    p_{X_{j+1}\given \Theta_{j+1}}(x_{j+1} \given \theta_{j+1})
    \mu_T(d\theta_{j+1} \given \theta_j)
    \mu_{\Theta_{j} \given \Xb_{1:j}}(d\theta_{j} \given \xb_{1:j})
    \\
    &=
    \frac{\rho}{p_{\Xb_{1:j+1}}(\xb_{1:j+1})}
    p_{X_{j+1}\given \Theta_{j+1}}(x_{j+1} \given \theta_{j+1})
    \mu_{\Theta_{j} \given \Xb_{1:j}}(d\theta_{j+1} \given \xb_{1:j})
    +
    (1-\rho)
    \mu_{\Dir(\alpha_{j+1})}(d\theta_{j+1})
  \end{aligned}
\end{multline*}
with $\mu_{\Theta_{1} \given \Xb_{1:1}}(d\theta_{1} \given \xb_{1:1})
= p_{X_{1}\given \Theta_{1}}(x_{1} \given \theta_{1})
\mu_{\Dir(\alpha_{1})}(d\theta_{1})$ and the backward probabilities
\begin{multline*}
  \begin{aligned}
    p_{\Xb_{j+1:n} \given \Theta_j}(\xb_{j+1:n} \given \theta_j)
    &=
    \int
    p_{X_{j+1}\given \Theta_{j+1}}(x_{j+1} \given \theta_{j+1})
    p_{\Xb_{j+2:n} \given \Theta_{j+1}}(\xb_{j+2:n} \given \theta_{j+1})
    \mu_T(d\theta_{j+1} \given \theta_j)
    \\
    &=
    \rho
    p_{X_{j+1}\given \Theta_{j+1}}(x_{j+1} \given \theta_{j})
    p_{\Xb_{j+2:n} \given \Theta_{j+1}}(\xb_{j+2:n} \given \theta_{j})
    \\
    &\quad+
    (1-\rho)
    \int
    p_{X_{j+1}\given \Theta_{j+1}}(x_{j+1} \given \theta_{j+1})
    p_{\Xb_{j+2:n} \given \Theta_{j+1}}(\xb_{j+2:n} \given \theta_{j+1})
    \mu_{\Dir(\alpha_{j+1})}(d\theta_{j+1})
  \end{aligned}
\end{multline*}

\section*{Utility}
%
Utility at position $j$ for response $l$
\begin{equation*}
  \begin{aligned}
    u(j, l, \xb_{1:n})
    &=
    \Dkl\left(
    \mu_{\Thetab_{1:n} \given \Xb_{1:n}}(\cdot \given \xb_{1:n}^{+(j,l)})
    \ggiven
    \mu_{\Thetab_{1:n} \given \Xb_{1:n}}(\cdot \given \xb_{1:n})
    \right)
    \\
    &=
    \ln
    \frac{
      p_{\Xb_{1:n}}(\xb_{1:n})
    }{
      p_{\Xb_{1:n}}(\xb_{1:n}^{+(j,l)})
    }
    +
    \frac{
      1
    }{
      p_{\Xb_{1:j+1}}(\xb_{1:n}^{+(j,l)})
    }
    \int
    \theta_{j,l} \ln(\theta_{j,l})
    \prod_{i=1}^n
    p_{X_{i}\given \Theta_{i}}(x_{i} \given \theta_{i})
    \mu_T(d\theta_{i} \given \theta_{i-1})
  \end{aligned}
\end{equation*}

\section*{Computation}
%
\begin{equation*}
  \begin{aligned}
    \lambda^F_{i,j}(h)
    &=
    \begin{cases}
      h(i, j)
      &
      \text{if} ~ i = 1
      \\
      \rho
      \lambda^F_{i-1,j}(h)
      +
      (1-\rho)
      \lambda^F_{i-1,j-1}(h)
      h(i,j)
      &
      \text{if} ~ i > 1
    \end{cases}
    \\
    \lambda^B_{j,k}(h)
    &=
    \begin{cases}
      h(j, k)
      &
      \text{if} ~ k = n
      \\
      \rho
      \lambda^B_{j,k+1}(h)
      +
      (1-\rho)
      \lambda^B_{j+1,k+1}(h)
      h(j,k)
      &
      \text{if} ~ k < n
    \end{cases}
  \end{aligned}
\end{equation*}
Posterior marginal expectation
\begin{equation*}
  \E[\theta_j \given \Xb_{1:n} = \xb_{1:n}]
  =
  \int
  \theta_j
  \mu_{\Theta_j \given \Xb_{1:n}}(d\theta_j \given x_{1:n})
  =
  \lambda^F_{j,j}(h_e) \cdot \lambda^B_{j,j}(h_e)
\end{equation*}
with
\begin{equation*}
  h_e(i,k)
  =
  \frac{
    \Beta(\alphab_i + \sum_{j=i}^k \nb_j)
  }{
    \Beta(\alphab_i)
  }
  \frac{
    \alphab_i + \sum_{j=i}^k \nb_j
  }{
    \sum_{l'}
    \alpha_{i,l'} + \sum_{j=i}^k n_{j,l'}
  }
\end{equation*}
Utility
\begin{equation*}
  u(j, l, \xb_{1:n})
  =
  \ln
  \frac{
    p_{\Xb_{1:n}}(\xb_{1:n})
  }{
    p_{\Xb_{1:n}}(\xb_{1:n}^{+(j,l)})
  }
  +
  \frac{
    \lambda^F_{n,n}(h_u^{(j,l)})
  }{
    p_{\Xb_{1:j+1}}(\xb_{1:n}^{+(j,l)})
  }
\end{equation*}
with
\begin{equation*}
  h_u^{(j,l)}(i, k)
  =
  \frac{
    \Beta(\alphab_i + \sum_{j'=i}^k \nb_{j'})
  }{
    \Beta(\alphab_i)
  }
  \left(
  \psi\left(\alpha_{i,l} + \sum_{j'=i}^k n_{j',l}\right)
  -
  \psi\left(\sum_{l'} \alpha_{i,l'} + \sum_{j'=i}^k n_{j',l'}\right)
  \right)^{\1(i \le j \le k)}
\end{equation*}
