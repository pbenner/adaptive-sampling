% -*- mode: latex -*-
%
\section*{Model}
%
\tikzstyle{rv}=        [shape=circle,    minimum size=1.2cm]
\tikzstyle{parameter}= [shape=rectangle, minimum size=1.2cm]
\begin{figure}[htbp]
  \begin{center}
    \begin{tikzpicture}[]
      % states
      \node[parameter] (A1) at (0,4) {$\alpha_{j-1}$}
      ;
      \node[parameter] (A2) at (2,4) {$\alpha_{j}$}
      ;
      \node[parameter] (A3) at (4,4) {$\alpha_{j+1}$}
      ;
      \node[parameter] (A4) at (6,4) {$\alpha_{j+2}$}
      ;
      % theta 1
      \node[rv] (T0) at (-2,2) {}
      ;
      \node[rv] (T1) at (-0.25,2) {$\Thetab^1_{j-1}$}
        edge [<-] (T0)
        edge [<-] (A1)
      ;
      \node[rv] (T2) at (1.75,2) {$\Thetab^1_{j}$}
        edge [<-] (T1)
        edge [<-] (A2)
      ;
      \node[rv] (T3) at (3.75,2) {$\Thetab^1_{j+1}$}
        edge [<-] (T2)
        edge [<-] (A3)
      ;
      \node[rv] (T4) at (5.75,2) {$\Thetab^1_{j+2}$}
        edge [<-] (T3)
        edge [<-] (A4)
      ;
      \node[rv] (T5) at (7.75,2) {}
        edge [<-] (T4)
      ;
      % theta 2
      \node[rv] (S0) at (-1.5,2.5) {}
      ;
      \node[rv] (S1) at (0.5,2.5) {$\Thetab^2_{j-1}$}
        edge [->] (S0)
        edge [<-] (A1)
      ;
      \node[rv] (S2) at (2.5,2.5) {$\Thetab^2_{j}$}
        edge [->] (S1)
        edge [<-] (A2)
      ;
      \node[rv] (S3) at (4.5,2.5) {$\Thetab^2_{j+1}$}
        edge [->] (S2)
        edge [<-] (A3)
      ;
      \node[rv] (S4) at (6.5,2.5) {$\Thetab^2_{j+2}$}
        edge [->] (S3)
        edge [<-] (A4)
      ;
      \node[rv] (S5) at (8.5,2.5) {}
        edge [->] (S4)
      ;
      % observations
      \node[rv] (X1) at (0,0) {$\Xb_{j-1}$}
        edge [<-] (T1)
        edge [<-] (S1)
      ;
      \node[rv] (X2) at (2,0) {$\Xb_{j}$}
        edge [<-] (T2)
        edge [<-] (S2)
      ;
      \node[rv] (X3) at (4,0) {$\Xb_{j+1}$}
        edge [<-] (T3)
        edge [<-] (S3)
      ;
      \node[rv] (X4) at (6,0) {$\Xb_{j+2}$}
        edge [<-] (T4)
        edge [<-] (S4)
      ;
    \end{tikzpicture}
  \end{center}
  \caption{Polya urn hidden Markov mixture process.}
  \label{fig:1}
\end{figure}
The model consists of two hidden Markov chains $M_1 =
\{\Thetab_j^1\}_{j=1}^n$ and $M_2 = \{\Thetab_j^2\}_{j=1}^n$ on a
probability space $(\Omega, \Bc(\Omega), P)$. The transition kernels
$\mu^1_T, \mu^2_T: \Bc(\Omega) \times \Omega \rightarrow \mathbb{R}^+$
with
\begin{equation*}
  \begin{aligned}
    P(\Theta^1_{j} \in A \given \Theta^1_{j-1} = \thetab_{j-1})
    =
    \mu^1_T(A \given \thetab_{j-1})
    \ ,
    \quad
    P(\Theta^2_{j} \in A \given \Theta^2_{j+1} = \thetab_{j+1})
    =
    \mu^2_T(A \given \thetab_{j+1})
    \ ,
  \end{aligned}
\end{equation*}
are defined as
\begin{equation*}
  \begin{aligned}
    \mu^1_T(d\thetab_j \given \thetab_{j-1})
    &=
    \rho
    \delta_{\thetab_{j-1}}(d\thetab_j)
    +
    (1-\rho)
    \mu_{\Dir(\alphab_j)}(d\thetab_j)
    \ ,
    \\
    \mu^2_T(d\thetab_j \given \thetab_{j+1})
    &=
    \rho
    \delta_{\thetab_{j+1}}(d\thetab_j)
    +
    (1-\rho)
    \mu_{\Dir(\alphab_j)}(d\thetab_j)
    \ ,
  \end{aligned}
\end{equation*}
where $\mu_{\Dir(\alphab_j)}$ is the Dirichlet distribution on
$\Omega$ with parameters $\alphab_j$ and $\rho \in [0,1]$ a mixture
parameter.  We also define $\Thetab_j = \gamma \Thetab^1_j +
(1-\gamma) \Thetab^2_j$ for $j = 1, 2, \dots, n$, so that at each
position $j$ we have $m = m(j)$ emissions $\Xb_j = (X_{j,m})_m$
obtained from a mixture of the two Markov chains, i.e.
\begin{equation*}
  X_{j,m}
  \given
  \Thetab_j = \thetab_j
  \sim 
  \Discrete(\thetab_j)
  \ ,
\end{equation*}
with $\gamma \in [0, 1]$. Each $X_{j,m}$ takes values in some finite
alphabet $\Ac$ such that $\Omega = \Delta_{|\Ac|}$, the
$|\Ac|$-dimensional probability simplex. The full model is depicted in
Figure~\ref{fig:1}. Our main interest is to compute
\begin{equation*}
  \mu_{\Thetab_j \given \Xb_{1:n}}
  f
  =
  \int_{\Omega}
  f(\thetab)
  \mu_{\Thetab_j \given \Xb_{1:n}}(d\thetab \given \xb_{1:n})
  =
  \int_{\Omega^n}
  f(\thetab_j)
  \prod_{i=1}^n
  p_{\Xb_{i}\given \Thetab_{i}}(\xb_{i} \given \thetab_{i})
  \mu_T(d\thetab_{i} \given \thetab_{i-1})
  \ .
\end{equation*}
for some $f \in L^1$ and $j \in \{1, 2, \dots, n\}$, that is to obtain
the measure $\mu_{\Thetab_j \given \Xb_{1:n}}$.

Since both Markov chains have the same transition kernel it suffices
to discuss inference with only one chain. Most posterior quantities of
interest, including the utility measure, can be computed with the
marginal posterior measure
\begin{equation*}
  \mu_{\Thetab^1_j \given \Xb_{1:n}}(d\thetab_j \given \xb_{1:n})
  \propto
  \mu_{\Thetab^1_{j} \given \Xb_{1:j}}(d\thetab_{j} \given \xb_{1:j})
  p_{\Xb_{j+1:n} \given \Thetab^1_j}(\xb_{j+1:n} \given \thetab_j)
  \ ,
\end{equation*}
which consists of the forward measure
\begin{multline*}
  \mu_{\Thetab^1_{j+1} \given \Xb_{1:j+1}}(d\thetab_{j+1} \given \xb_{1:j+1})
  \\
  \begin{aligned}
    &=
    \frac{1}{p_{\Xb_{1:j+1}}(\xb_{1:j+1})}
    \int_{\Omega}
    p_{\Xb_{j+1}\given \Thetab^1_{j+1}}(\xb_{j+1} \given \thetab_{j+1})
    \mu_T(d\thetab_{j+1} \given \thetab_j)
    \mu_{\Thetab^1_{j} \given \Xb_{1:j}}(d\thetab_{j} \given \xb_{1:j})
    \\
    &=
    \frac{\rho}{p_{\Xb_{1:j+1}}(\xb_{1:j+1})}
    p_{\Xb_{j+1}\given \Thetab^1_{j+1}}(\xb_{j+1} \given \thetab_{j+1})
    \mu_{\Thetab^1_{j} \given \Xb_{1:j}}(d\thetab_{j+1} \given \xb_{1:j})
    +
    (1-\rho)
    \mu_{\Dir(\alpha_{j+1})}(d\thetab_{j+1})
  \end{aligned}
\end{multline*}
with $\mu_{\Thetab^1_{1} \given \Xb_{1:1}}(d\thetab_{1} \given \xb_{1:1})
= p_{\Xb_{1}\given \Thetab^1_{1}}(\xb_{1} \given \thetab_{1})
\mu_{\Dir(\alpha_{1})}(d\thetab_{1})$ and backward probabilities
\begin{multline*}
  \begin{aligned}
    p_{\Xb_{j+1:n} \given \Thetab^1_j}(\xb_{j+1:n} \given \thetab_j)
    &=
    \int_{\Omega}
    p_{\Xb_{j+1}\given \Thetab^1_{j+1}}(\xb_{j+1} \given \thetab_{j+1})
    p_{\Xb_{j+2:n} \given \Thetab^1_{j+1}}(\xb_{j+2:n} \given \thetab_{j+1})
    \mu_T(d\thetab_{j+1} \given \thetab_j)
    \\
    &=
    \rho
    p_{\Xb_{j+1}\given \Thetab^1_{j+1}}(\xb_{j+1} \given \thetab_{j})
    p_{\Xb_{j+2:n} \given \Thetab^1_{j+1}}(\xb_{j+2:n} \given \thetab_{j})
    \\
    &\quad+
    (1-\rho)
    \int_{\Omega}
    p_{\Xb_{j+1}\given \Thetab^1_{j+1}}(\xb_{j+1} \given \thetab_{j+1})
    p_{\Xb_{j+2:n} \given \Thetab^1_{j+1}}(\xb_{j+2:n} \given \thetab_{j+1})
    \mu_{\Dir(\alpha_{j+1})}(d\thetab_{j+1})
    \ .
  \end{aligned}
\end{multline*}

\section*{Utility}
%
Assume that at position $j$ we already have $m$ samples. A suitable
utility measure $u(j, \xb_{1:n})$ for sampling at position $j$ is
defined by the mutual information between $\Thetab_{1:n}$ and $X_{j,m+1}$
given our previous observations $\{\Xb_{1:n} = \xb_{1:n} \}$, i.e.
\begin{equation*}
  \begin{aligned}
    u(j, \xb_{1:n})
    &=
    \I(\Thetab_{1:n} ; X_{j,m+1} \given \Xb_{1:n} = \xb_{1:n})
    \\
    &=
    \E_{X_{j,m+1}}\left[
      \Dkl\left(
      \mu_{\Thetab_{1:n} \given \Xb_{1:n}, X_{j,m+1}}(\cdot \given \xb_{1:n}, x)
      \ggiven
      \mu_{\Thetab_{1:n} \given \Xb_{1:n}}(\cdot \given \xb_{1:n})
      \right)
      \given
      \Xb_{1:n} = \xb_{1:n}
    \right]
    \ ,
  \end{aligned}
\end{equation*}
with
\begin{multline*}
  \Dkl\left(
  \mu_{\Thetab_{1:n} \given \Xb_{1:n}, X_{j,m+1}}(\cdot \given \xb_{1:n}, x)
  \ggiven
  \mu_{\Thetab_{1:n} \given \Xb_{1:n}}(\cdot \given \xb_{1:n})
  \right)
  \\
  \begin{aligned}
    &=
    \frac{
      1
    }{
      p_{\Xb_{1:j+1}, X_{j,m+1}}(\xb_{1:n}, x)
    }
    \int_{\Omega}
    \theta_{j,x} \ln(\theta_{j,x})
    \mu_{\Thetab_j \given \Xb_{1:n}}(d\thetab_j \given \xb_{1:n})
    -
    \ln
    p_{X_{j,m+1} \given \Xb_{1:n}}(x \given \xb_{1:n})
    \ ,
  \end{aligned}
\end{multline*}
where we used
\begin{equation*}
  \int_{\Omega^n}
  \theta_{j,x} \ln(\theta_{j,x})
  \prod_{i=1}^n
  p_{\Xb_{i}\given \Thetab_{i}}(\xb_{i} \given \thetab_{i})
  \mu_T(d\thetab_{i} \given \thetab_{i-1})
  =
  \int_{\Omega}
  \theta_{j,x} \ln(\theta_{j,x})
  \mu_{\Thetab_j \given \Xb_{1:n}}(d\thetab_j \given \xb_{1:n})
  \ .
\end{equation*}

\section*{Computation}
%
By solving the recursive definitions of the forward measures and
backward probabilities we obtain the same computational structure as
in \cite{Yao1984} or \cite{Barry1992}.
For $0 \le i \le j \le k \le n$ we define
\begin{equation*}
  \begin{aligned}
    \lambda^F_k(h)
    &=
    \rho^{k-1} h(1, k)
    +
    \sum_{j=1}^{k-1} \lambda^F_j(h) (1-\rho)\rho^{k-j-1}h(j+1, k)
    \ ,
    \\
    \lambda^B_i(h)
    &=
    \rho^{n-i} h(i, n)
    +
    \sum_{j=i+1}^{n} \lambda^B_j(h) (1-\rho)\rho^{j-i-1}h(i, j-1)
    \ ,
  \end{aligned}
\end{equation*}
so that, for instance, $p_{\Xb_{1:n}}(\xb_{1:n}) = \lambda^F_n(h_p) =
\lambda^B_1(h_p)$ with
\begin{equation*}
  h_p(i,k)
  =
  \gamma
  \frac{
    \Beta\left(\alphab_i + \sum_{j=i}^k \cb(\xb_j)\right)
  }{
    \Beta(\alphab_i)
  }
  +
  (1-\gamma)
  \frac{
    \Beta\left(\alphab_k + \sum_{j=i}^k \cb(\xb_j)\right)
  }{
    \Beta(\alphab_k)
  }
  \ ,
\end{equation*}
where $\cb(\xb_j)$ is the count statistic of $\{\Xb_j = \xb_j\}$.
To compute $\mu_{\Thetab_j \given \Xb_{1:n}} f$ for some $f \in L^1$
we define
\begin{multline*}
  \lambda^{FB}_j(h)
  =
  \rho^{n-1}h(1, n)
  +
  \sum_{i=1}^{j-1}(1-\rho)\rho^{n-i-1}\lambda^F_i(h_p) h(i+1, n)
  \\
  +
  \sum_{k=j+1}^n (1-\rho)
  \left[
    \rho^{k-2} h(1, k-1)
    +
    \sum_{i=1}^{j-1} (1-\rho) \rho^{k-i-2}
    \lambda^F_i(h_p) h(i+1, k-1)
  \right]
  \lambda^B_k(h_p)
\end{multline*}
with
\begin{equation*}
  h(i,k)
  =
  \int_{\Omega}
  f(\thetab)
  \prod_{j=i}^k
  p_{\Xb_j \given \Thetab_j}(\xb_j \given \thetab)
  \left[
    \gamma
    \mu_{\Dir(\alphab_i)}(d\thetab)
    +
    (1-\gamma)
    \mu_{\Dir(\alphab_k)}(d\thetab)
  \right]
\end{equation*}
so that
\begin{equation*}
  \mu_{\Thetab_j \given \Xb_{1:n}} f
  =
  \frac{
    \lambda^{FB}_j(h)
  }{
    \lambda^F_n(h_p)
  }
  \ .
\end{equation*}

For instance, the marginal posterior expectation $\mu_{\Thetab_j \given
  \Xb_{1:n}} \thetab$ can be obtained with
\begin{equation*}
  h(i,k)
  =
  \frac{h_e(i,k)}{h_p(i,k)}
  =
  \gamma
  \frac{
    \alphab_i + \sum_{j=i}^k \nb_j
  }{
    \sum_{x}
    \alpha_{i,x} + \sum_{j=i}^k c_x(\xb_j)
  }
  +
  (1-\gamma)
  \frac{
    \alphab_k + \sum_{j=i}^k \nb_j
  }{
    \sum_{x}
    \alpha_{k,x} + \sum_{j=i}^k c_x(\xb_j)
  }
  \ .
\end{equation*}
The utility for an event $x \in \Ac$ can be computed with $f(\thetab)
= \theta_x \ln \theta_x$ so that
\begin{equation*}
  \begin{aligned}
    h(i,k)
    =
    \frac{h^x_u(i,k)}{h_e(i,k)}
    &=
    \gamma
    \left[
    \psi\left(\alpha_{i,x} + \sum_{j=i}^k c_x(\xb_j) + 1\right)
    -
    \psi\left(\sum_{y\in \Ac} \alpha_{i,y} + \sum_{j=i}^k c_y(\xb_j) + 1\right)
    \right]
    \\
    &+
    (1-\gamma)
    \left[
    \psi\left(\alpha_{k,x} + \sum_{j=i}^k c_x(\xb_j) + 1\right)
    -
    \psi\left(\sum_{y\in \Ac} \alpha_{k,y} + \sum_{j=i}^k c_y(\xb_j) + 1\right)
    \right]
    \ .
  \end{aligned}
\end{equation*}

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.495\textwidth,page=2]{example/example1.pdf}
  \includegraphics[width=0.495\textwidth,page=3]{example/example1.pdf}\\
  \includegraphics[width=0.495\textwidth,page=4]{example/example1.pdf}
  \includegraphics[width=0.495\textwidth,page=5]{example/example1.pdf}\\
  \includegraphics[width=0.495\textwidth,page=6]{example/example1.pdf}
  \includegraphics[width=0.495\textwidth,page=7]{example/example1.pdf}
  \caption{
    Adaptive sampling in a hypothetical experiment with $\gamma = 0.5$
    and $\rho = 0.8$. The figure shows the experiment after $1$, $2$,
    $10$, $50$, $100$, and $200$ samples. At first, samples are
    uniformly distributed. In (a) only one measurement was taken and
    the expected utility is largest at the left boundary. (d) shows the
    experiment after 50 samples where the algorithm starts to locate
    measurements at sloped regions. The general shape of the
    stimulus-response function is already well established after 100
    samples (e). Many measurements are also taken at $x=1$ and $x=35$
    since those $x$ have only one neighbor.}
  \label{fig:05}
\end{figure}
