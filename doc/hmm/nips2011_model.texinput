% -*- mode: latex -*-
%
\section*{Model}
%
\tikzstyle{rv}=              [shape=circle,       minimum size=1.2cm]
\tikzstyle{parameter}= [shape=rectangle, minimum size=1.2cm]
\begin{figure}[htbp]
  \begin{center}
    \begin{tikzpicture}[]
      % states
      \node[parameter] (A1) at (0,4) {$\alpha_{j-1}$}
      ;
      \node[parameter] (A2) at (2,4) {$\alpha_{j}$}
      ;
      \node[parameter] (A3) at (4,4) {$\alpha_{j+1}$}
      ;
      \node[parameter] (A4) at (6,4) {$\alpha_{j+2}$}
      ;
      % theta 1
      \node[rv] (T0) at (-2,2) {}
      ;
      \node[rv] (T1) at (0,2) {$\Thetab^1_{j-1}$}
        edge [<-] (T0)
        edge [<-] (A1)
      ;
      \node[rv] (T2) at (2,2) {$\Thetab^1_{j}$}
        edge [<-] (T1)
        edge [<-] (A2)
      ;
      \node[rv] (T3) at (4,2) {$\Thetab^1_{j+1}$}
        edge [<-] (T2)
        edge [<-] (A3)
      ;
      \node[rv] (T4) at (6,2) {$\Thetab^1_{j+2}$}
        edge [<-] (T3)
        edge [<-] (A4)
      ;
      \node[rv] (T5) at (8,2) {}
        edge [<-] (T4)
      ;
      % theta 2
      \node[rv] (S0) at (-1.5,2.5) {}
      ;
      \node[rv] (S1) at (0.5,2.5) {$\Thetab^2_{j-1}$}
        edge [->] (S0)
        edge [<-] (A1)
      ;
      \node[rv] (S2) at (2.5,2.5) {$\Thetab^2_{j}$}
        edge [->] (S1)
        edge [<-] (A2)
      ;
      \node[rv] (S3) at (4.5,2.5) {$\Thetab^2_{j+1}$}
        edge [->] (S2)
        edge [<-] (A3)
      ;
      \node[rv] (S4) at (6.5,2.5) {$\Thetab^2_{j+2}$}
        edge [->] (S3)
        edge [<-] (A4)
      ;
      \node[rv] (S5) at (8.5,2.5) {}
        edge [->] (S4)
      ;
      % observations
      \node[rv] (X1) at (0,0) {$\Xb_{j-1}$}
        edge [<-] (T1)
        edge [<-] (S1)
      ;
      \node[rv] (X2) at (2,0) {$\Xb_{j}$}
        edge [<-] (T2)
        edge [<-] (S2)
      ;
      \node[rv] (X3) at (4,0) {$\Xb_{j+1}$}
        edge [<-] (T3)
        edge [<-] (S3)
      ;
      \node[rv] (X4) at (6,0) {$\Xb_{j+2}$}
        edge [<-] (T4)
        edge [<-] (S4)
      ;
    \end{tikzpicture}
  \end{center}
  \caption{Polya urn hidden Markov mixture process.}
  \label{fig:1}
\end{figure}

\begin{equation*}
  \begin{aligned}
    \Thetab^1_j
    &\given
    \Thetab^1_{j-1} = \thetab^1_{j-1}
    &\sim&\quad
    \rho
    \delta_{\theta^1_{j-1}}
    +
    (1-\rho)
    \mu_{\Dir(\alpha_j)}
    \\
    \Thetab^2_j
    &\given
    \Thetab^2_{j+1} = \thetab^2_{j+1}
    &\sim&\quad
    \rho
    \delta_{\theta^2_{j+1}}
    +
    (1-\rho)
    \mu_{\Dir(\alpha_j)}
    \\
    X_{j,m}
    &\given
    \Thetab^1_j = \thetab^1_j,
    \Thetab^2_j = \thetab^2_j
    &\sim&\quad
    \gamma
    \Discrete(\thetab^1_j)
    +
    (1-\gamma)
    \Discrete(\thetab^2_j)
  \end{aligned}
\end{equation*}
with $\Xb_j = (X_{j,m})_m$ and mixture parameter $\gamma \in [0, 1]$.

Define the transition kernel
\begin{equation*}
  \mu_T(d\theta_{j+1} \given \theta_j)
  =
  \rho
  \delta_{\theta_j}(d\theta_{j+1})
  +
  (1-\rho)
  \mu_{\Dir(\alpha_{j+1})}(d\theta_{j+1})
\end{equation*}
and multinomial emission probabilities $p_{X_{j}\given
  \Theta_{j}}(x_{j} \given \theta_{j})$.

The marginal posterior measure is given by
\begin{equation*}
  \mu_{\Theta_j \given \Xb_{1:n}}(d\theta_j \given x_{1:n})
  \propto
  \mu_{\Theta_{j} \given \Xb_{1:j}}(d\theta_{j} \given \xb_{1:j})
  p_{\Xb_{j+1:n} \given \Theta_j}(\xb_{j+1:n} \given \theta_j)
\end{equation*}

Forward measures
\begin{multline*}
  \mu_{\Theta_{j+1} \given \Xb_{1:j+1}}(d\theta_{j+1} \given \xb_{1:j+1})
  \\
  \begin{aligned}
    &=
    \frac{1}{p_{\Xb_{1:j+1}}(\xb_{1:j+1})}
    \int
    p_{X_{j+1}\given \Theta_{j+1}}(x_{j+1} \given \theta_{j+1})
    \mu_T(d\theta_{j+1} \given \theta_j)
    \mu_{\Theta_{j} \given \Xb_{1:j}}(d\theta_{j} \given \xb_{1:j})
    \\
    &=
    \frac{\rho}{p_{\Xb_{1:j+1}}(\xb_{1:j+1})}
    p_{X_{j+1}\given \Theta_{j+1}}(x_{j+1} \given \theta_{j+1})
    \mu_{\Theta_{j} \given \Xb_{1:j}}(d\theta_{j+1} \given \xb_{1:j})
    +
    (1-\rho)
    \mu_{\Dir(\alpha_{j+1})}(d\theta_{j+1})
  \end{aligned}
\end{multline*}
with $\mu_{\Theta_{1} \given \Xb_{1:1}}(d\theta_{1} \given \xb_{1:1})
= p_{X_{1}\given \Theta_{1}}(x_{1} \given \theta_{1})
\mu_{\Dir(\alpha_{1})}(d\theta_{1})$ and the backward probabilities
\begin{multline*}
  \begin{aligned}
    p_{\Xb_{j+1:n} \given \Theta_j}(\xb_{j+1:n} \given \theta_j)
    &=
    \int
    p_{X_{j+1}\given \Theta_{j+1}}(x_{j+1} \given \theta_{j+1})
    p_{\Xb_{j+2:n} \given \Theta_{j+1}}(\xb_{j+2:n} \given \theta_{j+1})
    \mu_T(d\theta_{j+1} \given \theta_j)
    \\
    &=
    \rho
    p_{X_{j+1}\given \Theta_{j+1}}(x_{j+1} \given \theta_{j})
    p_{\Xb_{j+2:n} \given \Theta_{j+1}}(\xb_{j+2:n} \given \theta_{j})
    \\
    &\quad+
    (1-\rho)
    \int
    p_{X_{j+1}\given \Theta_{j+1}}(x_{j+1} \given \theta_{j+1})
    p_{\Xb_{j+2:n} \given \Theta_{j+1}}(\xb_{j+2:n} \given \theta_{j+1})
    \mu_{\Dir(\alpha_{j+1})}(d\theta_{j+1})
  \end{aligned}
\end{multline*}

\section*{Utility}
%
Assume that at position $j$ we already have $m$ samples. A suitable
utility measure $u(j, \xb_{1:n})$ for sampling at position $j$ is
defined by the mutual information between $\Thetab_{1:n}$ and $X_{j,m+1}$
given our previous observations $\{\Xb_{1:n} = \xb_{1:n} \}$, i.e.
\begin{equation*}
  \begin{aligned}
    u(j, \xb_{1:n})
    &=
    \I(\Thetab_{1:n} ; X_{j,m+1}) \given \Xb_{1:n} = \xb_{1:n}
    \\
    &=
    \E_{X_{j,m+1}}\left[
      \Dkl\left(
      \mu_{\Thetab_{1:n} \given \Xb_{1:n}, X_{j,m+1}}(\cdot \given \xb_{1:n}, x)
      \ggiven
      \mu_{\Thetab_{1:n} \given \Xb_{1:n}}(\cdot \given \xb_{1:n})
      \right)
      \given
      \Xb_{1:n} = \xb_{1:n}
    \right]
    \ ,
  \end{aligned}
\end{equation*}
with
\begin{multline*}
  \Dkl\left(
  \mu_{\Thetab_{1:n} \given \Xb_{1:n}, X_{j,m+1}}(\cdot \given \xb_{1:n}, x)
  \ggiven
  \mu_{\Thetab_{1:n} \given \Xb_{1:n}}(\cdot \given \xb_{1:n})
  \right)
  \\
  \begin{aligned}
    &=
    \ln
    \frac{
      p_{\Xb_{1:n}}(\xb_{1:n})
    }{
      p_{\Xb_{1:n}, X_{j,m+1}}(\xb_{1:n}, x)
    }
    +
    \frac{
      1
    }{
      p_{\Xb_{1:j+1}, X_{j,m+1}}(\xb_{1:n}, x)
    }
    \int
    \theta_{j,x} \ln(\theta_{j,x})
    \prod_{i=1}^n
    p_{\Xb_{i}\given \Thetab_{i}}(\xb_{i} \given \thetab_{i})
    \mu_T(d\thetab_{i} \given \thetab_{i-1})
    \ .
  \end{aligned}
\end{multline*}

\section*{Computation}
%
\begin{equation*}
  \begin{aligned}
    \lambda^F_{i,j}(h)
    &=
    \begin{cases}
      h(i, j)
      &
      \text{if} ~ i = 1
      \\
      \rho
      \lambda^F_{i-1,j}(h)
      +
      (1-\rho)
      \lambda^F_{i-1,j-1}(h)
      h(i,j)
      &
      \text{if} ~ i > 1
    \end{cases}
    \\
    \lambda^B_{j,k}(h)
    &=
    \begin{cases}
      h(j, k)
      &
      \text{if} ~ k = n
      \\
      \rho
      \lambda^B_{j,k+1}(h)
      +
      (1-\rho)
      \lambda^B_{j+1,k+1}(h)
      h(j,k)
      &
      \text{if} ~ k < n
    \end{cases}
  \end{aligned}
\end{equation*}
Posterior marginal expectation
\begin{equation*}
  \E[\theta_j \given \Xb_{1:n} = \xb_{1:n}]
  =
  \int
  \theta_j
  \mu_{\Theta_j \given \Xb_{1:n}}(d\theta_j \given x_{1:n})
  =
  \lambda^F_{j,j}(h_e) \cdot \lambda^B_{j,j}(h_e)
\end{equation*}
with
\begin{equation*}
  h_e(i,k)
  =
  \frac{
    \Beta(\alphab_i + \sum_{j=i}^k \nb_j)
  }{
    \Beta(\alphab_i)
  }
  \frac{
    \alphab_i + \sum_{j=i}^k \nb_j
  }{
    \sum_{l'}
    \alpha_{i,l'} + \sum_{j=i}^k n_{j,l'}
  }
\end{equation*}
Utility
\begin{equation*}
  u(j, l, \xb_{1:n})
  =
  \ln
  \frac{
    p_{\Xb_{1:n}}(\xb_{1:n})
  }{
    p_{\Xb_{1:n}}(\xb_{1:n}^{+(j,l)})
  }
  +
  \frac{
    \lambda^F_{n,n}(h_u^{(j,l)})
  }{
    p_{\Xb_{1:j+1}}(\xb_{1:n}^{+(j,l)})
  }
\end{equation*}
with
\begin{equation*}
  h_u^{(j,l)}(i, k)
  =
  \frac{
    \Beta(\alphab_i + \sum_{j'=i}^k \nb_{j'})
  }{
    \Beta(\alphab_i)
  }
  \left(
  \psi\left(\alpha_{i,l} + \sum_{j'=i}^k n_{j',l}\right)
  -
  \psi\left(\sum_{l'} \alpha_{i,l'} + \sum_{j'=i}^k n_{j',l'}\right)
  \right)^{\1(i \le j \le k)}
\end{equation*}
